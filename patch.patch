diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/apply_sparsity_thres2.m NeuroMiner-1-main.clara/apply_sparsity_thres2.m
--- NeuroMiner-1-main/apply_sparsity_thres2.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/apply_sparsity_thres2.m	2021-06-21 14:02:36.530000000 +0200
@@ -0,0 +1,9 @@
+function S = apply_sparsity_thres2(A, thres)
+nEdges = size(A,2)*thres;
+nEdges = ceil(nEdges);
+%sorteA = sort(A, 'descend');
+
+[B,I] = maxk(A,nEdges);
+S = A;
+S(setdiff(1:length(A),I)) = 0;
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/config/data_import/ReadTabular.m NeuroMiner-1-main.clara/config/data_import/ReadTabular.m
--- NeuroMiner-1-main/config/data_import/ReadTabular.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/config/data_import/ReadTabular.m	2021-06-21 22:27:28.270000000 +0200
@@ -22,6 +22,7 @@
         switch fileflg
             case 3
                 IO.Y = readtable(IO.M_edit, 'delimiter', IO.delimit);
+                %IO.Y = readtable(IO.M_edit, 'delimiter', IO.delimit);
             case 4
                 IO.Y = readtable(IO.M_edit, 'Sheet', IO.sheet);
         end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/config/nk_Grid_config.m NeuroMiner-1-main.clara/config/nk_Grid_config.m
--- NeuroMiner-1-main/config/nk_Grid_config.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/config/nk_Grid_config.m	2021-05-21 11:38:06.980000000 +0200
@@ -23,6 +23,7 @@
         Tdefs                           = [.2 .5 .7];
         PolyCoefdefs                    = 0;
         PolyDegrdefs                    = 3;
+        WLiters                         = [2 4 6 8 10];
         Neurondefs                      = [25 50 75 100];
         Leafdefs                        = logspace(1,2,10);
         Treedefs                        = [25 50 75 100 150 200];
@@ -41,6 +42,22 @@
         OptRegul.RegulTypeComplexity    = nk_RegulFunc_config(1);
         OptRegul.RegulTypeDiversity     = nk_RegulFunc_config(1);
         NodeSelect.mode                 = 1;
+        if isfield(NM.TrainParam.SVM.kernel, 'customfunc_nargin')
+            if NM.TrainParam.SVM.kernel.customfunc_nargin >0
+                for n = 1:NM.TrainParam.SVM.kernel.customfunc_nargin
+                    argName = sprintf('customkernel_arg%d', n); 
+                    eval(sprintf("%s = 0;", argName)); 
+                end
+            end
+        end
+        
+%         if NM.SVM.kernel.customkernel_nargin >= 1
+%             for n = 1:SVM.kernel.customkernel_nargin
+%                 argName = sprintf('customkernel_arg%d', n); 
+%                 (argName) = 0;
+%             end
+%         end
+        
         switch CompStr
             case 'above'
                  NodeSelect.perc        = 95;
@@ -78,6 +95,18 @@
             if isfield(GRD,'CoxCutoffparams')           CoxCutoffsdefs = GRD.CoxCutoffparams; end
             if isfield(GRD,'OptRegul'),                 OptRegul = GRD.OptRegul; end
             if isfield(GRD,'NodeSelect'),               NodeSelect = GRD.NodeSelect; end
+            if isfield(GRD,'WLiters'),                  WLiters = GRD.WLiters; end
+            if isfield(NM.TrainParam.SVM.kernel,'customfunc_nargin')
+                if NM.TrainParam.SVM.kernel.customfunc_nargin >0
+                    for n = 1:NM.TrainParam.SVM.kernel.customfunc_nargin
+                        argName = sprintf('customkernel_arg%d', n); 
+                        if isfield(GRD, argName) 
+                            eval(sprintf("%s = GRD.%s", argName, argName)); 
+                        end
+                    end
+                end
+            end
+            %if isfield(GRD, 'CustomKernel'),            CustomKernel = GRD.CustomKernel; end
             
             %============================================================== 
             menustr = []; menuact = []; n_pars = [];
@@ -193,7 +222,42 @@
                                     Pcparstr = 'Sigmoid coefficients'; [Pcstr, n_pars(end+1)] = nk_ConcatParamstr(PolyCoefdefs);
                                     PX = nk_AddParam(PolyCoefdefs, ['ML-' Pcparstr], 2, PX);
                                     menustr = sprintf('%s|Define %s [ %s ]', menustr, Pcparstr, Pcstr);                 menuact = [ menuact 5 ];
-                            end    
+                            end
+                         case ' -t 4' % precomputed kernels
+                             if NM.TrainParam.SVM.kernel.kerndef <8
+                                WLparstr = 'WL iterations'; [WLstr, n_pars(end+1)] = nk_ConcatParamstr(WLiters);
+                                PX = nk_AddParam(WLiters, ['ML-' WLparstr], 2, PX);
+                                menustr = sprintf('%s|Define %s [ %s ]', menustr, WLparstr, WLstr);                        menuact = [ menuact 100];
+                             elseif NM.TrainParam.SVM.kernel.kerndef == 8
+                                 if NM.TrainParam.SVM.kernel.customfunc_nargin > 0
+                                    for n = 1:NM.TrainParam.SVM.kernel.customfunc_nargin
+                                        argParstr = sprintf('CFuncparstr_%d', n); 
+                                        argStr = sprintf('CFuncstr_%d', n);
+                                        arg = sprintf('customkernel_arg%d', n);
+                                        
+                                        eval(sprintf("%s = 'Custom kernel argument %d'", argParstr, n));
+                                        eval(sprintf('[%s, n_pars(end+1)] = nk_ConcatParamstr(%s)', argStr, arg));
+                                        eval(sprintf("PX = nk_AddParam(%s, ['ML-' %s], 2, PX)", arg, argParstr)); 
+                                        eval(sprintf('X = %s; Y = %s', argParstr, argStr)); 
+                                        eval(sprintf("menustr = '%s|Define %s [ %s ]'", menustr, X, Y));
+                                        
+%                                       
+                                        menuact = [ menuact 101];
+                                    end
+                                  
+                                 end
+                                                                        
+                           
+                             end
+%                              switch SVM.kernel.kerndef
+%                                  case 8 % custom kernel function
+%                                      CKFnamestr = 'Custom kernel function (input: 2 matrices, output: kernel matrix)'; [CKFstr, n_pars(end+1)] = nk_ConcatParamstr(CustomKernel);
+%                                      PX = nk_AddParam(CustomKernel, ['ML-' CKFnamestr], 2, PX);
+%                                      menustr = sprintf('%s|Define %s [ %s ]', menustr, CKFnamestr, CKFstr);                        menuact = [ menuact 101];
+%                              
+%                              end
+                            
+                      
                      end             
             end
             
@@ -392,6 +456,17 @@
                     SEQOPTlimsUdefs =  nk_input([LimsLparstr ' range'],0,'e',SEQOPTlimsUdefs);          PX = nk_AddParam(SEQOPTlimsUdefs, ['ML-' LimsUparstr], 2, PX);
                 case 25
                     CoxCutoffsdefs =  nk_input([CoxCutOffparstr ' range'],0,'e',CoxCutoffsdefs);        PX = nk_AddParam(CoxCutoffsdefs, ['ML-' CoxCutOffparstr], 2, PX);
+            
+                case 100
+                    WLiters         = nk_input([WLparstr ' range'],0,'e',WLiters);                      PX = nk_AddParam(WLiters, ['ML-' WLparstr], 2, PX);
+                case 101
+                    for n = 1:NM.TrainParam.SVM.kernel.customfunc_nargin
+                        argParstr = sprintf('CFuncparstr_%d', n); 
+                        arg = sprintf('customkernel_arg%d', n);
+                        eval(sprintf("%s = nk_input([%s ' range'], 0, 'e', %s)", arg, argParstr, arg)); 
+                        eval(sprintf("PX = nk_AddParam(%s, ['ML-' %s], 2, PX)", arg, argParstr));
+                    end
+                          
             end
             if ~isempty(PX) && ~isempty(PX.opt), n_pars = size(PX.opt,1); else n_pars = 0; end
         else
@@ -417,6 +492,15 @@
         GRD.OptRegul            = OptRegul;
         GRD.NodeSelect          = NodeSelect;
         GRD.n_params            = n_pars;
+        GRD.WLiters             = WLiters;
+        if isfield(TrainParam.SVM.kernel, 'customfunc_nargin')
+            if NM.TrainParam.SVM.kernel.customfunc_nargin > 0
+                for n = 1:NM.TrainParam.SVM.kernel.customfunc_nargin
+                    argName = sprintf('customkernel_arg%d', n); 
+                    eval(sprintf("GRD.%s = %s;", argName, argName)); 
+                end
+            end
+        end
     case 2
         
         % ***************** Setup for simulated annealing *****************
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/config/nk_Kernel_config.m NeuroMiner-1-main.clara/config/nk_Kernel_config.m
--- NeuroMiner-1-main/config/nk_Kernel_config.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/config/nk_Kernel_config.m	2021-05-19 11:06:42.830000000 +0200
@@ -58,16 +58,23 @@
                 kerndesc = { 'Linear',...
                              'Polynomial',...
                              'RBF (Gaussian)',...
-                             'Sigmoid'};
+                             'Sigmoid', ...
+                             'Graphkernel: WL', ...
+                             'Graphkernel: WLedge', ...
+                             'Graphkernel: WLspdelta', ...
+                             'Custom kernel'};
                 kernmenu = [ 'Linear|' ...
                              'Polynomial|' ...
                              'RBF (Gaussian)|' ...
-                             'Sigmoid|' ];
-                kernnum = 1:4;
-                kernstr = {' -t 0',' -t 1',' -t 2',' -t 3'};
-                
+                             'Sigmoid|' ...
+                             'Graphkernel: WL|' ...
+                             'Graphkernel: WLedge|' ...
+                             'Graphkernel: WLspdelta|' ...
+                             'Custom kernel|'];
+                kernnum = 1:8;
+                kernstr = {' -t 0',' -t 1',' -t 2',' -t 3', ' -t 4', ' -t 4', ' -t 4', ' -t 4'};
+         
             case 3
-                
                 kerndesc = { 'Linear',...
                              'Polynomial',...
                              'Gaussian',...
@@ -149,11 +156,18 @@
         kerndef = nk_input(['Select kernel type for ' progstr ], 0, 'mq', kernmenu, kernnum, kerndef);
     end
     if kerndef
+        if kerndef == 8
+            param.kernel.customfunc = nk_input(['Enter name of custom kernel function (input: 2 matrices, output: kernel matrix'], 0, 's');
+            param.kernel.customfunc_nargin = str2num(nk_input(['Number of arguments of custom kernel function'], 0, 's'));
+            
+        end
         param.kernel.kerndef = kerndef;
         param.kernel.kernstr = kernstr{param.kernel.kerndef};
         %param = nk_PolyKernel_config(param);
         param.kernel.kerndesc = kerndesc{param.kernel.kerndef};
     end
+
+
 else
     param.kernel.kerndesc = kerndesc{1};
     param.kernel.kerndef = kernnum(1);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/config/nk_Preproc_config.m NeuroMiner-1-main.clara/config/nk_Preproc_config.m
--- NeuroMiner-1-main/config/nk_Preproc_config.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/config/nk_Preproc_config.m	2021-06-26 18:27:10.130000000 +0200
@@ -55,7 +55,10 @@
                     'remmeandiff',  fl, ...
                     'rankfeat',     fl, ...
                     'remvarcomp',   fl, ...
-                    'devmap',       fl);
+                    'devmap',       fl, ...
+                    'graphSparsity', fl, ...
+                    'graphMetrics', fl);
+                    % CHANGE SPARSITY 
 else
     EF = enind;
 end
@@ -186,6 +189,7 @@
                         fprintf('\n   %s ', stepstr);
                     end
                 end
+           
             otherwise
                 if i==stepind 
                     fprintf('\n'); cprintf('*black','>> %s ',stepstr); 
@@ -440,6 +444,10 @@
             cmd = 15;
         case 'devmap'
             cmd = 16;
+        case 'graphSparsity'
+            cmd = 17;
+        case 'graphMetrics'
+            cmd = 18;
     end
    
 else    
@@ -513,6 +521,10 @@
                     end
                 case 'devmap'
                     cmdstr = 'Measure deviation from normative data';                               cmdmnu = 16;
+                case 'graphSparsity'
+                    cmdstr = 'Apply sparsity threshold to connectivity matrix';                     cmdmnu = 17;
+                case 'graphMetrics'
+                    cmdstr = 'Compute network metrics from connectivity matrices';                  cmdmnu = 18;
             end
             [actstr, actmnu] = ConcatMenu(actstr, actmnu, cmdstr, cmdmnu);
             cmdstr =[]; cmdmnu=[];
@@ -558,6 +570,10 @@
         CURACT = config_extdim( CURACT, PREPROC.ACTPARAM{stepind-1}.DR , navistr );
     case 16
         CURACT = config_devmap( NM, CURACT, navistr );
+    case 17
+        CURACT = config_graphSparsity(CURACT, navistr);
+    case 18
+        CURACT = config_graphMetrics(CURACT, navistr);
 end
 
 switch replflag
@@ -811,6 +827,25 @@
 
 end
 
+function CURACT = config_graphSparsity(CURACT, navistr)
+
+if ~isfield(CURACT,'GRAPHSPARSITY'), CURACT.GRAPHSPARSITY=[]; end
+if ~isfield(CURACT,'PX'), CURACT.PX = []; end
+act = 1; while act >0, [ CURACT.GRAPHSPARSITY, CURACT.PX, act ] = graphSparsity_config(CURACT.GRAPHSPARSITY, CURACT.PX, navistr); end
+CURACT.cmd = 'graphSparsity';
+
+end
+
+function CURACT = config_graphMetrics(CURACT, navistr)
+
+if ~isfield(CURACT,'GRAPHMETRICS'), CURACT.GRAPHMETRICS=[]; end
+if ~isfield(CURACT,'PX'), CURACT.PX = []; end
+act = 1; while act >0, [ CURACT.GRAPHMETRICS, CURACT.PX, act ] = graphMetrics_config(CURACT.GRAPHMETRICS, CURACT.PX, navistr); end
+CURACT.cmd = 'graphMetrics';
+
+end
+
+
 % -------------------------------------------------------------------------
 function [actstr, actmnu] = ConcatMenu(actstr, actmnu, cmdstr, cmdmnu)
 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/README NeuroMiner-1-main.clara/graph_kernels/README
--- NeuroMiner-1-main/graph_kernels/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/README	2012-10-29 12:53:10.000000000 +0100
@@ -0,0 +1,62 @@
+This folder contains Matlab scripts for computing several graph kernels for graphs with unlabeled or 
+categorically labeled nodes and scripts for evaluating the SVM classification performance using 
+precomputed kernels using cross-validation.
+
+The folder "unlabeled" contains
+- a README
+- the family of graphlet kernels (subfolders allgraphlets, connectedgraphlets, samplinggraphlets) from [6]
+- the random walk kernel (RWkernel.m) from [7]
+- the shortest path kernel (SPkernel.m) from [1]
+for unlabeled graphs.
+
+The folder "labeled" contains
+- a README
+- 3 kernels from the Weisfeiler-Lehman kernel family: the WL subtree (WL.m), the WL shortest path (WLspdelta.m), 
+  and the WL edge (WLedge.m) kernels from [5]
+- the labeled 3-node graphlet kernel (l3graphletkernel.m) - extension of an algorithm in [6]
+- the labeled random walk kernel (lRWkernel.m) from [7]
+- the labeled random walk kernel based on walks up to size p (untilpRWkernel.m) - extension of [2],[3]
+- the labeled shortest path kernel (spkernel.m) from [1]
+- the Ramon and Gaertner subtree kernel (RGkernel.m) from [4]
+for graphs with categorically labeled nodes. Note that all WL kernels in this folder also support unlabeled 
+graphs.
+
+"help X.m" executed in Matlab will display instructions on the usage of X.m for any script X.m
+
+MUTAG is an example graph data set in the required format for all scripts above.
+
+The folder "svm" contains scripts for evaluating the classification performance of SVM with precomputed kernels 
+using cross-validation. It also contains a separate README. You need to install libSVM and its Matlab interface 
+to be able to use scripts in this folder.
+
+
+
+
+
+References
+[1] K. M. Borgwardt and H.-P. Kriegel. 
+    Shortest-path kernels on graphs. In Proceedings of the International Conference on Data Mining, 
+    pages 74-81, 2005.
+
+[2] T. Gaertner, P. A. Flach, and S. Wrobel. 
+    On graph kernels: Hardness results and efficient alternatives. In Proceedings of the 16th Annual 
+    Conference on Computational Learning Theory and 7th Kernel Workshop, pages 129-143. 
+    ISBN 3-540-40720-0.
+
+[3] H. Kashima, K. Tsuda, and A. Inokuchi. 
+    Marginalized kernels between labeled graphs. In Proceedings of the 20th International Conference 
+    on Machine Learning, Washington, DC, United States, 2003.
+
+[4] J. Ramon and T. Gaertner. 
+    Expressivity versus efficiency of graph kernels. In First International Workshop on Mining Graphs, 
+    Trees and Sequences (held with ECML/PKDD'03), 2003.
+
+[5] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. 
+    Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12:2539-2561, 2011.
+
+[6] N. Shervashidze, S. V. N. Vishwanathan, T. Petri, K. Mehlhorn, and K. M. Borgwardt. 
+    Efficient graphlet kernels for large graph comparison. In Proceedings of the International 
+    Conference on Artificial Intelligence and Statistics, 2009.
+
+[7] S. V. N. Vishwanathan, N. N. Schraudolph, I. R. Kondor, and K. M. Borgwardt. 
+    Graph kernels. Journal of Machine Learning Research, 11:1201-1242, 2010.
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/README NeuroMiner-1-main.clara/graph_kernels/labeled/README
--- NeuroMiner-1-main/graph_kernels/labeled/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/README	2012-10-25 17:47:00.000000000 +0200
@@ -0,0 +1,23 @@
+=== Introduction ===
+
+This folder contains Matlab scripts: WL.m, WLedge.m, WLspdelta.m,
+RGkernel.m, l3graphletkernel.m. lRWkernel.m, untilpRWkernel.m, spkernel.m
+
+WL.m computes the Weisfeiler-Lehman subtree kernel
+WLedge.m computes the Weisfeiler-Lehman edge kernel
+WLspdelta.m computes the Weisfeiler-Lehman shortest path kernel (with 
+exact matching of shortest path lengths)
+on graphs with unweighted unlabeled edges, with/without discrete node labels.
+
+RGkernel.m - Ramon and Gaertner subtree kernel
+l3graphletkernel.m - labeled 3-node graphlet kernel
+lRWkernel.m - labeled random walk kernel
+untilpRWkernel.m - labeled random walk kernel up to p-step random walks
+spkernel.m - labeled shortest path kernel (with delta kernel on shortest path lengths)
+
+
+=== Usage ===
+
+For each script X.m, the Matlab command
+  help X
+will display instructions on how to use the script X.m 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/RGkernel.m NeuroMiner-1-main.clara/graph_kernels/labeled/RGkernel.m
--- NeuroMiner-1-main/graph_kernels/labeled/RGkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/RGkernel.m	2012-10-26 18:34:18.000000000 +0200
@@ -0,0 +1,124 @@
+function [K,runtime] = RGkernel(Graphs,k)
+% Compute height k Ramon & Gaertner kernel for a set of graphs
+% Copyright 2012 Nino Shervashidze
+% For the algorithm see P. Mahe and J.-P. Vert, Graph kernels based
+% on tree patterns for molecules, Arxiv.org, 2008, section 4.2 
+% 
+% Input: Graphs - a 1xN array of graphs
+% 		  Graphs(i).al is the adjacency list of the i'th
+% 		  graph, represented as a cell array of line vectors
+%                 containing ordered integers.
+%                 Graphs(i).nl.values is a column vector of node
+%                 labels for the i'th graph.
+%	 k - a natural number: maximal tree height
+% Output: K - NxN kernel matrix K
+%         runtime - scalar
+
+N=size(Graphs,2);
+Lists = cell(1,N);
+% extract adjacency lists and node labels from the Graphs structure
+for i=1:N
+  Lists{i}=Graphs(i).al;
+  labels{i}=Graphs(i).nl.values;
+end
+clear Graphs;
+
+t=cputime; % for measuring runtime
+
+K=zeros(N,N);
+
+for i=1:N
+  for j=i:N
+    li=length(Lists{i}); % li is the number of nodes in the i'th graph
+    lj=length(Lists{j});
+    k_nodes=double(repmat(labels{i},1,size(labels{j},1))==...
+                  repmat(labels{j}',size(labels{i},1),1));
+    % k_nodes is one matrix per a pair of graphs (i,j) and
+    % k_nodes(u,v) is a node kernel value between the node u of the i'th
+    % graph and the node v of the j'th graph    
+    K(i,j)=sum(sum(k_nodes));
+    for h=2:k
+      K(i,j)=0;
+      k_nodes_new=zeros(length(Lists{i}), length(Lists{i}));
+      % k_nodes_new will replace the former k_nodes in the next iteration
+      for u=1:li
+        for v=1:lj
+          kuv=0;  % disp(['u,v=',num2str([u,v])]); kuv us the node
+                  % kernel value for this iteration for the pair (u,v)
+          if labels{i}(u)==labels{j}(v)
+            if isempty(Lists{i}{u}) || isempty(Lists{j}{v}) 
+              M=[]; 
+            else
+              M=repmat(labels{i}(Lists{i}{u}'), 1, size(labels{j}(Lists{j}{v}'),1))==...
+                repmat(labels{j}(Lists{j}{v}')', size(labels{i}(Lists{i}{u}'),1),1);
+            end
+            [ind1,ind2,junk]=find(M); % nonzero elements = matching pairs
+            if size(ind1,2)>1 ind1=ind1'; ind2=ind2'; end
+            pairs=[(Lists{i}{u}(ind1'))',(Lists{j}{v}(ind2'))'];
+            clear ind1 ind2 junk
+            % Here we compute the maximum number of nodes in each
+            % graph that can be matched with each other (that is,
+            % maximum cardinality that any R can have - and this
+            % maximum will be reached).
+            commonlabels=intersect(labels{i}(Lists{i}{u}')', labels{j}(Lists{j}{v}')');
+            max_matching_size=0;
+            for l=1:length(commonlabels)
+              max_matching_size=max_matching_size+min(length(find(labels{i}(Lists{i}{u}')==commonlabels(l))),...
+                                                      length(find(labels{j}(Lists{j}{v}')==commonlabels(l))));
+            end
+            clear commonlabels
+            if ~isempty(pairs)
+              for l=1:size(pairs,1)
+                matchings(l).pairs=l;
+                kuv=kuv+k_nodes(pairs(l,1), pairs(l,2)); % R's of size 1
+              end
+            end
+            
+            % m will denote the cardinality of R's considered in
+            % each iteration. Note that if isempty(pairs), max_matching_size=0.
+            for m=2:max_matching_size              
+              % matchings holds sets R of size m-1
+              % new_matchings holds sets R of size m
+              new_matchings=[]; new_matchings_counter=1;
+              for l=1:length(matchings)
+                % n (below) is the number (order) of the last pair in the l'th
+                % matching (R) of cardinality m-1
+                % Note that matchings are ordered in ascending
+                % order to guarantee that we do not encounter the
+                % same matching twice. For example the matching 
+                % (pair No 1, pair No 3, pair No 7) can exist, but
+                % (pair No 1, pair No 7, pair No 3) not. 
+                n=matchings(l).pairs(end);
+                while n<size(pairs,1)
+                  n=n+1;
+                  if (any(pairs(matchings(l).pairs,1)==pairs(n,1)) ||...
+                      any(pairs(matchings(l).pairs,2)==pairs(n,2)))	
+                    continue; % avoid that one node occurs in two pairs
+                  end									
+                  new_matchings(new_matchings_counter).pairs=[matchings(l).pairs; n];
+                  prod=1;
+                  for p=1:length(new_matchings(new_matchings_counter).pairs)
+                    prod=prod*k_nodes(pairs(new_matchings(new_matchings_counter).pairs(p),1),...
+                                     pairs(new_matchings(new_matchings_counter).pairs(p),2));
+                  end
+                  kuv=kuv+prod;
+                  new_matchings_counter=new_matchings_counter+1;
+                end
+              end
+              matchings=new_matchings; % at each iteration the size of matchings grows
+            end
+          end
+          k_nodes_new(u,v)=kuv;
+          K(i,j)=K(i,j)+kuv;
+        end
+      end
+      k_nodes=k_nodes_new;
+    end
+    %k_nodes
+    K(j,i)=K(i,j);    
+    disp(['kernel value for the pair of graphs ',num2str([i,j]),' computed']);
+    % K will hold the kernel
+  end
+end
+runtime=cputime-t;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/WL.m NeuroMiner-1-main.clara/graph_kernels/labeled/WL.m
--- NeuroMiner-1-main/graph_kernels/labeled/WL.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/WL.m	2021-04-30 09:38:11.810000000 +0200
@@ -0,0 +1,138 @@
+% Compute h-step Weisfeiler-Lehman kernel for a set of graphs
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2009 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+% 		  Graphs(i).am is the adjacency matrix of the i'th graph, 
+% 		  Graphs(i).al is the adjacency list of the i'th graph, 
+%                 Graphs(i).nl.values is a column vector of node
+%                 labels for the i'th graph.
+%                 Graphs(i) may have other fields, but they will not be
+%                 used here.
+%	 h - a natural number: number of iterations of WL
+%	 nl - a boolean: 1 if we want to use original node labels, 0 otherwise
+% Output: K - a h+1-element cell array of NxN kernel matrices K for
+%             each iter = 0,...,h
+%         runtime - scalar (total runtime in seconds)
+
+function [K,runtime] = WL(Graphs,h,nl)
+% THIS SOURCE CODE IS SUPPLIED "AS IS" WITHOUT WARRANTY OF ANY KIND, AND
+% ITS AUTHOR AND THE JOURNAL OF MACHINE LEARNING RESEARCH (JMLR) AND
+% JMLR'S PUBLISHERS AND DISTRIBUTORS, DISCLAIM ANY AND ALL WARRANTIES,
+% INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES OF MERCHANTABILITY
+% AND FITNESS FOR A PARTICULAR PURPOSE, AND ANY WARRANTIES OR NON 
+% INFRINGEMENT. THE USER ASSUMES ALL LIABILITY AND RESPONSIBILITY FOR 
+% USE OF THIS SOURCE CODE, AND NEITHER THE AUTHOR NOR JMLR, NOR JMLR'S
+% PUBLISHERS AND DISTRIBUTORS, WILL BE LIABLE FOR DAMAGES OF ANY KIND
+% RESULTING FROM ITS USE. Without limiting the generality of the foregoing, 
+% neither the author, nor JMLR, nor JMLR's publishers and distributors,
+% warrant that the Source Code will be error-free, will operate without
+% interruption, or will meet the needs of the user.
+
+N=size(Graphs,2);
+Lists = cell(1,N);
+K = cell(1,h+1);
+n_nodes=0;
+% compute adjacency lists and n_nodes, the total number of nodes in the dataset
+for i=1:N
+  Lists{i}=Graphs(i).al;
+  n_nodes=n_nodes+size(Graphs(i).am,1);
+end
+phi=sparse(n_nodes,N); %each column j of phi will be the explicit feature 
+% representation for the graph j
+
+t=cputime; % for measuring runtime
+
+%%% INITIALISATION
+% initialize the node labels for each graph with their labels or 
+% with degrees (for unlabeled graphs).
+
+label_lookup=containers.Map();
+label_counter=uint32(1);
+% label_lookup is an associative array, which will contain the
+% mapping from multiset labels (strings) to short labels (integers)
+if nl==1
+  for i=1:N
+    % the type of labels{i} is uint32, meaning that it can only handle
+    % 2^32 labels and compressed labels over all iterations. If
+    % more is needed, switching (all occurences of uint32) to
+    % uint64 is a possibility
+    labels{i}=zeros(size(Graphs(i).nl.values,1),1,'uint32');
+    for j=1:length(Graphs(i).nl.values)
+      str_label=num2str(Graphs(i).nl.values(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        %str_label
+        %label_counter
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+      phi(labels{i}(j),i)=phi(labels{i}(j),i)+1;
+    end
+  end
+else
+  for i=1:N
+    labels{i}=uint32(full(sum(Graphs(i).am,2)));
+    for j=1:length(labels{i})
+      str_label=num2str(labels{i}(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;                
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+      phi(labels{i}(j),i)=phi(labels{i}(j),i)+1;
+    end
+  end
+end
+L=label_counter-1;
+disp(['Number of original labels: ',num2str(L)]);
+clear Graphs;
+K{1}=full(phi'*phi);
+
+%%% MAIN LOOP
+iter=1;
+new_labels=labels;
+while iter<=h
+  disp(['iter=',num2str(iter)]);
+  % create an empty lookup table
+  label_lookup=containers.Map();
+  label_counter=uint32(1);
+  % create a sparse matrix for feature representations of graphs
+  phi=sparse(n_nodes,N);
+  for i=1:N
+    for v=1:length(Lists{i})
+      % form a multiset label of the node v of the i'th graph
+      % and convert it to a string
+      long_label=[labels{i}(v), sort(labels{i}(Lists{i}{v}))'];
+      long_label_2bytes=typecast(long_label,'uint16');
+      long_label_string=char(long_label_2bytes);
+      % if the multiset label has not yet occurred, add it to the
+      % lookup table and assign a number to it
+      if ~isKey(label_lookup, long_label_string)
+        label_lookup(long_label_string)=label_counter;
+        new_labels{i}(v)=label_counter;
+        label_counter=label_counter+1;
+      else
+        new_labels{i}(v)=label_lookup(long_label_string);
+      end
+    end
+    % fill the column for i'th graph in phi
+    aux=accumarray(new_labels{i}, ones(length(new_labels{i}),1));
+    phi(new_labels{i},i)=phi(new_labels{i},i)+aux(new_labels{i});
+  end
+  L=label_counter-1;
+  disp(['Number of compressed labels: ',num2str(L)]);
+  K{iter+1}=K{iter}+full(phi'*phi);
+  labels=new_labels;
+  iter=iter+1;
+end
+runtime=cputime-t; % computation time of K
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/WLedge.m NeuroMiner-1-main.clara/graph_kernels/labeled/WLedge.m
--- NeuroMiner-1-main/graph_kernels/labeled/WLedge.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/WLedge.m	2012-10-25 17:44:48.000000000 +0200
@@ -0,0 +1,166 @@
+% Compute h-step Weisfeiler-Lehman edge kernel for a set of graphs
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2010 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+% 		  Graphs(i).am is the adjacency matrix of the i'th graph, 
+% 		  Graphs(i).al is the adjacency list of the i'th graph, 
+%                 Graphs(i).nl.values is a column vector of node
+%                 labels for the i'th graph.
+%                 Graphs(i) may have other fields, but they will not be
+%                 used here.
+%	 h - a natural number: number of iterations of WL
+%	 nl - a boolean: 1 if we want to use original node labels, 0 otherwise
+% Output: K - a h+1-element cell array of NxN kernel matrices K for
+%             each iter = 0,...,h
+%         runtime - scalar (total runtime in seconds)
+
+function [K,runtime] = WLedge(Graphs,h,nl)
+% THIS SOURCE CODE IS SUPPLIED "AS IS" WITHOUT WARRANTY OF ANY KIND, AND
+% ITS AUTHOR AND THE JOURNAL OF MACHINE LEARNING RESEARCH (JMLR) AND
+% JMLR'S PUBLISHERS AND DISTRIBUTORS, DISCLAIM ANY AND ALL WARRANTIES,
+% INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES OF MERCHANTABILITY
+% AND FITNESS FOR A PARTICULAR PURPOSE, AND ANY WARRANTIES OR NON 
+% INFRINGEMENT. THE USER ASSUMES ALL LIABILITY AND RESPONSIBILITY FOR 
+% USE OF THIS SOURCE CODE, AND NEITHER THE AUTHOR NOR JMLR, NOR JMLR'S
+% PUBLISHERS AND DISTRIBUTORS, WILL BE LIABLE FOR DAMAGES OF ANY KIND
+% RESULTING FROM ITS USE. Without limiting the generality of the foregoing, 
+% neither the author, nor JMLR, nor JMLR's publishers and distributors,
+% warrant that the Source Code will be error-free, will operate without
+% interruption, or will meet the needs of the user.
+
+N=size(Graphs,2);
+Lists = cell(1,N);
+K = cell(1,h+1);
+n_nodes=0;
+% compute adjacency lists and n_nodes, the total number of nodes in the dataset
+for i=1:N
+  Lists{i}=Graphs(i).al;
+  n_nodes=n_nodes+size(Graphs(i).am,1);
+end
+% copy adjacency matrices
+for i=1:N
+  AMs{i}=Graphs(i).am;
+end
+
+t=cputime; % for measuring runtime
+
+%%% INITIALISATION
+% initialize the node labels for each graph with their labels or 
+% with degrees (for unlabeled graphs).
+
+label_lookup=containers.Map();
+label_counter=uint32(1);
+% label_lookup is an associative array, which will contain the
+% mapping from multiset labels (strings) to short labels (integers)
+if nl==1
+  for i=1:N
+    % the type of labels{i} is uint32, meaning that it can only handle
+    % 2^32 labels and compressed labels over all iterations. If
+    % more is needed, switching (all occurences of uint32) to
+    % uint64 is a possibility
+    labels{i}=zeros(size(Graphs(i).nl.values,1),1,'uint32');
+    for j=1:length(Graphs(i).nl.values)
+      str_label=num2str(Graphs(i).nl.values(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        %str_label
+        %label_counter
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+    end
+  end
+else
+  for i=1:N
+    labels{i}=uint32(full(sum(Graphs(i).am,2)));
+    for j=1:length(labels{i})
+      str_label=num2str(labels{i}(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;                
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+    end
+  end
+end
+L=double(label_counter)-1;
+clear Graphs;
+disp(['Number of original labels: ',num2str(L)]);
+disp(['Number of potential edge features: ',num2str(L*(L+1)/2)]);
+ed=sparse(L*(L+1)/2,N);
+for i=1:N
+  labels_aux=repmat(double(labels{i}),1,length(labels{i}));
+  a=min(labels_aux, labels_aux');
+  b=max(labels_aux, labels_aux');
+  I=triu(AMs{i}~=0,1);
+  Ind=(a(I)-1).*(2*L+2-a(I))/2+b(I)-a(I)+1;
+  minind=min(Ind);
+  diff=max(Ind)-minind;
+  aux=accumarray(Ind,ones(nnz(I),1),[],[],[],(minind > 5000 || diff > 3000));
+  % sparse of full accumarray depending on the range of values in Ind
+  % (and based on my empirical observations on the speed of accumarray)
+  ed(Ind,i)=aux(Ind);
+end
+ed=ed(sum(ed,2)~=0,:);
+K{1}=full(ed'*ed);
+
+
+%%% MAIN LOOP
+iter=1;
+new_labels=labels;
+while iter<=h
+  disp(['iter=',num2str(iter)]);
+  % create an empty lookup table
+  label_lookup=containers.Map();
+  label_counter=uint32(1);
+  for i=1:N
+    for v=1:length(Lists{i})
+      % form a multiset label of the node v of the i'th graph
+      % and convert it to a string
+      long_label=[labels{i}(v), sort(labels{i}(Lists{i}{v}))'];
+      long_label_2bytes=typecast(long_label,'uint16');
+      long_label_string=char(long_label_2bytes);
+      % if the multiset label has not yet occurred, add it to the
+      % lookup table and assign a number to it
+      if ~isKey(label_lookup, long_label_string)
+        label_lookup(long_label_string)=label_counter;
+        new_labels{i}(v)=label_counter;
+        label_counter=label_counter+1;
+      else
+        new_labels{i}(v)=label_lookup(long_label_string);
+      end
+    end
+  end
+  L=double(label_counter)-1;
+  disp(['Number of compressed labels: ',num2str(L)]);
+  disp(['Number of potential edge features: ',num2str(L*(L+1)/2)]);
+  labels=new_labels;
+  ed=sparse(L*(L+1)/2,N);
+  for i=1:N
+    labels_aux=repmat(double(labels{i}),1,length(labels{i}));
+    a=min(labels_aux, labels_aux');
+    b=max(labels_aux, labels_aux');
+    I=triu(AMs{i}~=0,1);
+    Ind=(a(I)-1).*(2*L+2-a(I))/2+b(I)-a(I)+1;
+    minind=min(Ind);
+    diff=max(Ind)-minind;
+    aux=accumarray(Ind,ones(nnz(I),1),[],[],[],(minind > 5000 || diff > 3000));
+    % sparse of full accumarray depending on the range of values in Ind
+    % (and based on my empirical observations on the speed of accumarray)
+    ed(Ind,i)=aux(Ind);
+  end
+  ed=ed(sum(ed,2)~=0,:);
+  K{iter+1}=K{iter}+full(ed'*ed);
+  iter=iter+1;
+end
+runtime=cputime-t; % computation time of K
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/WLspdelta.m NeuroMiner-1-main.clara/graph_kernels/labeled/WLspdelta.m
--- NeuroMiner-1-main/graph_kernels/labeled/WLspdelta.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/WLspdelta.m	2012-10-25 17:44:48.000000000 +0200
@@ -0,0 +1,221 @@
+% Compute h-step Weisfeiler-Lehman shortest path delta kernel for a set of graphs
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2010 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+% 		  Graphs(i).am is the adjacency matrix of the i'th graph, 
+% 		  Graphs(i).al is the adjacency list of the i'th graph, 
+%                 Graphs(i).nl.values is a column vector of node
+%                 labels for the i'th graph.
+%                 Graphs(i).sp (may be missing) is the shortest
+%                 path length matrix of the i'th graph
+%                 Graphs(i) may have other fields, but they will not be
+%                 used here.
+%	 h - a natural number: number of iterations of WL
+%	 nl - a boolean: 1 if we want to use original node labels, 0 otherwise
+%        spprec - a boolean: 1 if shortest paths are precomputed, 0
+%        otherwise. Default: 0
+% Output: K - a h+1-element cell array of NxN kernel matrices K for
+%             each iter = 0,...,h
+%         runtime - scalar (total runtime in seconds)
+
+function [K,runtime] = WLspdelta(Graphs,h,nl,spprec)
+% THIS SOURCE CODE IS SUPPLIED "AS IS" WITHOUT WARRANTY OF ANY KIND, AND
+% ITS AUTHOR AND THE JOURNAL OF MACHINE LEARNING RESEARCH (JMLR) AND
+% JMLR'S PUBLISHERS AND DISTRIBUTORS, DISCLAIM ANY AND ALL WARRANTIES,
+% INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES OF MERCHANTABILITY
+% AND FITNESS FOR A PARTICULAR PURPOSE, AND ANY WARRANTIES OR NON 
+% INFRINGEMENT. THE USER ASSUMES ALL LIABILITY AND RESPONSIBILITY FOR 
+% USE OF THIS SOURCE CODE, AND NEITHER THE AUTHOR NOR JMLR, NOR JMLR'S
+% PUBLISHERS AND DISTRIBUTORS, WILL BE LIABLE FOR DAMAGES OF ANY KIND
+% RESULTING FROM ITS USE. Without limiting the generality of the foregoing, 
+% neither the author, nor JMLR, nor JMLR's publishers and distributors,
+% warrant that the Source Code will be error-free, will operate without
+% interruption, or will meet the needs of the user.
+
+N=size(Graphs,2);
+Lists = cell(1,N);
+K = cell(1,h+1);
+if nargin<4 spprec=0; end
+n_nodes=0;
+% compute adjacency lists and n_nodes, the total number of nodes in the dataset
+for i=1:N
+  Lists{i}=Graphs(i).al;
+  n_nodes=n_nodes+size(Graphs(i).am,1);
+end
+% compute/copy shortest path matrices
+maxpath = 0;
+if ~spprec
+  disp('computing shortest paths...');
+  for i=1:N
+    Ds{i}=floydwarshall(Graphs(i).am);
+    aux=max(Ds{i}(~isinf(Ds{i})));
+    if aux > maxpath
+      maxpath=aux;
+    end
+  end
+else
+  for i=1:N
+    Ds{i}=Graphs(i).sp;
+    aux=max(Ds{i}(~isinf(Ds{i})));
+    if aux > maxpath
+      maxpath=aux;
+    end
+  end
+end  
+t=cputime; % for measuring runtime
+
+%%% INITIALISATION
+% initialize the node labels for each graph with their labels or 
+% with degrees (for unlabeled graphs).
+label_lookup=containers.Map();
+label_counter=uint32(1);
+% label_lookup is an associative array, which will contain the
+% mapping from multiset labels (strings) to short labels (integers)
+if nl==1
+  for i=1:N
+    % the type of labels{i} is uint32, meaning that it can only handle
+    % 2^32 labels and compressed labels over all iterations. If
+    % more is needed, switching (all occurences of uint32) to
+    % uint64 is a possibility
+    labels{i}=zeros(size(Graphs(i).nl.values,1),1,'uint32');
+    for j=1:length(Graphs(i).nl.values)
+      str_label=num2str(Graphs(i).nl.values(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        %str_label
+        %label_counter
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+    end
+  end
+else
+  for i=1:N
+    labels{i}=uint32(full(sum(Graphs(i).am,2)));
+    for j=1:length(labels{i})
+      str_label=num2str(labels{i}(j));
+      % str_label is the node label of the current node of the
+      % current graph converted into a string
+      if ~isKey(label_lookup, str_label)
+        label_lookup(str_label)=label_counter;
+        labels{i}(j)=label_counter;                
+        label_counter=label_counter+1;
+      else
+        labels{i}(j)=label_lookup(str_label);
+      end
+    end
+  end
+end
+L=double(label_counter)-1;
+clear Graphs;
+disp(['Number of original labels: ',num2str(L)]);
+disp(['Number of potential shortest path features: ',num2str((maxpath+1)*L*(L+1)/2)]);
+sp=sparse((maxpath+1)*L*(L+1)/2,N);
+for i=1:N
+  labels_aux=repmat(double(labels{i}),1,length(labels{i}));
+  a=min(labels_aux, labels_aux');
+  b=max(labels_aux, labels_aux');
+  I=triu(~(isinf(Ds{i})));
+  Ind=Ds{i}(I)*L*(L+1)/2+(a(I)-1).*(2*L+2-a(I))/2+b(I)-a(I)+1;
+  minind=min(Ind);
+  diff=max(Ind)-minind;
+  aux=accumarray(Ind,ones(nnz(I),1),[],[],[],(minind > 5000 || diff > 3000));
+  % sparse of full accumarray depending on the range of values in Ind
+  % (and based on empirical observations on the speed of accumarray)
+  sp(Ind,i)=aux(Ind);
+end
+sp=sp(sum(sp,2)~=0,:);
+K{1}=full(sp'*sp);
+
+%%% MAIN LOOP
+iter=1;
+new_labels=labels;
+while iter<=h
+  disp(['iter=',num2str(iter)]);
+  % create an empty lookup table
+  label_lookup=containers.Map();
+  label_counter=uint32(1);
+  for i=1:N
+    for v=1:length(Lists{i})
+      % form a multiset label of the node v of the i'th graph
+      % and convert it to a string
+      long_label=[labels{i}(v), sort(labels{i}(Lists{i}{v}))'];
+      long_label_2bytes=typecast(long_label,'uint16');
+      long_label_string=char(long_label_2bytes);
+      % if the multiset label has not yet occurred, add it to the
+      % lookup table and assign a number to it
+      if ~isKey(label_lookup, long_label_string)
+        label_lookup(long_label_string)=label_counter;
+        new_labels{i}(v)=label_counter;
+        label_counter=label_counter+1;
+      else
+        new_labels{i}(v)=label_lookup(long_label_string);
+      end
+    end
+  end
+  L=double(label_counter)-1;
+  disp(['Number of compressed labels: ',num2str(L)]);
+  disp(['Number of potential shortest path features: ',num2str((maxpath+1)*L*(L+1)/2)]);
+  labels=new_labels;
+  sp=sparse((maxpath+1)*L*(L+1)/2,N);
+  for i=1:N
+    labels_aux=repmat(double(labels{i}),1,length(labels{i}));
+    a=min(labels_aux, labels_aux');
+    b=max(labels_aux, labels_aux');
+    I=triu(~(isinf(Ds{i})));
+    Ind=Ds{i}(I)*L*(L+1)/2+(a(I)-1).*(2*L+2-a(I))/2+b(I)-a(I)+1;
+    minind=min(Ind);
+    diff=max(Ind)-minind;
+    aux=accumarray(Ind,ones(nnz(I),1),[],[],[],(minind > 5000 || diff > 3000));
+    % sparse of full accumarray depending on the range of values in Ind
+    % (and based on empirical observations on the speed of accumarray)
+    sp(Ind,i)=aux(Ind);
+  end
+  sp=sp(sum(sp,2)~=0,:);
+  K{iter+1}=K{iter}+full(sp'*sp);
+  iter=iter+1;
+end
+runtime=cputime-t; % computation time of K
+end
+
+function [D] = floydwarshall(A, sym, w)
+% Input: A - nxn adjacency matrix,
+%           sym - boolean, 1 if A and w symmetric
+%	    w - nxn weight matrix
+% Output: D - nxn distance matrix
+
+n = size(A,1); % number of nodes
+D=zeros(n,n);
+
+if nargin<2 % if the graph is not weighted and we have no information about sym, then 
+  sym=1;
+  w=A;
+end
+
+if nargin<3 % if the graph is not weighted, then
+  w=A;
+end
+
+D=w.*A;
+D(A+diag(repmat(Inf,n,1))==0)=Inf; 
+D=full(D.*(ones(n)-eye(n))); % set the diagonal to zero
+
+if sym % then it is a bit faster
+  for k=1:n
+    Daux=repmat(full(D(:,k)),1,n);
+    Sumdist=Daux+Daux';
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+else  
+  for k=1:n
+    Daux1=repmat(full(D(:,k)),1,n);
+    Daux2=repmat(full(D(k,:)),n,1);
+    Sumdist=Daux1+Daux2;
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/floydwarshall.m NeuroMiner-1-main.clara/graph_kernels/labeled/floydwarshall.m
--- NeuroMiner-1-main/graph_kernels/labeled/floydwarshall.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/floydwarshall.m	2012-10-26 18:35:52.000000000 +0200
@@ -0,0 +1,40 @@
+function [D] = floydwarshall(A, sym, w)
+% % Copyright 2012 Nino Shervashidze
+% Input: A - nxn adjacency matrix,
+%           sym - boolean, 1 if A and w symmetric
+%	    w - nxn weight matrix
+% Output: D - nxn distance matrix
+
+n = size(A,1); % number of nodes
+D=zeros(n,n);
+
+if nargin<2 % if the graph is not weighted and we have no information about sym, then 
+  sym=1;
+  w=A;
+end
+
+if nargin<3 % if the graph is not weighted, then
+  w=A;
+end
+
+D=w.*A; % if A(i,j)=1,  D(i,j)=w(i,j);
+D(A+diag(repmat(Inf,n,1))==0)=Inf; % If A(i,j)~=0 and i~=j D(i,j)=Inf;
+D=full(D.*(ones(n)-eye(n))); % set the diagonal to zero
+
+%t=cputime;
+if sym % then it is a bit faster
+  for k=1:n
+    Daux=repmat(full(D(:,k)),1,n);
+    Sumdist=Daux+Daux';
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+else  
+  for k=1:n
+    Daux1=repmat(full(D(:,k)),1,n);
+    Daux2=repmat(full(D(k,:)),n,1);
+    Sumdist=Daux1+Daux2;
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+end
+%cputime-t
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/l3graphletkernel.m NeuroMiner-1-main.clara/graph_kernels/labeled/l3graphletkernel.m
--- NeuroMiner-1-main/graph_kernels/labeled/l3graphletkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/l3graphletkernel.m	2012-10-26 18:56:56.000000000 +0200
@@ -0,0 +1,79 @@
+function [K,runtime] = l3graphletkernel(Graphs)
+% Compute a 3-node connected labeled graphlet kernel on a set of graphs
+% Author: Nino Shervashidze- nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+%
+% Input: Graphs - a 1xN array of graphs
+% Output: K - NxN kernel matrix K
+%         runtime - scalar
+ 
+
+N=size(Graphs,2);
+
+t=cputime; % for measuring runtime
+%%% PREPROCESSING (mainly in order to find out the size of the node
+%%% label alphabet, L, and rename labels as 1 ,..., L)
+label_lookup=containers.Map();
+label_counter=1;
+for i=1:N
+  for j=1:length(Graphs(i).nl.values)
+    str_label=num2str(Graphs(i).nl.values(j));
+    % str_label is the node label of the current node of the
+    % current graph converted into a string
+    if ~isKey(label_lookup, str_label)
+      label_lookup(str_label)=label_counter;
+      Graphs(i).nl.values(j)=label_counter;
+      label_counter=label_counter+1;
+    else
+      Graphs(i).nl.values(j)=label_lookup(str_label);
+    end
+  end
+end
+L=label_counter-1; % L is the size of the node label alphabet
+B=2*L^3; % upper bound on the size of the set of possible 
+       % connected 3-node graphlets labeled with this alphabet
+disp(['the preprocessing step took ', num2str(cputime-t), ' sec']);
+t=cputime;
+
+phi=sparse(B,N); %each column j of phi will be the explicit feature 
+		 % representation for the graph j
+graphlet_lookup=containers.Map();
+graphlet_counter=1;
+
+for g=1:N
+  % just some shorthands...
+  A=Graphs(g).am;
+  nl=Graphs(g).nl.values;
+  Al=Graphs(g).al;
+  for i=1:length(Al)
+    for j=Al{i}
+      for k=Al{j}
+        if i~=k
+          if A(i,k)==0
+            graphlet=[1,  min(nl(i),nl(k)),  nl(j),   max(nl(i),nl(k))];
+            increment=1/2;
+          else 
+            graphlet=[2,  sort([nl(i),nl(j),nl(k)])];
+            increment=1/6;
+          end
+          graphlet_2_bytes=typecast(graphlet,'uint16');
+          graphletstring=char(graphlet_2_bytes);
+          if ~isKey(graphlet_lookup, graphletstring)
+            graphlet_lookup(graphletstring)=graphlet_counter;
+            phi(graphlet_counter,g)=phi(graphlet_counter,g)+increment;
+            graphlet_counter=graphlet_counter+1;
+          else
+            phi(graphlet_lookup(graphletstring),g)= ...
+                phi(graphlet_lookup(graphletstring),g)+increment;
+          end
+        end
+      end
+    end
+  end
+end
+
+K=full(phi'*phi);
+runtime=cputime-t; % computation time of K
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/lRWkernel.m NeuroMiner-1-main.clara/graph_kernels/labeled/lRWkernel.m
--- NeuroMiner-1-main/graph_kernels/labeled/lRWkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/lRWkernel.m	2012-10-26 18:57:52.000000000 +0200
@@ -0,0 +1,137 @@
+function [K,runtime]=lRWkernel(Graphs, lambda, small)
+% Compute a random walk kernel for a set of labeled graphs
+% Copyright 2012 Karsten Borgwardt, Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+% 	 Graphs(i).am is the adjacency matrix, Graphs(i).nl.values is a column vector of node labels.
+%        lambda - scalar: a rule of thumb for setting it is to
+%                 take the largest power of 10 which is smaller than 1/d^2,
+%                 d being the largest degree in the dataset
+%        small - a boolean, indicating how "small" the dataset is.
+%                If graphs are roughly less than 100 nodes big and
+%                there are not more than 10-20 labels, then set small=1, 0
+%                otherwise. In the latter case the filtered adjacency matrices 
+%                will be computed for each pair of graphs, so the computation
+%                will be slower, but more memory-efficient.
+% Output: K - NxN kernel matrix K
+%         runtime - scalar: runtime in seconds
+
+K=[]; runtime=0;
+if nargin < 3 disp('The function requires 3 arguments'); return; end
+
+N=size(Graphs,2);
+
+t=cputime; % for measuring runtime
+
+%%% PREPROCESSING (mainly in order to find out the size of the node
+%%% label alphabet, L, and rename labels as 1 ,..., L)
+label_lookup=containers.Map();
+label_counter=1;
+for i=1:N
+  for j=1:length(Graphs(i).nl.values)
+    str_label=num2str(Graphs(i).nl.values(j));
+    % str_label is the node label of the current node of the
+    % current graph converted into a string
+    if ~isKey(label_lookup, str_label)
+      label_lookup(str_label)=label_counter;
+      Graphs(i).nl.values(j)=label_counter;
+      label_counter=label_counter+1;
+    else
+      Graphs(i).nl.values(j)=label_lookup(str_label);
+    end
+  end
+end
+L=label_counter-1; % L is the size of the node label alphabet
+labelset=[1:L];
+disp(['the preprocessing step took ', num2str(cputime-t), ' sec']);
+t=cputime;
+K=zeros(N,N);
+for i=1:N
+  %	if rem((i-1),100)==0 
+  %		disp(['filtering graph ',num2str(i)]); 
+  %	end 
+  G(i).nl.values=Graphs(i).nl.values;
+  G(i).am=Graphs(i).am;
+  if small
+    counter=1;
+    for k=1:L
+      for l=1:L
+        aux = double(Graphs(i).nl.values==labelset(k))*double(Graphs(i).nl.values==labelset(l))';
+        G(i).filteredam(counter).am=sparse(Graphs(i).am.*aux);
+        counter=counter+1;
+      end
+    end
+  end
+  
+end
+clear Graphs;
+for i=1:N
+  for j=i:N
+    %disp(['processing pair ',num2str(i),',',num2str(j)]);
+    K(i,j)=K(i,j)+labeledrandomwalk(G(i),G(j),labelset,lambda, small);
+    K(j,i)=K(i,j);
+  end
+end
+runtime=cputime-t;
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
+
+function result = labeledrandomwalk(g1,g2,labelset,lambda, small)
+L=size(labelset,2);
+if ~small
+  %g1.am is the adjacency matrix of graph g1
+  %g2.am is the adjacency matrix of graph g2
+  counter=1;
+  for k=1:L
+    for l=1:L
+      aux = double(g1.nl.values==labelset(k))*double(g1.nl.values==labelset(l))';
+      g1.filteredam(counter).am=sparse(g1.am.*aux);
+      counter=counter+1;
+    end
+  end
+  counter=1;
+  for k=1:L
+    for l=1:L
+      aux = double(g2.nl.values==labelset(k))*double(g2.nl.values==labelset(l))';
+      g2.filteredam(counter).am=sparse(g2.am.*aux);
+      counter=counter+1;
+    end
+  end
+end
+
+am1 = g1.filteredam; % filtered version of g1, where am1(1).am is the first
+                     % filtered am, am1(2).am is the second filtered am, etc
+
+am2 = g2.filteredam; % filtered version of g2, where am2(1).am is the first
+                     % filtered am, am2(2).am is the second filtered am, etc
+
+
+%lambda = 10e-2;
+
+[x,rubbish] = pcg(@(x)smtfilter(x,am1,am2,lambda),ones(size(g1.am,1)*size(g2.am,1),1),1e-6,20);
+result =  sum(sum(x));
+
+end
+
+function result = smtfilter(x,am1,am2,lambda)
+
+yy = 0;
+for i = 1:size(am1,2)
+  yy = yy + vec(am1(i).am * invvec(x,size(am1(i).am,1),size(am2(i).am,1)) * am2(i).am);
+end
+
+yy = lambda * yy;
+
+vecu = minus(x,yy);
+
+result = vecu;
+end
+
+function v = vec(M)
+[m,n] = size(M);
+v = reshape(M,m*n,1);
+end
+
+function v = invvec(M,m,n)
+v = reshape(M,m,n);
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/spkernel.m NeuroMiner-1-main.clara/graph_kernels/labeled/spkernel.m
--- NeuroMiner-1-main/graph_kernels/labeled/spkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/spkernel.m	2014-07-21 13:15:52.000000000 +0200
@@ -0,0 +1,70 @@
+function [K, runtime, Phi] = spkernel(Graphs, features)
+% Compute the labeled shortest path kernel for a set of node-labeled graphs
+% Copyright 2012 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+%        features - a boolean: 1 if we want to output the feature
+%                   vector representation for each graph, 0 otherwise
+% Output: K - NxN kernel matrix K
+%         runtime - scalar
+%         Phi (if features = 1) - a NFxN times sparse matrix, where NF is the
+%               number of features, containing the feature vector representation
+%               of each graph
+
+
+N=size(Graphs,2);
+Ds = cell(1,N); % shortest path distance matrices for each graph
+if nargin<2 features=0; end
+Phi = [];
+
+t=cputime; % for measuring runtime
+
+%%% PREPROCESSING (mainly in order to find out the size of the node
+%%% label alphabet, L, and rename labels as 1 ,..., L)
+label_lookup=containers.Map();
+label_counter=1;
+for i=1:N
+  for j=1:length(Graphs(i).nl.values)
+    str_label=num2str(Graphs(i).nl.values(j));
+    % str_label is the node label of the current node of the
+    % current graph converted into a string
+    if ~isKey(label_lookup, str_label)
+      label_lookup(str_label)=label_counter;
+      Graphs(i).nl.values(j)=label_counter;
+      label_counter=label_counter+1;
+    else
+      Graphs(i).nl.values(j)=label_lookup(str_label);
+    end
+  end
+end
+L=label_counter-1; % L is the size of the node label alphabet
+disp(['the preprocessing step took ', num2str(cputime-t), ' sec']);
+t=cputime;
+
+% compute Ds and the length of the maximal shortest path over the dataset
+maxpath=0;
+for i=1:N
+  Ds{i}=floydwarshall(Graphs(i).am);
+  aux=max(Ds{i}(~isinf(Ds{i})));
+  if aux > maxpath
+    maxpath=aux;
+  end
+ % if rem(i,100)==0 disp(i); end
+end
+
+sp=sparse((maxpath+1)*L*(L+1)/2,N);
+for i=1:N
+  labels_aux=repmat(Graphs(i).nl.values,1,length(Graphs(i).nl.values));
+  a=min(labels_aux, labels_aux');
+  b=max(labels_aux, labels_aux');
+  I=triu(~(isinf(Ds{i})));
+  Ind=Ds{i}(I)*L*(L+1)/2+(a(I)-1).*(2*L+2-a(I))/2+b(I)-a(I)+1;
+  aux=accumarray(Ind,ones(nnz(I),1));
+  sp(Ind,i)=aux(Ind);
+end
+sp=sp(sum(sp,2)~=0,:);
+K=full(sp'*sp);
+if features Phi=sp; end
+runtime=cputime-t;
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/labeled/untilpRWkernel.m NeuroMiner-1-main.clara/graph_kernels/labeled/untilpRWkernel.m
--- NeuroMiner-1-main/graph_kernels/labeled/untilpRWkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/labeled/untilpRWkernel.m	2012-10-26 18:59:02.000000000 +0200
@@ -0,0 +1,137 @@
+function [K,runtime]=untilpRWkernel(Graphs, p, small, intermediate, name)
+% Compute a labeled p-step random walk kernel for a set of graphs
+% Copyright 2012 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs
+% 	 Graphs(i).am is the adjacency matrix, Graphs(i).nl.values is a column vector of node labels.
+%        p - integer scalar: the length up to which we consider all walks
+%        small - a boolean, indicating how "small" is the dataset.
+%                If graphs are roughly less than 100 nodes big and
+%                there are not more than 10-20 labels, then set small=1, 0
+%                otherwise. In the latter case the filtered adjacency matrices 
+%                will be computed for each pair of graphs, so the computation
+%                will be slower, but more memory-efficient.
+%        intermediate (MAKES SENSE ONLY IF small=1) - a boolean: 1
+%                     if we want to output kernel matrices
+%                     corresponding to k=1..p, 0 otherwise (default: 0)
+%        name (MAKES SENSE ONLY IF small=1) - a string: the name of
+%                     files where the kernel matrices will be
+%                     exported to (with suffixes _1,...,_p) (default: [])
+% Output: K - NxN kernel matrix K
+%         runtime - scalar: runtime in seconds
+
+K=[]; runtime=0;
+if nargin < 3 disp('The function requires 3 arguments'); return; end
+if nargin < 4 intermediate=0; end
+if nargin < 5 name=''; intermediate=0; end % if we do not know where
+                                           % to put intermediate
+                                           % matrices, we do not
+                                           % output them
+
+N=size(Graphs,2);
+
+t=cputime; % for measuring runtime
+
+%%% PREPROCESSING (mainly in order to find out the size of the node
+%%% label alphabet, L, and rename labels as 1 ,..., L)
+label_lookup=containers.Map();
+label_counter=1;
+for i=1:N
+  for j=1:length(Graphs(i).nl.values)
+    str_label=num2str(Graphs(i).nl.values(j));
+    % str_label is the node label of the current node of the
+    % current graph converted into a string
+    if ~isKey(label_lookup, str_label)
+      label_lookup(str_label)=label_counter;
+      Graphs(i).nl.values(j)=label_counter;
+      label_counter=label_counter+1;
+    else
+      Graphs(i).nl.values(j)=label_lookup(str_label);
+    end
+  end
+end
+L=label_counter-1; % L is the size of the node label alphabet
+labelset=[1:L];
+disp(['the preprocessing step took ', num2str(cputime-t), ' sec']);
+t=cputime;
+
+K=zeros(N,N);
+for i=1:N
+  %	if rem((i-1),100)==0 
+  %		disp(['filtering graph ',num2str(i)]); 
+  %	end 
+  G(i).nl.values=Graphs(i).nl.values;
+  G(i).am=Graphs(i).am;
+
+  if small
+    counter=1;
+    for k=1:L
+      for l=1:L
+        aux = double(Graphs(i).nl.values==labelset(k))*double(Graphs(i).nl.values==labelset(l))';
+        G(i).filteredam(counter).am=sparse(Graphs(i).am.*aux);
+        counter=counter+1;
+      end
+    end
+  end
+  
+end
+clear Graphs;
+
+L2=L^2;
+if small
+  for k=1:p
+    disp(['p = ',num2str(k)]);
+    for l=1:L2
+   %   disp(['l = ', num2str(l)]);
+      for i=1:N
+        x=sum((G(i).filteredam(l).am)^k,2);
+        for j=i:N
+%          disp(['processing pair ',num2str(i),',',num2str(j)]);
+          y=sum((G(j).filteredam(l).am)^k,1);
+          K(i,j)=K(i,j)+sum(sum(x*y));
+          K(j,i)=K(i,j);
+        end
+      end
+    end
+    if intermediate
+      runtime=cputime-t;
+      save([name,'_', num2str(k)], 'K','runtime');
+    end
+  end
+else
+  for i=1:N
+    g1=G(i);
+    counter=1;
+    for k=1:L
+      for l=1:L
+        aux = double(g1.nl.values==labelset(k))*double(g1.nl.values==labelset(l))';
+        g1.filteredam(counter).am=sparse(g1.am.*aux);
+        counter=counter+1;
+      end
+    end
+    for j=i:N
+%     disp(['processing pair ',num2str(i),',',num2str(j)]);
+      g2=G(j);
+      counter=1;
+      for k=1:L
+        for l=1:L
+          aux = double(g2.nl.values==labelset(k))*double(g2.nl.values==labelset(l))';
+          g2.filteredam(counter).am=sparse(g2.am.*aux);
+          counter=counter+1;
+        end
+      end
+      for l=1:L2
+        for k=1:p
+%         disp(['p = ',num2str(k)]);
+          x=sum((g1.filteredam(l).am)^k,2);
+          y=sum((g2.filteredam(l).am)^k,1);
+          K(i,j)=K(i,j)+sum(sum(x*y));
+          K(j,i)=K(i,j);
+        end
+      end
+    end
+  end
+end  
+    
+runtime=cputime-t;
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/README NeuroMiner-1-main.clara/graph_kernels/svm/README
--- NeuroMiner-1-main/graph_kernels/svm/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/README	2012-10-26 19:18:52.000000000 +0200
@@ -0,0 +1,11 @@
+runmultiplesvm - cross-validate over different kernels and the parameter C of SVM and repeat the experiment n times with different folds. 
+                 This is used with the output of all WL* scripts in the folder "labeled" - a cell array of kernel matrices. To be able 
+                 to use it, you have to replace "~/code/libsvm" in runsvm.m by the name of the folder where you put the compiled mex files 
+                 (svmtrain and svmpredict) of the libSVM Matlab interface.
+
+runntimes      - cross-validate over the parameter C of SVM and repeat the experiment n times with different folds.
+                 This is used with a single kernel matrix. To be able to use it, you have to replace "~/code/libsvm" 
+                 in runIndependent.m by the name of the folder where you put the compiled mex files 
+                 (svmtrain and svmpredict) of the libSVM Matlab interface.
+
+To obtain information on input and output, run "help runmultiplesvm" and "help runntimes" respectively.
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/normalizekm.m NeuroMiner-1-main.clara/graph_kernels/svm/normalizekm.m
--- NeuroMiner-1-main/graph_kernels/svm/normalizekm.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/normalizekm.m	2012-10-26 19:04:40.000000000 +0200
@@ -0,0 +1,19 @@
+function result = normalizekm(km)
+% Copyright 2012 Nino Shervashidze, Karsten Borgwardt
+% normalizes kernelmatrix km 
+% such that diag(result) = 1, i.e. K(x,y) / sqrt(K(x,x) * K(y,y))
+% @author Karsten Borgwardt
+% @date June 3rd 2005
+% all rights reserved 
+
+nv = sqrt(diag(km));
+nm =  nv * nv';
+knm = nm .^ -1;
+for i = 1:size(knm,1)
+for j = 1:size(knm,2)
+if (knm(i,j) == Inf)
+  knm(i,j) = 0;
+end
+end
+end
+result = km .* knm;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/runIndependent.m NeuroMiner-1-main.clara/graph_kernels/svm/runIndependent.m
--- NeuroMiner-1-main/graph_kernels/svm/runIndependent.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/runIndependent.m	2012-10-26 19:04:36.000000000 +0200
@@ -0,0 +1,122 @@
+function [result,mean_accuracy,std_accuracy] = runIndependent(K,lk)
+% Copyright 2012 Nino Shervashidze, Karsten Borgwardt
+% K = kernel matrix (n*n)
+% lk = vector of labels (n*1)
+% cv = number of folds in cross-validation
+
+% standard deviation
+% independent scheme
+% best c
+
+addpath('~/code/libsvm');
+% randomly permute kernel matrix and labels
+r = randperm(size(K,1));
+K = K(r,r);
+lk = lk(r);
+lk=lk';
+lkoriginal = lk; 
+Koriginal = K;
+
+%% stratified cross-validation
+%sum(sum(Koriginal));
+%neworder = stratifiedsplit(lk)
+%for i = 1:size(neworder,2) 
+%m = size(neworder(i).old,1);
+%r = randperm(m);
+%newlk(neworder(i).new) =  lk(neworder(i).old(r));
+%Knew([neworder(i).new]',[neworder(i).new]') = K([neworder(i).old(r)]',[neworder(i).old(r)]');  
+%end
+%
+%sum(sum(Knew)) - sum(sum(Koriginal))
+%dbstop
+%lk = newlk'
+%K = Knew;
+%size(lk);
+%size(K);
+%dbstop 
+%Koriginal = K;
+
+
+% bring kernel matrix into libsvm format
+p80 = ceil(size(K,2) * 0.8);
+p90 = ceil(size(K,2) * 0.9);
+
+% specify range of c-values
+cvalues = (10 .^ [-7:2:7]) / size(K,2);
+
+cv = 10;
+fs = size(K,2) - p90;
+
+% cross-validation loop
+for k = 1:cv
+K = Koriginal;
+lk = lkoriginal;
+
+K = K([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs],[k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]);  
+lk = lk([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]); 
+K = makepos(K);
+K1 = [(1:size(K,1))', normalizekm(K)];
+
+%if any(strcmp('optimal',options))
+imresult=[];
+for i = 1:size(cvalues,2)
+    % train on 80%, predict on 10% (from 81% to 90%) 
+  size(lk(1:p80));
+  size(K1(1:p80,1:p80+1));
+  model = svmtrain(lk(1:p80,1), K1(1:p80,1:p80+1), strcat(['-t 4  -c ' num2str(cvalues(i))]));
+  [predict_label, accuracy, dec_values] = svmpredict(lk(p80+1:p90,1),K1(p80+1:p90,1:p80+1), model);
+  accuracy80 = accuracy;
+  imresult(i)= accuracy(1);
+end
+  
+  
+  % determine optimal c
+  [junk,optimalc]= max(fliplr(imresult));
+  optimalc = size(cvalues,2)+1 - optimalc; 
+  % train on 90% with optimal c, predict on 10% (from 91% to 100%)
+  model = svmtrain(lk(1:p90,1), K1(1:p90,1:p90+1),strcat(['-t 4  -c ' num2str(cvalues(optimalc))]) );
+  [predict_label, accuracy, dec_values] = svmpredict(lk(p90+1:size(K,1),1), K1(p90+1:size(K,1),1:p90+1), model);
+  accuracy90 = accuracy
+  result(k)=accuracy(1)
+
+
+end  
+mean_accuracy =  mean(result) 
+std_accuracy = std(result)
+
+
+end
+
+%
+%% cross-validation
+%if any(strcmp('cv',options))
+%options = strcat(['-t 4 -v ' num2str(cv) ' -c ' num2str(cvalues(i))])
+%result(i) = svmtrain(lk, K1, options); %', num2str(cv)));
+%end
+%
+%end
+
+function result = makepos(K)
+pd = 0;
+addc = 10e-7;
+while (pd ==  0)
+  
+  addc = addc * 10
+  try
+    if (isinf(addc) == 1)
+      pd = 1;
+    else 
+      chol(normalizekm(K + eye(size(K,1),size(K,1)) * addc));
+      pd = 1;
+    end
+  catch
+    
+  end
+  
+end
+if (isinf(addc)==0)
+  result = K + eye(size(K,1),size(K,1)) * addc;
+else
+  result = eye(size(K,1));
+end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/runmultiplesvm.m NeuroMiner-1-main.clara/graph_kernels/svm/runmultiplesvm.m
--- NeuroMiner-1-main/graph_kernels/svm/runmultiplesvm.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/runmultiplesvm.m	2012-10-26 19:04:30.000000000 +0200
@@ -0,0 +1,18 @@
+function res = runmultiplesvm(Ks,lk,n)
+% Copyright 2012 Nino Shervashidze, Karsten Borgwardt
+% Input: Ks - cell array of h  m x m kernel matrices
+%        lk - m x 1 array of class labels
+%        n - number of times we want to run svm
+% Output: res is a 1 x n+1 array of structures. Each of the first n
+%         elements contains fields optkernel, optc, accuracy, mean_acc and std_acc,
+%         and res(n+1) has only two fields - the mean and the std of the n
+%         mean_acc's
+%
+s=0;
+for i=1:n
+  res(i)=runsvm(Ks,lk);
+  meanacc(i) = res(i).mean_acc;
+end
+res(n+1).mean_acc = mean(meanacc);
+res(n+1).std_acc = std(meanacc);
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/runntimes.m NeuroMiner-1-main.clara/graph_kernels/svm/runntimes.m
--- NeuroMiner-1-main/graph_kernels/svm/runntimes.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/runntimes.m	2012-10-26 19:04:32.000000000 +0200
@@ -0,0 +1,25 @@
+function result = runntimes(K,lk,n)
+% Copyright 2012 Nino Shervashidze, Karsten Borgwardt
+% Input: K  - m x m kernel matrix
+%        lk - m x 1 array of class labels
+%        n - number of times we want to run svm
+% Output: result - a structure with fields accuracy, mean, std and
+%                  mean, std and se are the mean, the standard
+%                  deviation and the standard error of accuracy
+
+accuracy = zeros(n,1);
+
+for i = 1:n
+[junk1, accuracy(i), junk2] = runIndependent(K, lk)
+
+end
+
+result.mean = mean(accuracy)
+
+result.std = std(accuracy)
+
+result.se = result.std / sqrt(n)
+
+result.accuracy = accuracy
+
+result
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/svm/runsvm.m NeuroMiner-1-main.clara/graph_kernels/svm/runsvm.m
--- NeuroMiner-1-main/graph_kernels/svm/runsvm.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/svm/runsvm.m	2012-10-26 19:04:20.000000000 +0200
@@ -0,0 +1,101 @@
+function [res] = runsvm(Ks,lk)
+% Copyright 2012 Nino Shervashidze, Karsten Borgwardt
+% runsvm(Ks,lk)
+% K = 1 x h cell array of kernelmatrices (n*n)
+% lk = vector of class labels (n*1)
+% cv = number of folds in cross-validation
+
+% independent scheme
+% best c
+
+addpath('~/code/libsvm');
+n=length(lk) % size of the dataset
+% randomly permute labels: r will be also used for permuting the kernel matrices
+r = randperm(n);
+lk = lk(r);
+
+% specify range of c-values
+cvalues = (10 .^ [-7:2:7]) / size(lk,1);
+
+cv = 10;
+p80 = ceil(n * (1-2/cv));
+p90 = ceil(n * (1-1/cv));
+fs = n - p90; % fold size
+
+
+% output variables
+res.optkernel=zeros(cv,1);
+res.optc=zeros(cv,1);
+res.accuracy=zeros(cv,1);
+
+% cross-validation loop
+opth=zeros(1,cv);
+for k = 1:cv
+  imresult=[];
+  
+  height = length(Ks);
+  for h=1:height
+    K = Ks{h}(r,r);
+    K_current = K([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs],[k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]);  
+    lk_current = lk([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]); 
+    K_current = makepos(K_current);
+    K1 = [(1:size(K_current,1))', normalizekm(K_current)];
+    
+    
+    for i = 1:size(cvalues,2)
+      % train on 80%, predict on 10% (from 81% to 90%) 
+      size(lk_current(1:p80));
+      size(K1(1:p80,1:p80+1));
+      model = svmtrain(lk_current(1:p80,1), K1(1:p80,1:p80+1), strcat(['-t 4  -c ' num2str(cvalues(i))]));
+      [predict_label, accuracy, dec_values] = svmpredict(lk_current(p80+1:p90,1),K1(p80+1:p90,1:p80+1), model);
+      imresult(h,i)= accuracy(1);
+    end
+  end
+  
+  % determine optimal h and c
+  [junk,position]= max(imresult(:));
+  [optimalh, indoptimalc]=ind2sub(size(imresult),position);
+  
+  opth(k)=optimalh;
+  res.optc(k)= cvalues(indoptimalc);
+  res.optkernel(k)=optimalh;
+  % train on 90% with optimal c, predict on 10% (from 91% to 100%)
+  K = Ks{optimalh}(r,r);
+  K_current = K([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs],[k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]);  
+  lk_current = lk([k*fs+1:size(K,2),1:(k-1)*fs,(k-1)*fs+1:k*fs]); 
+  K_current = makepos(K_current);
+  K1 = [(1:size(K_current,1))', normalizekm(K_current)];
+  
+  model = svmtrain(lk_current(1:p90,1), K1(1:p90,1:p90+1),strcat(['-t 4  -c ' num2str(cvalues(indoptimalc))]) );
+  [predict_label, accuracy, dec_values] = svmpredict(lk_current(p90+1:size(K,1),1), K1(p90+1:size(K,1),1:p90+1), model);
+  res.accuracy(k)=accuracy(1)
+end
+res.mean_acc =  mean(res.accuracy) 
+res.std_acc = std(res.accuracy)
+end
+
+
+function result = makepos(K)
+pd = 0;
+addc = 10e-7;
+while (pd ==  0)
+ 
+addc = addc * 10
+try
+if (isinf(addc) == 1)
+pd = 1;
+else 
+chol(normalizekm(K + eye(size(K,1),size(K,1)) * addc));
+pd = 1;
+end
+catch
+
+end
+
+end
+if (isinf(addc)==0)
+result = K + eye(size(K,1),size(K,1)) * addc;
+else
+result = eye(size(K,1));
+end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/README NeuroMiner-1-main.clara/graph_kernels/unlabeled/README
--- NeuroMiner-1-main/graph_kernels/unlabeled/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/README	2012-10-26 18:18:04.000000000 +0200
@@ -0,0 +1,11 @@
+This folder contains Matlab scripts for computing various kernels for unweighted unlabeled graphs. These are:
+- the random walk kernel (RWkernel.m) 
+- the shortest path kernel (SPkernel.m) 
+- all 3 and 4-node graphlet kernels (allkernel.m in allgraphlets folder)
+- all 3,4,5-node graphlet kernels that use sampling (gestkernelK.m for K={3,4,5} in samplinggraphlets folder)
+- connected 3,4,5-node graphlet kernels (connectedkernel.m in connectedgraphlets folder)
+
+The other scripts are auxiliary.
+
+Typing "help script_name" will give you information about the input and output of the corresponding script.
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/RWkernel.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/RWkernel.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/RWkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/RWkernel.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,60 @@
+function [K,runtime]=RWkernel(Graphs, lambda)
+% Compute a random walk kernel for a set of graphs
+% Copyright 2011 Karsten Borgwardt, Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs represented just with adjacency matrices
+% 	 Graphs(i).am is the i'th adjacency matrix
+%        Graphs(i) may have other fields, but they will not be considered by this script
+%        lambda - scalar<1: a rule of thumb for setting it is to
+%                 take the largest power of 10 which is smaller than 1/d^2,
+%                 d being the largest degree in the dataset
+% Output: K - NxN kernel matrix K
+%         runtime - scalar: runtime in seconds
+
+K=[]; runtime=0;
+if nargin < 2 disp('The function requires 2 arguments'); return; end
+
+N=size(Graphs,2);
+K=zeros(N,N);
+
+t=cputime; % for measuring runtime
+
+for i=1:N
+  for j=i:N
+    K(i,j)=K(i,j)+randomwalk(Graphs(i),Graphs(j),lambda);
+    K(j,i)=K(i,j);
+  end
+end
+runtime=cputime-t;
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
+
+function result = randomwalk(g1,g2,lambda)
+
+am1 = g1;
+am2 = g2;
+
+[x,rubbish] = pcg(@(x)smtfilter(x,am1,am2,lambda),ones(size(g1.am,1)*size(g2.am,1),1),1e-6,20);
+result =  sum(sum(x));
+
+end
+
+function result = smtfilter(x,am1,am2,lambda)
+
+yy = vec(am1.am * invvec(x,size(am1.am,1),size(am2.am,1)) * am2.am);
+
+yy = lambda * yy;
+
+vecu = minus(x,yy);
+
+result = vecu;
+end
+
+function v = vec(M)
+[m,n] = size(M);
+v = reshape(M,m*n,1);
+end
+
+function v = invvec(M,m,n)
+v = reshape(M,m,n);
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/SPkernel.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/SPkernel.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/SPkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/SPkernel.m	2014-07-21 13:13:38.000000000 +0200
@@ -0,0 +1,48 @@
+function [K, runtime, Phi] = SPkernel(Graphs, features)
+% Compute the shortest path kernel for a set of graphs (by exact matching
+% of shortest path lengths)
+% Copyright 2012 Nino Shervashidze
+% Input: Graphs - a 1xN array of graphs represented just with adjacency matrices
+% 	 Graphs(i).am is the i'th adjacency matrix
+%        Graphs(i) may have other fields, but they will not be considered by this script
+%        features - a boolean: 1 if we want to output the feature
+%                   vector representation for each graph, 0 otherwise
+% Output: K - nxn kernel matrix K
+%         runtime - scalar
+%         Phi (if features = 1) - a NFxN times sparse matrix, where NF is the number
+%               of features (shortest path lengths), containing the feature vector
+%               representation of each graph
+
+N=size(Graphs,2);
+Ds = cell(1,N); % shortest path distance matrices for each graph
+if nargin<2 features=0; end
+Phi = [];
+
+t=cputime; % for measuring runtime
+
+% compute Ds and the length of the maximal shortest path over the dataset
+maxpath=0;
+for i=1:N
+  Ds{i}=floydwarshall(Graphs(i).am);
+  aux=max(Ds{i}(~isinf(Ds{i})));
+  if aux > maxpath
+    maxpath=aux;
+  end
+ % if rem(i,100)==0 disp(i); end
+end
+disp(['the preprocessing step took ', num2str(cputime-t), ' sec']);
+
+sp=sparse((maxpath+1),N);
+for i=1:N
+  I=triu(~(isinf(Ds{i})));
+  Ind=Ds{i}(I)+1; % some shortest paths will equal 0, so we have to
+                  % add 1 to use them as indices of features
+  aux=accumarray(Ind,ones(nnz(I),1));
+  sp(Ind,i)=aux(Ind);
+end
+K=full(sp'*sp);
+if features Phi=sp; end
+runtime=cputime-t;
+disp(['kernel computation took ', num2str(cputime-t), ' sec']);
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/allkernel.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/allkernel.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/allkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/allkernel.m	2012-10-26 18:48:10.000000000 +0200
@@ -0,0 +1,40 @@
+function [K,runtime] = allkernel(Graphs,k)
+
+% Compute all k-node graphlet kernel for a set of graphs
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+% Input: Graphs - a 1xn array of graphs
+%        k - the size of considered graphlets - 3, 4
+% Output: K - nxn kernel matrix K
+%         runtime - scalar
+
+n=size(Graphs,2);
+switch k
+ case 3
+  freq=zeros(n,4);
+ case 4
+  freq=zeros(n,11);
+ case 5
+  freq=zeros(n,34);
+end
+
+t=cputime; % for measuring runtime
+switch k
+  case 3
+   for i=1:n
+     freq(i,:)=countall3graphlets(Graphs(i).al);
+     sum_freq=sum(freq(i,:));
+     if sum_freq~=0 freq(i,:)=freq(i,:)/sum_freq; end 	
+   end
+  case 4
+   for i=1:n
+     freq(i,:)=countall4graphlets(Graphs(i).al);
+     sum_freq=sum(freq(i,:));
+     if sum_freq~=0 freq(i,:)=freq(i,:)/sum_freq; end 	
+   end
+end
+K=freq*freq';
+
+runtime=cputime-t; % computation time of K
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/card_3inter.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/card_3inter.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/card_3inter.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/card_3inter.m	2012-10-26 18:48:10.000000000 +0200
@@ -0,0 +1,131 @@
+function [n] = card_3inter(L1, L2, L3, l1, l2, l3)
+
+% count cardinalities of subsets L1&L2&L3 (n(7)), L1&L2\L3 (n(4)), L1&L3\L2 (n(5)), L2&L3\L1 (n(6)), 
+% L1\(L2|L3) (n(1)), L2\(L1|L3) (n(2)), L3\(L1|L2) (n(3))
+% Li - a set
+% li - number of elements in Li
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+
+n=zeros(1,7);
+i=1; j=1; k=1;
+
+while i<=l1 && j <=l2 && k<=l3
+  m=find_min(L1(i), L2(j), L3(k));
+  n(m)=n(m)+1;
+  switch m
+   case 1
+    i=i+1;
+   case 2
+    j=j+1;
+   case 3
+    k=k+1;
+   case 4
+    i=i+1; j=j+1;
+   case 5
+    i=i+1; k=k+1;
+   case 6
+    j=j+1; k=k+1;
+   case 7
+    i=i+1; j=j+1; k=k+1;
+  end
+end
+%[i j k]
+if i>l1 && j>l2 && k>l3
+else 
+  if i>l1 && j>l2
+    n(3)=n(3)+l3-k+1; k=l3+1;
+  else 
+    if i>l1 && k>l3
+      n(2)=n(2)+l2-j+1; j=l2+1;
+    else 
+      if j>l2 && k>l3
+        n(1)=n(1)+l1-i+1; i=l1+1;
+      else 
+        if i>l1
+          while j<=l2 && k<=l3
+            if L2(j)<L3(k) n(2)=n(2)+1; j=j+1;	
+            else 
+              if L2(j)>L3(k) n(3)=n(3)+1; k=k+1;
+              else n(6)=n(6)+1; j=j+1; k=k+1;
+              end
+            end
+          end
+        else 
+          if j>l2
+            while i<=l1 && k<=l3
+              if L1(i)<L3(k) n(1)=n(1)+1; i=i+1;	
+              else 
+                if L1(i)>L3(k) n(3)=n(3)+1; k=k+1;
+                else n(5)=n(5)+1; i=i+1; k=k+1;
+                end
+              end
+            end
+          else 
+            if k>l3
+              while i<=l1 && j<=l2
+                if L1(i)<L2(j) n(1)=n(1)+1; i=i+1;	
+                else
+                  if L1(i)>L2(j) n(2)=n(2)+1; j=j+1;
+                  else n(4)=n(4)+1; i=i+1; j=j+1;
+                  end
+                end
+              end
+            end
+          end
+        end
+      end
+    end
+  end
+end
+
+
+if i>l1 && j>l2 && k>l3
+else 
+  if i>l1 && j>l2
+    n(3)=n(3)+l3-k+1;
+  else 
+    if i>l1 && k>l3
+      n(2)=n(2)+l2-j+1;
+    else
+      if j>l2 && k>l3
+        n(1)=n(1)+l1-i+1;
+      end
+    end
+  end
+end
+
+end
+
+function [m]=find_min(a,b,c)
+% auxiliary function: determine minimum (minima) from the set a,b,c 
+% return 7 if a=b=c, 6 if b=c<a, 5 if a=c<b, 4 if a=b<c, 3 if c<a & c<b, 2 if b<a & b<c, 1 if a<b & a<c.
+
+mini=a;
+if b<mini mini=b; end
+if c<mini mini=c; end
+
+if mini==a
+  if mini==b
+    if mini==c
+      m=7;
+    else m=4;
+    end
+  else 
+    if mini==c
+      m=5;
+    else m=1;
+    end
+  end
+else 
+  if mini==b
+    if mini==c
+      m=6;
+    else m=2;
+    end
+  else m=3;
+  end
+end
+
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/countall3graphlets.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/countall3graphlets.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/countall3graphlets.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/countall3graphlets.m	2012-10-26 18:48:10.000000000 +0200
@@ -0,0 +1,41 @@
+function [count] = countall3graphlets(L)
+
+% Count all 3-node subgraphs in an undirected graph
+% without node labels and with unweighted edges
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+% Input: L - 1xn cell array - adjacency list
+% Output: count - 1x4 vector of integers. count(i)= number of graphlets with 
+%                 3-i+1 edges (see 3graphlets.pdf)
+
+n=length(L); % number of nodes
+count=zeros(1,4);
+w=[1/6, 1/4, 1/2];
+for v1=1:n
+  for v2=L{v1}
+    cardinalities=card_inter(L{v1}, L{v2}, length(L{v1}), length(L{v2}));
+    count(1)=count(1)+w(1)*cardinalities(3);
+    count(2)=count(2)+w(2)*(cardinalities(1)+cardinalities(2)-2);
+    count(3)=count(3)+w(3)*(n-sum(cardinalities));
+  end
+end
+count(4)=n*(n-1)*(n-2)/6-sum(count(1:3));
+end
+
+function [n] = card_inter(o_set1, o_set2, l1, l2)
+% Find the cardinality of the intersection of two ordered sets of lengths l1 and l2 respectively
+% n(1)=o_set1\o_set2, n(2)=o_set2\o_set1, n(3)=(o_set2 inter o_set1)
+n=zeros(1,3);
+i=1; j=1;
+
+while i<=l1 && j <=l2
+  if o_set1(i)<o_set2(j) n(1)=n(1)+1; i=i+1;
+  else
+    if o_set1(i)>o_set2(j) n(2)=n(2)+1; j=j+1;
+    else i=i+1; j=j+1; n(3)=n(3)+1;
+    end
+  end
+end
+n(1)=n(1)+l1-i+1;
+n(2)=n(2)+l2-j+1;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/countall4graphlets.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/countall4graphlets.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/allgraphlets/countall4graphlets.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/allgraphlets/countall4graphlets.m	2012-10-26 18:48:10.000000000 +0200
@@ -0,0 +1,121 @@
+function [count] = countall4graphlets(L)
+
+% Count all 4-node subgraphs in an undirected graph
+% without node labels and with unweighted edges
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze 
+% Input:    L - 1xn cell array - corresponding adjacency list
+% Output:   count - 1x11 vector of integers. count(1) is the number of occurences of the
+%		    graphlet with 4 nodes and 6 edges (type 1), count(2) is the number of 
+%		    occurences of the graphlet with 4 nodes and 5 edges (type 2) and so
+%		    forth (see 4graphlets.pdf)
+
+
+
+w = [1/12, 1/10, 1/8, 1/6, 1/8, 1/6, 1/6, 1/4, 1/4, 1/2, 0];
+
+n = length(L); % number of nodes
+count = zeros(1,11);
+
+%precompute the number of edges
+m=0;
+for i=1:n
+  m=m+length(L{i});
+end
+m=m/2;
+
+for v1=1:n
+  for v2=L{v1}
+    K=0;
+    internalcount=zeros(1,11);
+    [inter, diff1, diff2]=card_inter(L{v1},L{v2},length(L{v1}),length(L{v2})); 
+    for v3=inter
+      cardinalities=card_3inter(L{v1},L{v2},L{v3},length(L{v1}),...
+                                length(L{v2}), length(L{v3}));
+      internalcount(1)=internalcount(1)+1/2*cardinalities(7);
+      internalcount(2)=internalcount(2)+1/2*(cardinalities(4)-1);
+      internalcount(2)=internalcount(2)+1/2*(cardinalities(5)-1);
+      internalcount(2)=internalcount(2)+1/2*(cardinalities(6)-1);
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(1));
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(2));
+      internalcount(3)=internalcount(3)+cardinalities(3);
+      internalcount(7)=internalcount(7)+ (n-sum(cardinalities));
+      K=K+1/2*cardinalities(7)+1/2*(cardinalities(5)-1)+1/2*(cardinalities(6)-1)+cardinalities(3);
+    end
+    for v3=setdiff(diff1,v2)
+      cardinalities=card_3inter(L{v1},L{v2},L{v3},length(L{v1}),...
+                                length(L{v2}), length(L{v3}));
+      internalcount(2)=internalcount(2)+1/2*(cardinalities(7));
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(4));
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(5));
+      internalcount(5)=internalcount(5)+1/2*(cardinalities(6)-1);
+      internalcount(4)=internalcount(4)+1/2*(cardinalities(1)-2);
+      internalcount(6)=internalcount(6)+1/2*(cardinalities(2));
+      internalcount(6)=internalcount(6)+cardinalities(3);
+      internalcount(8)=internalcount(8)+(n-sum(cardinalities));
+      K=K+1/2*(cardinalities(7))+1/2*(cardinalities(5))+1/2*(cardinalities(6)-1)+cardinalities(3);
+    end
+    for v3=setdiff(diff2,v1)
+      cardinalities=card_3inter(L{v1},L{v2},L{v3},length(L{v1}),...
+                                length(L{v2}), length(L{v3}));
+      internalcount(2)=internalcount(2)+1/2*(cardinalities(7));
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(4));
+      internalcount(5)=internalcount(5)+1/2*(cardinalities(5)-1);
+      internalcount(3)=internalcount(3)+1/2*(cardinalities(6));
+      internalcount(6)=internalcount(6)+1/2*(cardinalities(1));
+      internalcount(4)=internalcount(4)+1/2*(cardinalities(2)-2);
+      internalcount(6)=internalcount(6)+cardinalities(3);
+      internalcount(8)=internalcount(8)+(n-sum(cardinalities));
+      K=K+1/2*(cardinalities(7))+1/2*(cardinalities(5)-1)+1/2*(cardinalities(6))+cardinalities(3);
+    end
+    internalcount(9)=internalcount(9)+(m+1-length(L{v1})-length(L{v2})-K);
+    internalcount(10)=internalcount(10)+((n-length(inter)-length(diff1)-length(diff2) )*...
+                                         (n-length(inter)-length(diff1)-length(diff2)-1)/2-...
+                                         (m+1-length(L{v1})-length(L{v2})-K));
+    count=count+internalcount.*w; 
+    %ones(size(internalcount))
+    % DEBUG    
+    %    v1
+    %    v2
+    %    K
+  end
+end
+
+count(11)= n*(n-1)*(n-2)*(n-3)/(4*3*2) - sum(count(1:10));
+end
+
+function [inter, diff1, diff2] = card_inter(o_set1, o_set2, l1, l2)
+% Find the intersection and set differences of two ordered sets of 
+% lengths l1 and l2 respectively
+
+inter=zeros(1,min(l1,l2));
+diff1=zeros(1,max(l1,l2));
+diff2=zeros(1,max(l1,l2));
+
+i=1; j=1;
+while i<=l1 && j <=l2
+  if o_set1(i)<o_set2(j) 
+    diff1(i)=o_set1(i);
+    i=i+1;
+  else 
+    if o_set1(i)>o_set2(j)
+      diff2(j)=o_set2(j);
+      j=j+1;
+    else
+      inter(i)=o_set1(i);
+      i=i+1; j=j+1;
+    end
+  end
+end
+if i<=l1
+  diff1(i:l1)=o_set1(i:l1);
+else 
+  if j<=l2
+    diff2(j:l2)=o_set2(j:l2);
+  end
+end
+inter=inter(inter>0);
+diff1=diff1(diff1>0);
+diff2=diff2(diff2>0);
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/connectedkernel.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/connectedkernel.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/connectedkernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/connectedkernel.m	2012-10-26 18:41:12.000000000 +0200
@@ -0,0 +1,46 @@
+function [K,runtime] = connectedkernel(Graphs, k)
+% Compute connected k-node graphlet kernel for a set of graphs
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+% Input: Graphs - a 1xn array of graphs
+%        k - the size of considered graphlets - 3, 4, or 5
+% Output: K - nxn kernel matrix K
+%         runtime - scalar
+
+n=size(Graphs,2);
+switch k
+ case 3
+  freq=zeros(n,2);
+ case 4
+  freq=zeros(n,6);
+ case 5
+  freq=zeros(n,21);
+end
+
+t=cputime; % for measuring runtime
+switch k
+  case 3
+   for i=1:n
+     freq(i,:)=countconnected3graphlets(Graphs(i).am, Graphs(i).al);
+     sum_freq=sum(freq(i,:));
+     if sum_freq~=0 freq(i,:)=freq(i,:)/sum_freq; end % for relative number of graphlets
+   end
+ case 4
+   for i=1:n
+     freq(i,:)=countconnected4graphlets(Graphs(i).am, Graphs(i).al);
+     sum_freq=sum(freq(i,:));
+     if sum_freq~=0 freq(i,:)=freq(i,:)/sum_freq; end % for relative number of graphlets
+   end
+ case 5
+   for i=1:n
+     freq(i,:)=countconnected5graphlets(Graphs(i).am, Graphs(i).al);
+     sum_freq=sum(freq(i,:));
+     if sum_freq~=0 freq(i,:)=freq(i,:)/sum_freq; end % for relative number of graphlets
+   end
+end
+K=freq*freq';
+
+runtime=cputime-t; % computation time of K
+
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected3graphlets.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected3graphlets.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected3graphlets.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected3graphlets.m	2012-10-26 18:41:12.000000000 +0200
@@ -0,0 +1,33 @@
+function [count] = countconnected3graphlets(A, L)
+
+% Count all 3-node connected subgraphs in an undirected graph
+% without node labels and with unweighted edges
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+%
+% Input: A - nxn adjacency matrix
+% 	 L - 1xn cell array - corresponding adjacency list
+% Output: count - 1x2 vector of integers. count(1) is the number of occurences of the
+%		  graphlet with 3 nodes and 2 edges (type 1), count(2) is the number of 
+%		  occurences of the graphlet with 3 nodes and 3 edges (type 2)
+
+w = [1/2, 1/6]; % 1/number of length-2 paths in graphlets of type 1 and type 2 respectively
+n = size(A,1); % number of nodes
+count = zeros(1,2);
+
+for i=1:n
+  for j=L{i}
+    for k=L{j}	
+      if k~=i
+        if A(i,k)==1
+          count(2)=count(2)+w(2);
+        else
+          count(1)=count(1)+w(1);
+        end
+      end
+    end
+  end
+end
+
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected4graphlets.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected4graphlets.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected4graphlets.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected4graphlets.m	2012-10-26 18:41:12.000000000 +0200
@@ -0,0 +1,72 @@
+function [count] = countconnected4graphlets(A, L)
+
+% Count all 4-node connected subgraphs in  an undirected graph
+% without node labels and with unweighted edges
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+%
+% Input: A - nxn adjacency matrix
+% 	 L - 1xn cell array - corresponding adjacency list
+% Output: count - 1x6 vector of integers. count(1) is the number of occurences of the
+%		  graphlet with 4 nodes and 6 edges (type 1), count(2) is the number of 
+%		  occurences of the graphlet with 4 nodes and 5 edges (type 2) and so
+%		  forth (cf. g_1--g_6 in ../4graphlets.pdf)
+
+w = [1/24, 1/12, 1/4, 0, 1/8, 1/2]; % 1/number of length-3 paths in graphlets of type 1-6 respectively
+n = size(A,1); % number of nodes
+count = zeros(1,6);
+
+for i=1:n
+  for j=L{i}
+    for k=L{j}	
+      if k~=i
+        for l=L{k}
+          if l~=i && l~=j
+            aux = A(i,k)+A(i,l)+A(j,l);
+            if aux==3 % if there are 6 edges in the graphlet, then it
+                      % is fully connected
+              count(1)=count(1)+w(1);
+            else
+              if aux==2 % if there are 5 edges in the graphlet,
+                        % then it is of type 2
+                count(2)=count(2)+w(2);
+              else
+                if aux==1 % if there are 4 edges, then it is either
+                          % of type 3 or 5
+                  if A(i,l)==1 % if i and l are connected, it is of
+                               % type 5, 3 otherwise
+                    count(5)=count(5)+w(5);
+                  else
+                    count(3)=count(3)+w(3);
+                  end
+                else count(6)=count(6)+w(6); % the only connected
+                                             % graphlet with 3
+                                             % edges is that of
+                                             % type 6
+                end
+              end
+            end
+          end
+        end
+      end
+    end
+  end
+  
+  
+  %%%% count "stars" %%%%	
+  N=length(L{i});
+  for j=1:N-2
+    for k=j+1:N-1
+      for l=k+1:N % with this kind of enumeration we make sure
+                  % that i, j, k, l, and m are pairwise different
+        if A(L{i}(j),L{i}(k))==0 && A(L{i}(j),L{i}(l))==0 && A(L{i}(k),L{i}(l))==0
+          count(4)=count(4)+1;
+        end
+      end
+    end
+  end
+  %%%% end count "stars" %%%%
+end
+
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected5graphlets.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected5graphlets.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/connectedgraphlets/countconnected5graphlets.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/connectedgraphlets/countconnected5graphlets.m	2012-10-26 18:41:12.000000000 +0200
@@ -0,0 +1,189 @@
+function [count] = countconnected5graphlets(A, L)
+
+% Count all 5-node connected subgraphs in an undirected graph
+% without node labels and with unweighted edges
+% Author: Nino Shervashidze - nino.shervashidze@tuebingen.mpg.de
+% Copyright 2012 Nino Shervashidze
+%
+% Input: A - nxn adjacency matrix
+% 	 L - 1xn cell array - corresponding adjacency list
+% Output: count - 1x21 vector of integers (21 being the number of
+%                 non-isomorphic connected 5-node graphs). count(1) is the number of occurences of the
+%		  graphlet with 5 nodes and 10 edges (type 1), count(2) is the number of 
+%		  occurences of the graphlet with 5 nodes and 9 edges (type 2) and so
+%		  forth (see g_1--g_21 in ../5graphlets.pdf)
+
+w = [1/120, 1/72, 1/48, 1/36, 1/28, 1/20, 1/14, 1/10, 1/12, 1/8, 1/8, 1/4, 1/2, 1/12, 1/12, 1/4, 1/4, 1/2, 0,0,0];
+% 1/number of length-4 paths in graphlets of type 1-21 respectively
+n = size(A,1); % number of nodes
+count = zeros(1,21);
+
+for i=1:n
+  for j=L{i}
+    for k=L{j}
+      if k~=i
+        for l=L{k}
+          if l~=i && l~=j
+            for m=L{l}
+              if m~=i && m~=j && m~=k
+         
+                %%% Tests: Which type of connected 5 node graphlet
+                %%% is induced by nodes in this length-4 simple
+                %%% path (that is, nodes i,j,k,l,m)?
+                aux = A(i,k)+A(i,l)+A(i,m)+A(j,l)+A(j,m)+A(k,m);
+                if aux==6 % if a graphlet has 10 edges, there is
+                          % only one possibility - it is the
+                          % complete graph of 5 nodes 
+                  count(1)=count(1)+w(1); % count increases by
+                                          % w(1), because this
+                                          % graphlet will be
+                                          % counted as many times,
+                                          % as the number of 5-node
+                                          % paths it contains
+                else
+                  if aux==5 % if it has 9 edges, there is only one
+                            % possibility as well
+                    count(2)=count(2)+w(2);
+                  else
+                    if aux==4 % if it has 8 edges, it can be either
+                              % graphlet 3 or 4, which can be
+                              % distinguished by looking at the
+                              % minimum degree of the graphlet
+                      aux1 = min(sum(A([i,j,k,l,m],[i,j,k,l,m]),1));
+                      if aux1==2 % the sorted degree distribution of the graphlet of type 4 is (2 3 3 4 4)
+                        count(4)=count(4)+w(4);
+                      else % the sorted degree distribution of the graphlet of type 3 is (3 3 3 3 4)
+                        count(3)=count(3)+w(3);
+                      end
+                    else 
+                      if aux==3 % if the graphlet has 7 edges,
+                                % it can be of type 5, 6, 9, or 14
+                        aux1 = sort(sum(A([i,j,k,l,m],[i,j,k,l,m]),1));
+                        if aux1(1)==1 % the sorted degree distribution of the graphlet of type 9 is (1 3 3 3 4)
+                          count(9)=count(9)+w(9);
+                        else
+                          if aux1(2)==3 % the sorted degree distribution of the graphlet of type 5 is (2 3 3 3 3) 
+                            count(5)=count(5)+w(5);
+                          else
+                            if aux1(3)==2 % the sorted degree distribution of the graphlet of type 14 is (2 2 2 4 4)
+                              count(14)=count(14)+w(14);
+                            else % the sorted degree distribution of the graphlet of type 6 is (2 2 3 3 4)
+                              count(6)=count(6)+w(6);	
+                            end
+                          end
+                        end
+                      else 
+                        if aux==2 % if the graphlet has 6 edges, if
+                                  % can be of type 7, 10, 11, 15,
+                                  % or 16
+                          aux2=sum(A([i,j,k,l,m],[i,j,k,l,m]),1);
+                          aux1 = sort(aux2);
+                          if aux1(1)==1
+                            if aux1(3)==2 % the sorted degree distribution of the graphlet of type 16 is (1 2 2 3 4)
+                              count(16)=count(16)+w(16);
+                            else % the sorted degree distribution of the graphlet of type 10 is (1 2 3 3 3)
+                              count(10)=count(10)+w(10);			
+                            end
+                          else 
+                            if aux1(4)==2 % the sorted degree distribution of the graphlet of type 11 is (2 2 2 2 4)
+                              count(11)=count(11)+w(11);
+                            else 
+                              % there are two types of connected length-6 graphlets left: 7 and 15. both have 
+                              % the same sorted degree distribution - (2 2 2 3 3), but in 7 the nodes with degree 3
+                              % are connected with an edge and in 15 they are not. 
+                              ind=find(aux2==3);
+                              path=[i,j,k,l,m];										
+                              if A(path(ind(1)), path(ind(2)))==1
+                                count(7)=count(7)+w(7);
+                              else
+                                count(15)=count(15)+w(15);
+                              end
+                              clear path;
+                            end	
+                          end
+                        else 
+                          if aux==1 % if the graphlet has 5 edges,
+                                    % it can be of type 8, 12, 17,
+                                    % or 18
+                            aux2 = sum(A([i,j,k,l,m],[i,j,k,l,m]),1);		
+                            aux1 = sort(aux2);
+                            if aux1(1)==2 % the sorted degree distribution of the graphlet of type 8 is (2 2 2 2 2) 
+                              count(8)=count(8)+w(8);
+                            else
+                              if aux1(2)==1 % the sorted degree distribution of the graphlet of type 18 is (1 1 2 3 3) 
+                                count(18)=count(18)+w(18);
+                              else
+                                % there are two types of connected length-5 graphlets (containing length-4 paths) left: 
+                                % 12 and 17. Both have the same sorted degree distribution - (1 2 2 2 3), but in 17 
+                                % the nodes with degrees 1 and 3 are connected with an edge, while in 12 they are not.
+                                ind=[find(aux2==3), find(aux2==1)];
+                                path=[i,j,k,l,m];
+                                if A(path(ind(1)), path(ind(2)))==1
+                                  count(17)=count(17)+w(17);
+                                else
+                                  count(12)=count(12)+w(12);
+                                end			
+                                clear path;
+                              end
+                            end
+                          else % if the graphlet has 4 edges, there
+                               % is only one possibility 
+                            count(13)=count(13)+w(13);
+                          end
+                        end
+                      end
+                    end
+                  end
+                end
+                %%% end Tests
+              end
+            end
+          end
+        end
+      end
+    end
+  end
+  
+  %%%% count graphlets of type 20 %%%%
+  for j=L{i}
+    for k=L{j}
+      if k~=i && A(i,k)==0
+        for l=L{k}
+          if l~=i && l~=j && A(i,l)==0 && A(j,l)==0
+            for m=L{k}
+              if m~=i && m~=j && m~=l && A(i,m)==0 && A(j,m)==0 && A(l,m)==0
+                count(20)=count(20)+0.5;
+              end
+            end
+          end
+        end
+      end
+    end
+  end
+  %%%% end count graphlets of type 20 %%%%
+  
+  %%%% count graphlets of type 19 and 21 %%%%
+  N=length(L{i});
+  for j=1:N-3
+    for k=j+1:N-2
+      for l=k+1:N-1
+        for m=l+1:N % with this kind of enumeration we make sure
+                    % that i, j, k, l, and m are pairwise different
+          aux = A(L{i}(j),L{i}(k)) + A(L{i}(j),L{i}(l)) + ...
+                A(L{i}(j),L{i}(m)) + A(L{i}(k),L{i}(l)) + ...
+                A(L{i}(k),L{i}(m)) + A(L{i}(l),L{i}(m));
+          if aux == 1
+            count(19)=count(19)+1;
+          else
+            if aux == 0 %%% "star"
+              count(21)=count(21)+1;
+            end
+          end
+        end
+      end
+    end
+  end
+  %%%% end count graphlets of type 19 and 21 %%%%	
+end
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/floydwarshall.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/floydwarshall.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/floydwarshall.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/floydwarshall.m	2012-10-26 18:35:52.000000000 +0200
@@ -0,0 +1,40 @@
+function [D] = floydwarshall(A, sym, w)
+% % Copyright 2012 Nino Shervashidze
+% Input: A - nxn adjacency matrix,
+%           sym - boolean, 1 if A and w symmetric
+%	    w - nxn weight matrix
+% Output: D - nxn distance matrix
+
+n = size(A,1); % number of nodes
+D=zeros(n,n);
+
+if nargin<2 % if the graph is not weighted and we have no information about sym, then 
+  sym=1;
+  w=A;
+end
+
+if nargin<3 % if the graph is not weighted, then
+  w=A;
+end
+
+D=w.*A; % if A(i,j)=1,  D(i,j)=w(i,j);
+D(A+diag(repmat(Inf,n,1))==0)=Inf; % If A(i,j)~=0 and i~=j D(i,j)=Inf;
+D=full(D.*(ones(n)-eye(n))); % set the diagonal to zero
+
+%t=cputime;
+if sym % then it is a bit faster
+  for k=1:n
+    Daux=repmat(full(D(:,k)),1,n);
+    Sumdist=Daux+Daux';
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+else  
+  for k=1:n
+    Daux1=repmat(full(D(:,k)),1,n);
+    Daux2=repmat(full(D(k,:)),n,1);
+    Sumdist=Daux1+Daux2;
+    D(Sumdist<D)=Sumdist(Sumdist<D);
+  end
+end
+%cputime-t
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix3.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix3.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix3.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix3.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,33 @@
+function P = Permutationmatrix3
+
+P = zeros(8,8);
+
+for a = 0:1
+
+for b = 0:1
+
+for c = 0:1
+
+ am = [0, a, b;
+       a, 0, c;
+       b, c, 0;];
+  
+perm=perms([1 2 3]);
+for k=1:6
+  P(graphlettype(am), graphlettype(am(perm(k,:),perm(k,:)) )) = 1;    
+end
+
+end
+end
+end
+
+end
+
+function result = graphlettype(am)
+  % determine graphlet type
+  
+  factor = 2 .^ [0:2]';
+  
+  upper = [am(1,2:3)';am(2,3)'];  
+  result = sum(factor .* upper) +1;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix4.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix4.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix4.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix4.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,103 @@
+function P = Permutationmatrix4
+% graphlet kernel
+% @author Karsten Borgwardt
+% @date May 11th 2007
+
+% calculate results for pairs of graphs
+%
+%for i = 1:size(graph,2)
+%%  %i
+%%  am1 = graph(i).am;
+%%  sp1 = floydwarshall(am1);
+%%  sp1(find(sp1 == Inf)) = 0;
+%%  graph(i).sp = sp1;
+%
+%m = size(graph(i).am,1)
+%am = graph(i).am;
+%graph(i).dv = zeros(64,1);
+%
+%for j = 1:samplesize
+%  r= randperm(m+0);
+%  graph(i).dv(graphlettype(am(r(1:4),r(1:4)))) = ...
+%      graph(i).dv(graphlettype(am(r(1:4),r(1:4)))) + 1;
+%  
+%end
+%graph(i).dv = graph(i).dv .* (1/samplesize);
+%end
+%
+%for i = 1:size(graph,2)
+%  %  i
+%    for j = 1:size(graph,2)
+%
+%      P = eye(64,64);
+%      
+%      result(i,j) = graph(i).dv' * P * graph(j).dv; 
+%    
+%    end
+%  end
+%
+%  
+
+P = zeros(64,64);
+
+for a = 0:1
+
+for b = 0:1
+
+for c = 0:1
+
+for d = 0:1
+
+for e = 0:1
+
+for f = 0:1
+  am = [0, a, b, c; 
+           a, 0 , d , e; 
+           b,d,0,f; 
+          c, e, f, 0] ;
+  
+  P(graphlettype(am),graphlettype(am([ 1 2 3 4], [ 1 2 3 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 1 2 4 3], [ 1 2 4 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 1 3 2 4], [ 1 3 2 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 1 3 4 2], [ 1 3 4 2]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 1 4 2 3], [ 1 4 2 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 1 4 3 2], [ 1 4 3 2]))) = 1;
+
+  P(graphlettype(am),graphlettype(am([ 2 1 3 4], [ 2 1 3 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 2 1 4 3], [ 2 1 4 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 2 3 1 4], [ 2 3 1 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 2 3 4 1], [ 2 3 4 1]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 2 4 1 3], [ 2 4 1 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([ 2 4 3 1], [ 2 4 3 1]))) = 1;
+
+  P(graphlettype(am),graphlettype(am([3  1 2 4], [ 3  1 2 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([3  1 4 2], [ 3  1 4 2]))) = 1;
+  P(graphlettype(am),graphlettype(am([3  2 1 4], [ 3  2 1 4]))) = 1;
+  P(graphlettype(am),graphlettype(am([3  2 4 1], [ 3  2 4 1]))) = 1;
+  P(graphlettype(am),graphlettype(am([3  4 1 2], [ 3  4 1 2]))) = 1;
+  P(graphlettype(am),graphlettype(am([3  4 2 1], [ 3  4 2 1]))) = 1;
+
+  P(graphlettype(am),graphlettype(am([4  1 2 3], [ 4  1 2 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([4  1 3 2], [ 4  1 3 2]))) = 1;
+  P(graphlettype(am),graphlettype(am([4  2 1 3], [ 4  2 1 3]))) = 1;
+  P(graphlettype(am),graphlettype(am([4  2 3 1], [ 4  2 3 1]))) = 1;
+  P(graphlettype(am),graphlettype(am([4  3 1 2], [ 4  3 1 2]))) = 1;
+  P(graphlettype(am),graphlettype(am([4  3 2 1], [ 4  3 2 1]))) = 1;
+
+
+end
+end
+end
+end
+end
+end
+
+
+
+  function result = graphlettype(am)
+  % determine graphlet type
+  
+  factor = 2 .^ [0:5]';
+  
+  upper = [am(1,2:4)';am(2,3:4)'; am(3,4)'];  
+  result = sum(factor .* upper) +1;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix5.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix5.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix5.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/Permutationmatrix5.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,55 @@
+function P = Permutationmatrix5
+
+P = zeros(1024,1024);
+
+for a = 0:1
+
+for b = 0:1
+
+for c = 0:1
+
+for d = 0:1
+
+for e = 0:1
+
+for f = 0:1
+
+for g = 0:1
+
+for h = 0:1
+
+for i = 0:1
+
+for j = 0:1
+
+  am = [0, a, b, c, d;
+        a, 0, e, f, g;
+        b, e, 0, h, i;
+	   c, f, h, 0, j;	 
+        d, g, i, j, 0] ;
+  
+perm=perms([1 2 3 4 5]);
+for k=1:120
+  P(graphlettype(am), graphlettype(am(perm(k,:),perm(k,:)) )) = 1;    
+end
+
+end
+end
+end
+end
+end
+end
+end
+end
+end
+end
+
+
+
+  function result = graphlettype(am)
+  % determine graphlet type
+  
+  factor = 2 .^ [0:9]';
+  
+  upper = [am(1,2:5)';am(2,3:5)'; am(3,4:5)'; am(4,5)'];  
+  result = sum(factor .* upper) +1;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/README NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/README
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/README	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,3 @@
+gestkernelK.m - compute a kernel based on sampling size-K graphlets
+gestkernelK.m needs a parameter specifying the sample size. This can be computed via the script samplesize.m and depends on two probabilities epsilon and delta, and the number of equivalence classes of graphlets. This number is equal to 4 for K=3, 11 for K=4 and 34 for K=5.
+If the parameter equals -1, there is no sampling, it computes exact distributions (but takes forever).
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel3.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel3.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel3.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel3.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,79 @@
+function [K, runtime] = gestkernel3(graph,samplesize)
+% 3-node graphlet kernel
+
+% calculate results for pairs of graphs
+
+P = Permutationmatrix3;
+
+t=cputime; % for measuring runtime
+for i = 1:size(graph,2)
+  %  %i
+  %  am1 = graph(i).am;
+  %  sp1 = floydwarshall(am1);
+  %  sp1(find(sp1 == Inf)) = 0;
+  %  graph(i).sp = sp1;
+  
+  m = size(graph(i).am,1);
+  if m < 3
+    graph(i).dv = zeros(8,1);
+    
+  else
+    
+    
+    am = graph(i).am;
+    mmax = max(size(am,1),size(am,2));
+    
+    am(mmax,mmax)=0;
+    
+    graph(i).dv = zeros(8,1);
+    
+    if (samplesize ~= -1)
+      
+      for j = 1:samplesize
+        r= randperm(m+0);
+        graph(i).dv(graphlettype(am(r(1:3),r(1:3)))) = ...
+            graph(i).dv(graphlettype(am(r(1:3),r(1:3)))) + 1;
+        
+      end
+      
+      graph(i).dv = graph(i).dv .* (1/samplesize);
+      
+    else 
+      for r = 1:m
+        for s = r+1:m
+          for t = s+1:m
+            graph(i).dv(graphlettype(am([r s t],[r s t]))) = ...
+                graph(i).dv(graphlettype(am([r s t],[r s t]))) + 1;
+          end
+        end
+      end
+      
+      graph(i).dv = graph(i).dv .* (6 / (m*(m-1)*(m-2)));
+      
+    end
+  end
+end
+
+
+for i = 1:size(graph,2)
+  %  i
+  for j = 1:size(graph,2)
+    
+    %eye(64,64);
+    
+    K(i,j) = graph(i).dv' * P * graph(j).dv; 
+    
+  end
+end
+runtime=cputime-t; % computation time of K
+
+end
+
+function result = graphlettype(am)
+% determine graphlet type
+  
+  factor = 2 .^ [0:2]';
+  
+  upper = [am(1,2:3)';am(2,3)'];  
+  result = sum(factor .* upper) +1;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel4.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel4.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel4.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel4.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,85 @@
+function     [K,      runtime]     =     gestkernel4(graph,samplesize)
+% graphlet kernel
+% @author Karsten Borgwardt
+% @date May 11th 2007
+  
+% calculate results for pairs of graphs
+  
+  P = Permutationmatrix4;
+  
+  t=cputime; % for measuring runtime
+  for i = 1:size(graph,2)
+    %  %i
+    %  am1 = graph(i).am;
+    %  sp1 = floydwarshall(am1);
+    %  sp1(find(sp1 == Inf)) = 0;
+    %  graph(i).sp = sp1;
+    
+    m = size(graph(i).am,1);
+    if m < 4
+      graph(i).dv = zeros(64,1);
+      
+    else
+      
+      
+      am = graph(i).am;
+      mmax = max(size(am,1),size(am,2));
+      
+      am(mmax,mmax)=0;
+      
+      graph(i).dv = zeros(64,1);
+      
+      if (samplesize ~= -1)
+        
+        
+        
+        for j = 1:samplesize
+          r= randperm(m+0);
+          graph(i).dv(graphlettype(am(r(1:4),r(1:4)))) = ...
+              graph(i).dv(graphlettype(am(r(1:4),r(1:4)))) + 1;
+          
+        end
+        
+        graph(i).dv = graph(i).dv .* (1/samplesize);
+        
+      else 
+        for r = 1:m
+          for s = r+1:m
+            for t = s+1:m
+              for u = t+1:m
+                
+                graph(i).dv(graphlettype(am([r s t u],[r s t u]))) = ...
+                    graph(i).dv(graphlettype(am([r s t u],[r s t u]))) + 1;
+              end 
+            end
+          end
+        end
+        %(graph(i).dv'*P)'
+        graph(i).dv = graph(i).dv .* (24 / (m*(m-1)*(m-2)*(m-3)));
+        
+      end
+    end
+  end
+  
+  
+  
+  for i = 1:size(graph,2)
+    %  i
+    for j = 1:size(graph,2)
+      
+      %eye(64,64);
+      K(i,j) = graph(i).dv' * P * graph(j).dv; 
+      
+    end
+  end
+  runtime=cputime-t; % computation time of K
+end
+
+function result = graphlettype(am)
+% determine graphlet type
+  
+  factor = 2 .^ [0:5]';
+  
+  upper = [am(1,2:4)';am(2,3:4)'; am(3,4)'];  
+  result = sum(factor .* upper) +1;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel5.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel5.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/gestkernel5.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/gestkernel5.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,81 @@
+function [K, runtime] = gestkernel5(graph,samplesize)
+% 5-node graphlet kernel
+  
+% calculate results for pairs of graphs
+  
+  P = Permutationmatrix5;
+  
+  tt=cputime; % for measuring runtime
+  for i = 1:size(graph,2)
+    %  %i
+    %  am1 = graph(i).am;
+    %  sp1 = floydwarshall(am1);
+    %  sp1(find(sp1 == Inf)) = 0;
+    %  graph(i).sp = sp1;
+    
+    m = size(graph(i).am,1);
+    if m < 5
+      graph(i).dv = zeros(1024,1);
+      
+    else
+      
+      
+      am = graph(i).am;
+      mmax = max(size(am,1),size(am,2));
+      
+      am(mmax,mmax)=0;
+      
+      graph(i).dv = zeros(1024,1);
+      
+      if (samplesize ~= -1)
+        
+        for j = 1:samplesize
+          r= randperm(m+0);
+          graph(i).dv(graphlettype(am(r(1:5),r(1:5)))) = ...
+              graph(i).dv(graphlettype(am(r(1:5),r(1:5)))) + 1;
+          
+        end
+        
+        graph(i).dv = graph(i).dv .* (1/samplesize);
+        
+      else 
+        for r = 1:m
+          for s = r+1:m
+            for t = s+1:m
+              for u = t+1:m
+                for v = u+1:m	 
+                  graph(i).dv(graphlettype(am([r s t u v],[r s t u v]))) = ...
+                      graph(i).dv(graphlettype(am([r s t u v],[r s t u v]))) + 1;
+                end
+              end 
+            end
+          end
+        end
+        
+        graph(i).dv = graph(i).dv .* (120 / (m*(m-1)*(m-2)*(m-3)*(m-4)));
+        
+      end
+    end
+  end
+  
+  
+  for i = 1:size(graph,2)
+    for j = 1:size(graph,2)
+      
+      K(i,j) = graph(i).dv' * P * graph(j).dv; 
+      
+    end
+  end
+  runtime=cputime-tt; % computation time of K
+  
+end
+
+function result = graphlettype(am)
+% determine graphlet type
+  
+  factor = 2 .^ [0:9]';
+  
+  upper = [am(1,2:5)';am(2,3:5)'; am(3,4:5)';am(4,5)'];  
+  result = sum(factor .* upper) +1;
+end
+  
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/samplesize.m NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/samplesize.m
--- NeuroMiner-1-main/graph_kernels/unlabeled/samplinggraphlets/samplesize.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graph_kernels/unlabeled/samplinggraphlets/samplesize.m	2012-10-25 17:15:02.000000000 +0200
@@ -0,0 +1,10 @@
+function result = samplesize(delta, epsilon, a)
+% delta = confidence level (typically 0.05 or 0.1)
+% epsilon = precision level (typically 0.05 or 1)
+% a = number of isomorphism classes of graphlets
+%
+% Karsten Borgwardt
+% 4/11/2008
+
+
+result = 2 * ( a* log(2) + log(1/delta) ) / (epsilon^2)
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/adjacency_plot_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/adjacency_plot_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/adjacency_plot_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/adjacency_plot_und.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,53 @@
+function [X,Y,Z] = adjacency_plot_und(aij,coor)
+%ADJACENCY_PLOT_UND     Quick visualization tool
+%
+%   [X,Y,Z] = ADJACENCY_PLOT(AIJ,COOR) takes adjacency matrix AIJ and node
+%   spatial coordinates COOR and generates three vectors that can be used
+%   for quickly plotting the edges in AIJ. If no coordinates are specified,
+%   then each node is assigned a position on a circle. COOR can, in 
+%   general, be 2D or 3D.
+%
+%   Example:
+%
+%   >> load AIJ;                                % load your adjacency matrix
+%   >> load COOR;                               % load 3D coordinates for each node
+%   >> [x,y,z] = adjacency_plot_und(AIJ,COOR);  % call function
+%   >> plot3(x,y,z);                            % plots network as a single line object
+%
+%   If COOR were 2D, the PLOT3 command changes to a PLOT command.
+%
+%   NOTE: This function is similar to MATLAB's GPLOT command.
+%
+%   Richard Betzel, Indiana University, 2013
+
+n = length(aij);
+if nargin < 2
+    coor = zeros(n,2);
+    for i = 1:n
+        coor(i,:) = [cos(2*pi*(i - 1)./n), sin(2*pi*(i - 1)./n)];
+    end
+end
+
+[i,j] = find(triu(aij,1));
+[~, p] = sort(max(i,j));
+i = i(p);
+j = j(p);
+
+X = [ coor(i,1) coor(j,1)]';
+Y = [ coor(i,2) coor(j,2)]';
+if size(coor,2) == 3
+    Z = [ coor(i,3) coor(j,3)]';
+end
+if isfloat(coor) || nargout ~= 0
+    X = [X; NaN(size(i))'];
+    Y = [Y; NaN(size(i))'];
+    if size(coor,2) == 3
+        Z = [Z; NaN(size(i))'];
+    end
+end
+
+X = X(:);
+Y = Y(:);
+if size(coor,2) == 3
+    Z = Z(:);
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/agreement.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/agreement.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/agreement.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/agreement.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,60 @@
+function D = agreement(ci,buffsz)
+%AGREEMENT      Agreement matrix from clusters
+%
+%   D = AGREEMENT(CI) takes as input a set of vertex partitions CI of
+%   dimensions [vertex x partition]. Each column in CI contains the
+%   assignments of each vertex to a class/community/module. This function
+%   aggregates the partitions in CI into a square [vertex x vertex]
+%   agreement matrix D, whose elements indicate the number of times any two
+%   vertices were assigned to the same class.
+%
+%   In the case that the number of nodes and partitions in CI is large
+%   (greater than ~1000 nodes or greater than ~1000 partitions), the script
+%   can be made faster by computing D in pieces. The optional input BUFFSZ
+%   determines the size of each piece. Trial and error has found that
+%   BUFFSZ ~ 150 works well.
+%
+%   Inputs,     CI,     set of (possibly) degenerate partitions
+%               BUFFSZ, optional second argument to set buffer size
+%
+%   Outputs:    D,      agreement matrix
+%
+%   Richard Betzel, Indiana University, 2012
+
+%modification history
+%09.24.2012 - added loop for big N that makes the function slower but also
+% prevents it from maxing out memory.
+
+n = size(ci,2);
+
+if nargin < 2
+    buffsz = 1000;
+end
+
+if n <= buffsz
+    
+    ind = dummyvar(ci);
+    D = ind*ind';
+    
+else
+    
+    a = 1:buffsz:n;
+    b = buffsz:buffsz:n;
+    
+    if length(a) ~= length(b)
+        b = [b, n];
+    end
+    
+    x = [a' b'];
+    nbuff = size(x,1);
+    
+    D = zeros(size(ci,1));
+    for i = 1:nbuff
+       y = ci(:,x(i,1):x(i,2));
+       ind = dummyvar(y);
+       D = D + ind*ind';
+    end
+    
+end
+
+D = D.*~eye(length(D));
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/agreement_weighted.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/agreement_weighted.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/agreement_weighted.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/agreement_weighted.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,28 @@
+function D = agreement_weighted(CI,Wts)
+% AGREEMENT_WEIGHTED     Weights agreement matrix
+%
+%   D = AGREEMENT_WEIGHTED(CI,WTS) is identical to AGREEMENT, with the 
+%   exception that each partitions contribution is weighted according to 
+%   the corresponding scalar value stored in the vector WTS. As an example,
+%   suppose CI contained partitions obtained using some heuristic for 
+%   maximizing modularity. A possible choice for WTS might be the Q metric
+%   (Newman's modularity score). Such a choice would add more weight to 
+%   higher modularity partitions.
+%
+%   NOTE: Unlike AGREEMENT, this script does not have the input argument
+%   BUFFSZ.
+%
+%   Inputs:     CI,     set of partitions
+%               WTS,    relative weight of importance of each paritition
+%
+%   Outputs:    D,      weighted agreement matrix
+%
+%   Richard Betzel, Indiana University, 2013
+
+Wts = Wts./sum(Wts);
+[N,M] = size(CI);
+D = zeros(N);
+for i = 1:M
+    d = dummyvar(CI(:,i));
+    D = D + (d*d')*Wts(i);
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/align_matrices.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/align_matrices.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/align_matrices.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/align_matrices.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,135 @@
+function [Mreordered,Mindices,cost] = align_matrices(M1,M2,dfun,flag)
+%ALIGN_MATRICES         Alignment of two matrices
+%
+%   [Mreordered,Mindices,cost] = align_matrices(M1,M2,dfun,flag)
+%
+%   This function aligns two matrices relative to one another by reordering
+%   the nodes in M2.  The function uses a version of simulated annealing.
+%
+%   Inputs:     M1             = first connection matrix (square)
+%               M2             = second connection matrix (square)
+%               dfun           = distance metric to use for matching:
+%                                'absdff' = absolute difference
+%                                'sqrdff' = squared difference
+%                                'cosang' = cosine of vector angle
+%
+%               Mreordered     = reordered connection matrix M2
+%               Mindices       = reordered indices
+%               cost           = distance between M1 and Mreordered
+%
+%   Connection matrices can be weighted or binary, directed or undirected.
+%   They must have the same number of nodes.  M1 can be entered in any
+%   node ordering.
+%
+%   Note that in general, the outcome will depend on the initial condition
+%   (the setting of the random number seed).  Also, there is no good way to 
+%   determine optimal annealing parameters in advance - these parameters 
+%   will need to be adjusted "by hand" (particularly H, Texp, T0, and Hbrk).  
+%   For large and/or dense matrices, it is highly recommended to perform 
+%   exploratory runs varying the settings of 'H' and 'Texp' and then select 
+%   the best values.
+%
+%   Based on extensive testing, it appears that T0 and Hbrk can remain
+%   unchanged in most cases.  Texp may be varied from 1-1/H to 1-10/H, for
+%   example.  H is the most important parameter - set to larger values as
+%   the problem size increases.  Good solutions can be obtained for
+%   matrices up to about 100 nodes.  It is advisable to run this function
+%   multiple times and select the solution(s) with the lowest 'cost'.
+%
+%   If the two matrices are related it may be very helpful to pre-align them
+%   by reordering along their largest eigenvectors:
+%       [v,~] = eig(M1); v1 = abs(v(:,end)); [a1,b1] = sort(v1);
+%       [v,~] = eig(M2); v2 = abs(v(:,end)); [a2,b2] = sort(v2);
+%       [a,b,c] = overlapMAT2(M1(b1,b1),M2(b2,b2),'dfun',1);
+%
+%   Setting 'Texp' to zero cancels annealing and uses a greedy algorithm
+%   instead.
+%
+%   Yusuke Adachi, University of Tokyo, 2010
+%   Olaf Sporns, Indiana University, 2010
+
+N = size(M1,1);
+
+% define maxcost (greatest possible difference)
+switch dfun
+case 'absdff'
+    maxcost = sum(abs(sort(M1(:))-(sort(M2(:),'descend'))));
+case 'sqrdff'
+    maxcost = sum((sort(M1(:))-(sort(M2(:),'descend'))).^2);
+case 'cosang'
+    maxcost = pi/2;
+end;
+
+% initialize lowcost
+switch dfun
+case 'absdff'
+    lowcost = sum(sum(abs(M1-M2)))/maxcost;
+case 'sqrdff'
+    lowcost = sum(sum((M1-M2).^2))/maxcost;
+case 'cosang'
+    lowcost = acos(dot(M1(:),M2(:))./sqrt(dot(M1(:),M1(:))*dot(M2(:),M2(:))))/maxcost;
+end;
+
+% initialize 
+mincost = lowcost;
+anew = 1:N;
+amin = 1:N;
+h = 0; hcnt = 0;
+
+% set annealing parameters
+% H determines the maximal number of steps
+% Texp determines the steepness of the temperature gradient
+% T0 sets the initial temperature (and scales the energy term)
+% Hbrk sets a break point for the simulation (no further improvement)
+H = 1e06; Texp = 1-1/H; T0 = 1e-03; Hbrk = H/10;
+%Texp = 0;
+
+while h<H
+    h = h+1; hcnt = hcnt+1;
+    % terminate if no new mincost has been found for some time
+    if (hcnt>Hbrk) 
+        break; 
+    end;
+    % current temperature
+    T = T0*Texp^h;
+    % choose two positions at random and flip them
+    atmp = anew;
+    %r = randperm(N);  % slower
+    r = ceil(rand(1,2).*N);
+    atmp(r(1)) = anew(r(2));
+    atmp(r(2)) = anew(r(1));
+    switch dfun
+        case 'absdff'
+            costnew = sum(sum(abs(M1-M2(atmp,atmp))))/maxcost;
+        case 'sqrdff'
+            costnew = sum(sum((M1-M2(atmp,atmp)).^2))/maxcost;
+        case 'cosang'
+            M2atmp = M2(atmp,atmp);
+            costnew = acos(dot(M1(:),M2atmp(:))./sqrt(dot(M1(:),M1(:))*dot(M2atmp(:),M2atmp(:))))/maxcost;
+    end;
+    % annealing step
+    if (costnew < lowcost) || (rand < exp(-(costnew-lowcost)/T))
+        anew = atmp;
+        lowcost = costnew;
+        % is this the absolute best?
+        if (lowcost<mincost)
+            amin = anew;
+            mincost = lowcost;
+            if (flag==1) 
+                disp(['step ',num2str(h),' ... current lowest cost = ',num2str(mincost)]);
+            end;
+            hcnt = 0;
+        end;
+        % if the cost is 0 we're done
+        if (mincost==0)
+            break;
+        end;
+    end;
+end;
+disp(['step ',num2str(h),' ... final lowest cost = ',num2str(mincost)]);
+
+% prepare output
+Mreordered = M2(amin,amin);
+Mindices = amin;
+cost = mincost;
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_bin.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,64 @@
+function   r = assortativity_bin(CIJ,flag)
+% ASSORTATIVITY_BIN      Assortativity coefficient
+%
+%   r = assortativity(CIJ,flag);
+%
+%   The assortativity coefficient is a correlation coefficient between the
+%   degrees of all nodes on two opposite ends of a link. A positive
+%   assortativity coefficient indicates that nodes tend to link to other
+%   nodes with the same or similar degree.
+%
+%   Inputs:     CIJ,    binary directed/undirected connection matrix
+%               flag,   0, undirected graph: degree/degree correlation
+%                       1, directed graph: out-degree/in-degree correlation
+%                       2, directed graph: in-degree/out-degree correlation
+%                       3, directed graph: out-degree/out-degree correlation
+%                       4, directed graph: in-degree/in-degree correlation
+%
+%   Outputs:    r,      assortativity coefficient
+%
+%   Notes: The function accepts weighted networks, but all connection
+%   weights are ignored. The main diagonal should be empty. For flag 1
+%   the function computes the directed assortativity described in Rubinov
+%   and Sporns (2010) NeuroImage.
+%
+%   Reference:  Newman (2002) Phys Rev Lett 89:208701
+%               Foster et al. (2010) PNAS 107:1081510820
+%
+%   Olaf Sporns, Indiana University, 2007/2008
+%   Vassilis Tsiaras, University of Crete, 2009
+%   Murray Shanahan, Imperial College London, 2012
+%   Mika Rubinov, University of Cambridge, 2012
+
+if (flag==0)                        % undirected version
+    deg = degrees_und(CIJ);
+    [i,j] = find(triu(CIJ,1)>0);
+    K = length(i);
+    degi = deg(i);
+    degj = deg(j);
+
+else                                % directed versions
+    [id,od] = degrees_dir(CIJ);
+    [i,j] = find(CIJ>0);
+    K = length(i);
+
+    switch flag
+        case 1
+            degi = od(i);
+            degj = id(j);
+        case 2
+            degi = id(i);
+            degj = od(j);
+        case 3
+            degi = od(i);
+            degj = od(j);
+        case 4
+            degi = id(i);
+            degj = id(j);
+    end
+end
+
+% compute assortativity
+r = ( sum(degi.*degj)/K - (sum(0.5*(degi+degj))/K)^2 ) / ...
+    ( sum(0.5*(degi.^2+degj.^2))/K - (sum(0.5*(degi+degj))/K)^2 );
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/assortativity_wei.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,62 @@
+function   r = assortativity_wei(CIJ,flag)
+% ASSORTATIVITY_WEI      Assortativity coefficient
+%
+%   r = assortativity_wei(CIJ,flag);
+%
+%   The assortativity coefficient is a correlation coefficient between the
+%   strengths (weighted degrees) of all nodes on two opposite ends of a link.
+%   A positive assortativity coefficient indicates that nodes tend to link to
+%   other nodes with the same or similar strength.
+%
+%   Inputs:     CIJ,    weighted directed/undirected connection matrix
+%               flag,   0, undirected graph: strength/strength correlation
+%                       1, directed graph: out-strength/in-strength correlation
+%                       2, directed graph: in-strength/out-strength correlation
+%                       3, directed graph: out-strength/out-strength correlation
+%                       4, directed graph: in-strength/in-strength correlation
+%
+%   Outputs:    r,      assortativity coefficient
+%
+%   Notes: The main diagonal should be empty. For flag 1 the function computes 
+%   the directed assortativity described in Rubinov and Sporns (2010) NeuroImage.
+%
+%   Reference:  Newman (2002) Phys Rev Lett 89:208701
+%               Foster et al. (2010) PNAS 107:10815-10820
+%
+%   Olaf Sporns, Indiana University, 2007/2008
+%   Vassilis Tsiaras, University of Crete, 2009
+%   Murray Shanahan, Imperial College London, 2012
+%   Mika Rubinov, University of Cambridge, 2012
+
+if (flag==0)                        % undirected version
+    str = strengths_und(CIJ);
+    [i,j] = find(triu(CIJ,1)>0);
+    K = length(i);
+    stri = str(i);
+    strj = str(j);
+
+else                                % directed versions
+    [is,os] = strengths_dir(CIJ);
+    [i,j] = find(CIJ>0);
+    K = length(i);
+
+    switch flag
+        case 1
+            stri = os(i);
+            strj = is(j);
+        case 2
+            stri = is(i);
+            strj = os(j);
+        case 3
+            stri = os(i);
+            strj = os(j);
+        case 4
+            stri = is(i);
+            strj = is(j);
+    end
+end
+
+% compute assortativity
+r = ( sum(stri.*strj)/K - (sum(0.5*(stri+strj))/K)^2 ) / ...
+    ( sum(0.5*(stri.^2+strj.^2))/K - (sum(0.5*(stri+strj))/K)^2 );
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/backbone_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/backbone_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/backbone_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/backbone_wu.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,64 @@
+function  [CIJtree,CIJclus] = backbone_wu(CIJ,avgdeg)
+%BACKBONE_WU        Backbone
+%
+%   [CIJtree,CIJclus] = backbone_wu(CIJ,avgdeg)
+%
+%   The network backbone contains the dominant connections in the network
+%   and may be used to aid network visualization. This function computes
+%   the backbone of a given weighted and undirected connection matrix CIJ, 
+%   using a minimum-spanning-tree based algorithm.
+%
+%   input:      CIJ,    connection/adjacency matrix (weighted, undirected)
+%            avgdeg,    desired average degree of backbone
+%   output: 
+%           CIJtree,    connection matrix of the minimum spanning tree of CIJ
+%           CIJclus,    connection matrix of the minimum spanning tree plus
+%                       strongest connections up to an average degree 'avgdeg'
+%
+%   NOTE: nodes with zero strength are discarded.
+%   NOTE: CIJclus will have a total average degree exactly equal to 
+%         (or very close to) 'avgdeg'.
+%   NOTE: 'avgdeg' backfill is handled slightly differently than in Hagmann
+%         et al 2008.
+%
+%   Reference: Hidalgo et al. (2007) Science 317, 482.
+%              Hagmann et al. (2008) PLoS Biol
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+N = size(CIJ,1);
+CIJtree = zeros(N);
+
+% find strongest edge (note if multiple edges are tied, only use first one)
+[i,j,s] = find(max(max(CIJ))==CIJ);                      %#ok<*ASGLU>
+im = [i(1) i(2)];
+jm = [j(1) j(2)];
+
+% copy into tree graph
+CIJtree(im,jm) = CIJ(im,jm);
+in = im;
+out = setdiff(1:N,in);
+
+% repeat N-2 times
+for n=1:N-2
+    
+    % find strongest link between 'in' and 'out',ignore tied ranks
+    [i,j,s] = find(max(max(CIJ(in,out)))==CIJ(in,out)); 
+    im = in(i(1));
+    jm = out(j(1));
+    
+    % copy into tree graph
+    CIJtree(im,jm) = CIJ(im,jm); CIJtree(jm,im) = CIJ(jm,im);
+    in = [in jm];                                       %#ok<AGROW>
+    out = setdiff(1:N,in);
+
+end;
+
+% now add connections back, with the total number of added connections 
+% determined by the desired 'avgdeg'
+CIJnotintree = CIJ.*~CIJtree;
+[a,b] = sort(nonzeros(CIJnotintree),'descend');
+cutoff = avgdeg*N - 2*(N-1);
+thr = a(cutoff);
+CIJclus = CIJtree + CIJnotintree.*(CIJnotintree>=thr);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_bin.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,53 @@
+function BC=betweenness_bin(G)
+%BETWEENNESS_BIN    Node betweenness centrality
+%
+%   BC = betweenness_bin(A);
+%
+%   Node betweenness centrality is the fraction of all shortest paths in 
+%   the network that contain a given node. Nodes with high values of 
+%   betweenness centrality participate in a large number of shortest paths.
+%
+%   Input:      A,      binary (directed/undirected) connection matrix.
+%
+%   Output:     BC,     node betweenness centrality vector.
+%
+%   Note: Betweenness centrality may be normalised to the range [0,1] as
+%   BC/[(N-1)(N-2)], where N is the number of nodes in the network.
+%
+%   Reference: Kintali (2008) arXiv:0809.1906v2 [cs.DS]
+%              (generalization to directed and disconnected graphs)
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2012
+
+
+n=length(G);                %number of nodes
+I=eye(n)~=0;                %logical identity matrix
+d=1;                     	%path length
+NPd=G;                      %number of paths of length |d|
+NSPd=NPd;                  	%number of shortest paths of length |d|
+NSP=NSPd; NSP(I)=1;        	%number of shortest paths of any length
+L=NSPd; L(I)=1;           	%length of shortest paths
+
+%calculate NSP and L
+while find(NSPd,1)
+    d=d+1;
+    NPd=NPd*G;
+    NSPd=NPd.*(L==0);
+    NSP=NSP+NSPd;
+    L=L+d.*(NSPd~=0);
+end
+L(~L)=inf; L(I)=0;          %L for disconnected vertices is inf
+NSP(~NSP)=1;                %NSP for disconnected vertices is 1
+
+Gt=G.';
+DP=zeros(n);            	%vertex on vertex dependency
+diam=d-1;                  	%graph diameter
+
+%calculate DP
+for d=diam:-1:2
+    DPd1=(((L==d).*(1+DP)./NSP)*Gt).*((L==(d-1)).*NSP);
+    DP=DP + DPd1;           %DPd1: dependencies on vertices |d-1| from source
+end
+
+BC=sum(DP,1);               %compute betweenness
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/betweenness_wei.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,76 @@
+function BC=betweenness_wei(G)
+%BETWEENNESS_WEI    Node betweenness centrality
+%
+%   BC = betweenness_wei(L);
+%
+%   Node betweenness centrality is the fraction of all shortest paths in 
+%   the network that contain a given node. Nodes with high values of 
+%   betweenness centrality participate in a large number of shortest paths.
+%
+%   Input:      L,      Directed/undirected connection-length matrix.
+%
+%   Output:     BC,     node betweenness centrality vector.
+%
+%   Notes:
+%       The input matrix must be a connection-length matrix, typically
+%   obtained via a mapping from weight to length. For instance, in a
+%   weighted correlation network higher correlations are more naturally
+%   interpreted as shorter distances and the input matrix should
+%   consequently be some inverse of the connectivity matrix. 
+%       Betweenness centrality may be normalised to the range [0,1] as
+%   BC/[(N-1)(N-2)], where N is the number of nodes in the network.
+%
+%   Reference: Brandes (2001) J Math Sociol 25:163-177.
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2012
+
+n=length(G);
+% E=find(G); G(E)=1./G(E);        %invert weights
+BC=zeros(n,1);                  %vertex betweenness
+
+for u=1:n
+    D=inf(1,n); D(u)=0;         %distance from u
+    NP=zeros(1,n); NP(u)=1;     %number of paths from u
+    S=true(1,n);                %distance permanence (true is temporary)
+    P=false(n);                 %predecessors
+    Q=zeros(1,n); q=n;          %order of non-increasing distance
+
+    G1=G;
+    V=u;
+    while 1
+        S(V)=0;                 %distance u->V is now permanent
+        G1(:,V)=0;              %no in-edges as already shortest
+        for v=V
+            Q(q)=v; q=q-1;
+            W=find(G1(v,:));                %neighbours of v
+            for w=W
+                Duw=D(v)+G1(v,w);           %path length to be tested
+                if Duw<D(w)                 %if new u->w shorter than old
+                    D(w)=Duw;
+                    NP(w)=NP(v);            %NP(u->w) = NP of new path
+                    P(w,:)=0;
+                    P(w,v)=1;               %v is the only predecessor
+                elseif Duw==D(w)            %if new u->w equal to old
+                    NP(w)=NP(w)+NP(v);      %NP(u->w) sum of old and new
+                    P(w,v)=1;               %v is also a predecessor
+                end
+            end
+        end
+
+        minD=min(D(S));
+        if isempty(minD), break             %all nodes reached, or
+        elseif isinf(minD)                  %...some cannot be reached:
+            Q(1:q)=find(isinf(D)); break	%...these are first-in-line
+        end
+        V=find(D==minD);
+    end
+
+    DP=zeros(n,1);                          %dependency
+    for w=Q(1:n-1)
+        BC(w)=BC(w)+DP(w);
+        for v=find(P(w,:))
+            DP(v)=DP(v)+(1+DP(w)).*NP(v)./NP(w);
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/breadth.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/breadth.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/breadth.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/breadth.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,62 @@
+function [distance,branch] = breadth(CIJ,source)
+%BREADTH        Auxiliary function for breadthdist.m
+%
+%   [distance,branch] = breadth(CIJ,source);
+%
+%   Implementation of breadth-first search.
+%
+%   Input:      CIJ,        binary (directed/undirected) connection matrix
+%               source,     source vertex
+%
+%   Outputs:    distance,   distance between 'source' and i'th vertex
+%                           (0 for source vertex)
+%               branch,     vertex that precedes i in the breadth-first search tree
+%                           (-1 for source vertex)
+%        
+%   Notes: Breadth-first search tree does not contain all paths (or all 
+%   shortest paths), but allows the determination of at least one path with
+%   minimum distance. The entire graph is explored, starting from source 
+%   vertex 'source'.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+N = size(CIJ,1);
+
+% colors: white, gray, black
+white = 0; 
+gray = 1; 
+black = 2;
+
+% initialize colors
+color = zeros(1,N);
+% initialize distances
+distance = inf*ones(1,N);
+% initialize branches
+branch = zeros(1,N);
+
+% start on vertex 'source'
+color(source) = gray;
+distance(source) = 0;
+branch(source) = -1;
+Q = source;
+
+% keep going until the entire graph is explored
+while ~isempty(Q)
+   u = Q(1);
+   ns = find(CIJ(u,:));
+   for v=ns
+% this allows the 'source' distance to itself to be recorded
+      if (distance(v)==0)
+         distance(v) = distance(u)+1;
+      end;
+      if (color(v)==white)
+         color(v) = gray;
+         distance(v) = distance(u)+1;
+         branch(v) = u;
+         Q = [Q v];                                             %#ok<AGROW>
+      end;
+   end;
+   Q = Q(2:length(Q));
+   color(u) = black;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/breadthdist.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/breadthdist.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/breadthdist.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/breadthdist.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,39 @@
+function  [R,D] = breadthdist(CIJ)
+%BREADTHDIST      Reachability and distance matrices
+%
+%   [R,D] = breadthdist(CIJ);
+%
+%   The binary reachability matrix describes reachability between all pairs
+%   of nodes. An entry (u,v)=1 means that there exists a path from node u
+%   to node v; alternatively (u,v)=0.
+%
+%   The distance matrix contains lengths of shortest paths between all
+%   pairs of nodes. An entry (u,v) represents the length of shortest path 
+%   from node u to  node v. The average shortest path length is the 
+%   characteristic path length of the network.
+%
+%   Input:      CIJ,     binary (directed/undirected) connection matrix
+%
+%   Outputs:    R,       reachability matrix
+%               D,       distance matrix
+%
+%   Note: slower but less memory intensive than "reachdist.m".
+%
+%   Algorithm: Breadth-first search.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+N = size(CIJ,1);
+
+D = zeros(N);
+for i=1:N
+   D(i,:) = breadth(CIJ,i);
+end;
+
+% replace zeros with 'Inf's
+D(D==0) = Inf;
+
+% construct R
+R = double(D~=Inf);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/charpath.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/charpath.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/charpath.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/charpath.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,79 @@
+function  [lambda,efficiency,ecc,radius,diameter] = charpath(D,diagonal_dist,infinite_dist)
+%CHARPATH       Characteristic path length, global efficiency and related statistics
+%
+%   lambda                                  = charpath(D);
+%   lambda                                  = charpath(D);
+%   [lambda,efficiency]                     = charpath(D);
+%   [lambda,efficiency,ecc,radius,diameter] = charpath(D,diagonal_dist,infinite_dist);
+%
+%   The network characteristic path length is the average shortest path
+%   length between all pairs of nodes in the network. The global efficiency
+%   is the average inverse shortest path length in the network. The nodal
+%   eccentricity is the maximal path length between a node and any other
+%   node in the network. The radius is the minimal eccentricity, and the
+%   diameter is the maximal eccentricity.
+%
+%   Input:      D,              distance matrix
+%               diagonal_dist   optional argument
+%                               include distances on the main diagonal
+%                                   (default: diagonal_dist=0)
+%               infinite_dist   optional argument
+%                               include infinite distances in calculation
+%                                   (default: infinite_dist=1)
+%
+%   Outputs:    lambda,         network characteristic path length
+%               efficiency,     network global efficiency
+%               ecc,            nodal eccentricity
+%               radius,         network radius
+%               diameter,       network diameter
+%
+%   Notes:
+%       The input distance matrix may be obtained with any of the distance
+%   functions, e.g. distance_bin, distance_wei.
+%       Characteristic path length is defined here as the mean shortest
+%   path length between all pairs of nodes, for consistency with common
+%   usage. Note that characteristic path length is also defined as the
+%   median of the mean shortest path length from each node to all other
+%   nodes.
+%       Infinitely long paths (i.e. paths between disconnected nodes) are
+%   included in computations by default. This behavior may be modified with
+%   via the infinite_dist argument.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+%   Mika Rubinov, U Cambridge, 2010/2015
+
+%   Modification history
+%   2002: original (OS)
+%   2010: incorporation of global efficiency (MR)
+%   2015: exclusion of diagonal weights by default (MR)
+%   2016: inclusion of infinite distances by default (MR)
+
+n = size(D,1);
+if any(any(isnan(D)))
+    error('The distance matrix must not contain NaN values');
+end
+if ~exist('diagonal_dist','var') || ~diagonal_dist || isempty(diagonal_dist)
+    D(1:n+1:end) = NaN;             % set diagonal distance to NaN
+end
+if  exist('infinite_dist','var') && ~infinite_dist
+    D(isinf(D))  = NaN;             % ignore infinite path lengths
+end
+
+Dv = D(~isnan(D));                  % get non-NaN indices of D
+
+% Mean of entries of D(G)
+lambda     = mean(Dv);
+
+% Efficiency: mean of inverse entries of D(G)
+efficiency = mean(1./Dv);
+
+% Eccentricity for each vertex
+ecc        = nanmax(D,[],2);
+
+% Radius of graph
+radius     = min(ecc);
+
+% Diameter of graph
+diameter   = max(ecc);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clique_communities.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clique_communities.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clique_communities.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clique_communities.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,102 @@
+function M = clique_communities(A, cq_thr)
+% CLIQUE_COMMUNITIES     Overlapping community structure via clique percolation
+%
+%   M = clique_communities(A, cq_thr)
+%
+%   The optimal community structure is a subdivision of the network into
+%   groups of nodes which have a high number of within-group connections
+%   and a low number of between group connections.
+%
+%   This algorithm uncovers overlapping community structure in binary
+%   undirected networks via the clique percolation method.
+%
+%   Inputs:
+%       A,          Binary undirected connection matrix.
+%
+%      	cq_thr,     Clique size threshold (integer). Larger clique size
+%                   thresholds potentially result in larger communities.
+%
+%   Output:     
+%       M,          Overlapping community-affiliation matrix
+%                   Binary matrix of size CxN [communities x nodes]
+%
+%   Algorithms:
+%       BronKerbosch algorithm for detection of maximal cliques.
+%       Dulmage-Mendelsohn decomposition for detection of components
+%                   (implemented in get_components.m)
+%
+%
+%   Note: This algorithm can be slow and memory intensive in large
+%   matrices. The algorithm requires the function get_components.m
+%
+%   Reference: Palla et al. (2005) Nature 435, 814-818.
+%
+%   Mika Rubinov, Janelia HHMI, 2017
+
+if ~isequal(A, A.')
+    error('A must be undirected.')
+end
+if ~isequal(size(A, 1), size(A, 2))
+    error('A must be square.')
+end
+if ~issparse(A)
+    A = sparse(A);
+end
+if ~islogical(A)
+    A = logical(A);
+end
+
+n = length(A);                                  % number of nodes
+A(1:n+1:end) = 0;                               % clear diagonal
+MQ = maximal_cliques(A, n);                     % get maximal cliques
+Cq = double(cell2mat(MQ)).';                    % convert to matrix
+Cq = Cq(sum(Cq, 2) >= cq_thr, :);               % remove subthreshold cliques
+Ov = Cq * Cq.';                                 % compute clique overlap
+Ov_thr = (Ov >= cq_thr - 1);                    % keep percolating cliques
+
+Cq_components = get_components(Ov_thr);         % find components 
+
+m = max(Cq_components);                         % get number of components
+M = zeros(m, n);                                % collect communities
+for i = 1:m
+    M(i, any( Cq(Cq_components==i, :), 1)) = 1;
+end
+
+end
+
+function MQ = maximal_cliques(A, n)             % Bron-Kerbosch algorithm
+
+MQ = cell(1, 1000*n);
+
+R = false(n, 1);               %current
+P = true(n, 1);                %prospective
+X = false(n, 1);               %processed
+q = 0;
+
+BK(R, P, X);
+
+    function BK(R, P, X)
+        if ~any(P | X)
+            q = q + 1;
+            MQ{q} = R;
+        else
+            U_p = find(any([P X], 2));
+            [~, idx] = max(A(:,U_p).' * double(P));
+            u_p = U_p(idx);
+            
+            U = find(all([P ~A(:,u_p)], 2)).';
+            for u = U
+                Nu = A(:,u);
+                P(u) = 0;
+                Rnew = R; Rnew(u) = 1;
+                Pnew = all([P Nu],2);
+                Xnew = all([X Nu],2);
+                BK(Rnew, Pnew, Xnew)
+                X(u) = 1;
+            end
+        end
+    end
+
+MQ=MQ(1:q);
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bd.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,35 @@
+function C=clustering_coef_bd(A)
+%CLUSTERING_COEF_BD     Clustering coefficient
+%
+%   C = clustering_coef_bd(A);
+%
+%   The clustering coefficient is the fraction of triangles around a node
+%   (equiv. the fraction of node's neighbors that are neighbors of each other).
+%
+%   Input:      A,      binary directed connection matrix
+%
+%   Output:     C,      clustering coefficient vector
+%
+%   Reference: Fagiolo (2007) Phys Rev E 76:026107.
+%
+%
+%   Mika Rubinov, UNSW, 2007-2010
+
+%Methodological note: In directed graphs, 3 nodes generate up to 8 
+%triangles (2*2*2 edges). The number of existing triangles is the main 
+%diagonal of S^3/2. The number of all (in or out) neighbour pairs is 
+%K(K-1)/2. Each neighbour pair may generate two triangles. "False pairs" 
+%are i<->j edge pairs (these do not generate triangles). The number of 
+%false pairs is the main diagonal of A^2.
+%Thus the maximum possible number of triangles = 
+%       = (2 edges)*([ALL PAIRS] - [FALSE PAIRS])
+%       = 2 * (K(K-1)/2 - diag(A^2))
+%       = K(K-1) - 2(diag(A^2))
+
+S=A+A.';                    %symmetrized input graph
+K=sum(S,2);                 %total degree (in + out)
+cyc3=diag(S^3)/2;           %number of 3-cycles (ie. directed triangles)
+K(cyc3==0)=inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+CYC3=K.*(K-1)-2*diag(A^2);	%number of all possible 3-cycles
+C=cyc3./CYC3;               %clustering coefficient
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_bu.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,28 @@
+function C=clustering_coef_bu(G)
+%CLUSTERING_COEF_BU     Clustering coefficient
+%
+%   C = clustering_coef_bu(A);
+%
+%   The clustering coefficient is the fraction of triangles around a node
+%   (equiv. the fraction of node's neighbors that are neighbors of each other).
+%
+%   Input:      A,      binary undirected connection matrix
+%
+%   Output:     C,      clustering coefficient vector
+%
+%   Reference: Watts and Strogatz (1998) Nature 393:440-442.
+%
+%
+%   Mika Rubinov, UNSW, 2007-2010
+
+n=length(G);
+C=zeros(n,1);
+
+for u=1:n
+    V=find(G(u,:));
+    k=length(V);
+    if k>=2                 %degree must be at least 2
+        S=G(V,V);
+        C(u)=sum(S(:))/(k^2-k);
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wd.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,42 @@
+function C=clustering_coef_wd(W)
+%CLUSTERING_COEF_WD     Clustering coefficient
+%
+%   C = clustering_coef_wd(W);
+%
+%   The weighted clustering coefficient is the average "intensity"
+%   (geometric mean) of all triangles associated with each node.
+%
+%   Input:      W,      weighted directed connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     C,      clustering coefficient vector
+%
+%   Reference: Fagiolo (2007) Phys Rev E 76:026107.
+%
+%   Note:   All weights must be between 0 and 1.
+%           This may be achieved using the weight_conversion.m function,
+%           W_nrm = weight_conversion(W, 'normalize');
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification history:
+%   2007: original
+%   2015: expanded documentation
+
+
+%   Methodological note (also see clustering_coef_bd)
+%   The weighted modification is as follows:
+%   - The numerator: adjacency matrix is replaced with weights matrix ^ 1/3
+%   - The denominator: no changes from the binary version
+%
+%   The above reduces to symmetric and/or binary versions of the clustering 
+%   coefficient for respective graphs.
+
+A=W~=0;                     %adjacency matrix
+S=W.^(1/3)+(W.').^(1/3);	%symmetrized weights matrix ^1/3
+K=sum(A+A.',2);            	%total degree (in + out)
+cyc3=diag(S^3)/2;           %number of 3-cycles (ie. directed triangles)
+K(cyc3==0)=inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+CYC3=K.*(K-1)-2*diag(A^2);	%number of all possible 3-cycles
+C=cyc3./CYC3;               %clustering coefficient
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,30 @@
+function C=clustering_coef_wu(W)
+%CLUSTERING_COEF_WU     Clustering coefficient
+%
+%   C = clustering_coef_wu(W);
+%
+%   The weighted clustering coefficient is the average "intensity"
+%   (geometric mean) of all triangles associated with each node.
+%
+%   Input:      W,      weighted undirected connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     C,      clustering coefficient vector
+%
+%   Note:   All weights must be between 0 and 1.
+%           This may be achieved using the weight_conversion.m function,
+%           W_nrm = weight_conversion(W, 'normalize');
+%
+%   Reference: Onnela et al. (2005) Phys Rev E 71:065103
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification history:
+%   2007: original
+%   2015: expanded documentation
+
+K=sum(W~=0,2);            	
+cyc3=diag((W.^(1/3))^3);           
+K(cyc3==0)=inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+C=cyc3./(K.*(K-1));         %clustering coefficient
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/clustering_coef_wu_sign.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,136 @@
+function [C_pos,C_neg,Ctot_pos,Ctot_neg] = clustering_coef_wu_sign(W,coef_type)
+%CLUSTERING_COEF_WU_SIGN     Multiple generalizations of the clustering coefficient 
+%
+%   [C_pos,C_neg,Ctot_pos,Ctot_neg] = clustering_coef_wu_sign(W,coef_type);
+%
+%   The weighted clustering coefficient is the average weight or intensity
+%   of all triangles associated with each node.
+%
+%   Inputs:
+%       W,          
+%           Weighted undirected connection matrix
+%
+%       corr_type,
+%           Desired type of clustering coefficient.
+%           Options:  
+%           1,  (default) Onnela et al. formula, used in original
+%               clustering_coef_wu.m. Computed separately for positive &
+%               negative weights.
+%           2,  Zhang & Horvath formula, similar to Onnela formula except
+%               denominator of Onnela formula relies on binarizing the
+%               network whereas this denominator is based on weight value,
+%               which reduces the sensitivity of this measure to the
+%               weights directly connected to the node of interest.
+%               Computed separately for positive & negative weights.
+%           3,  Constantini & Perugini's generalization of the Zhang &
+%               Horvath formula. This formula takes both positive &
+%               negative weights into account simultaneously, & is
+%               particularly sensitive to non-redundancy in path
+%               information based on sign (i.e., when two weights are
+%               positive & one negative, or all three are negative, both of
+%               which indicate that the weight of the third path is not
+%               redundant information). Produces only one value.
+%
+%
+%   Outputs: 
+%       C_pos/C_neg,
+%           Clustering coefficient vector for positive/negative weights.
+%           For the third option, only one vector is outputted (as C_pos). 
+%       Ctot_pos/Ctot_neg,
+%           Mean clustering coefficient for positive and negative weights.
+%
+%   References: 
+%       Onnela et al. (2005) Phys Rev E 71:065103
+%       Zhang & Horvath (2005) Stat Appl Genet Mol Biol 41:1544-6115
+%       Costantini & Perugini (2014) PLOS ONE 9:e88669
+%
+%
+%   Contributor: Jeff Spielberg, Boston University, 2014-2015
+%                (script based on clustering_coef_wu.m)
+
+%
+%   Modification History:
+%   May 2014: Added computation of pos & neg weights separately & 
+%             computation of mean coefficient (Jeff Spielberg)
+%   May 2015: Added computation of Zhang & Horvath and Constantini & 
+%             Perugini formulas (Jeff Spielberg)
+%   May 2016: Bugfix in computation of the denominator of the Costantini &
+%             Perugini (flag 3) version (Chiara Pintossi)
+
+if ~exist('coef_type','var')
+    coef_type = 1;
+end
+
+n            = length(W);                   %number of nodes
+W(1:n+1:end) = 0;
+
+switch coef_type
+    case 1
+        W_pos                = W.*(W>0);
+        K_pos                = sum(W_pos~=0,2);
+        cyc3_pos             = diag((W_pos.^(1/3))^3);
+        K_pos(cyc3_pos == 0) = inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+        C_pos                = cyc3_pos./(K_pos.*(K_pos-1));         %clustering coefficient
+        Ctot_pos             = mean(C_pos);
+        
+        W_neg                = -W.*(W<0);
+        K_neg                = sum(W_neg~=0,2);
+        cyc3_neg             = diag((W_neg.^(1/3))^3);
+        K_neg(cyc3_neg == 0) = inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+        C_neg                = cyc3_neg./(K_neg.*(K_neg-1));         %clustering coefficient
+        Ctot_neg             = mean(C_neg);
+    case 2
+        W_pos    = W.*(W>0);
+        cyc3_pos = zeros(n,1);
+        cyc2_pos = zeros(n,1);
+        for i = 1:n
+            for j = 1:n
+                for q = 1:n
+                    cyc3_pos(i) = cyc3_pos(i)+(W_pos(j,i)*W_pos(i,q)*W_pos(j,q));
+                    if j~=q
+                        cyc2_pos(i) = cyc2_pos(i)+(W_pos(j,i)*W_pos(i,q));
+                    end
+                end
+            end
+        end
+        cyc2_pos(cyc3_pos == 0) = inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+        C_pos                   = cyc3_pos./cyc2_pos;         %clustering coefficient
+        Ctot_pos                = mean(C_pos);
+        
+        W_neg    = -W.*(W<0);
+        cyc3_neg = zeros(n,1);
+        cyc2_neg = zeros(n,1);
+        for i = 1:n
+            for j = 1:n
+                for q = 1:n
+                    cyc3_neg(i) = cyc3_neg(i)+(W_neg(j,i)*W_neg(i,q)*W_neg(j,q));
+                    if j~=q
+                        cyc2_neg(i) = cyc2_neg(i)+(W_neg(j,i)*W_neg(i,q));
+                    end
+                end
+            end
+        end
+        cyc2_neg(cyc3_neg == 0) = inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+        C_neg                   = cyc3_neg./cyc2_neg;         %clustering coefficient
+        Ctot_neg                = mean(C_neg);
+    case 3
+        cyc3         = zeros(n,1);
+        cyc2         = zeros(n,1);
+        
+        for i = 1:n
+            for j = 1:n
+                for q = 1:n
+                    cyc3(i) = cyc3(i)+(W(j,i)*W(i,q)*W(j,q));
+                    if j~=q
+                        cyc2(i) = cyc2(i)+abs(W(j,i)*W(i,q));
+                    end
+                end
+            end
+        end
+        
+        cyc2(cyc3 == 0) = inf;             %if no 3-cycles exist, make C=0 (via K=inf)
+        C_pos           = cyc3./cyc2;         %clustering coefficient
+        Ctot_pos        = mean(C_pos);
+        C_neg           = nan(size(C_pos));
+        Ctot_neg        = nan(size(Ctot_pos));
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/community_louvain.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/community_louvain.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/community_louvain.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/community_louvain.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,198 @@
+function [M,Q]=community_louvain(W,gamma,M0,B)
+%COMMUNITY_LOUVAIN     Optimal community structure
+%
+%   M     = community_louvain(W);
+%   [M,Q] = community_louvain(W,gamma);
+%   [M,Q] = community_louvain(W,gamma,M0);
+%   [M,Q] = community_louvain(W,gamma,M0,'potts');
+%   [M,Q] = community_louvain(W,gamma,M0,'negative_asym');
+%   [M,Q] = community_louvain(W,[],[],B);
+%
+%   The optimal community structure is a subdivision of the network into
+%   nonoverlapping groups of nodes which maximizes the number of within-
+%	group edges, and minimizes the number of between-group edges.
+%
+%   This function is a fast and accurate multi-iterative generalization of
+%   the Louvain community detection algorithm. This function subsumes and
+%   improves upon,
+%		modularity_louvain_und.m, modularity_finetune_und.m,
+%		modularity_louvain_dir.m, modularity_finetune_dir.m,
+%       modularity_louvain_und_sign.m
+%	and additionally allows to optimize other objective functions (includes
+%	built-in Potts-model Hamiltonian, allows for custom objective-function
+%	matrices).
+%
+%   Inputs:
+%       W,
+%           directed/undirected weighted/binary connection matrix with
+%           positive and possibly negative weights.
+%       gamma,
+%           resolution parameter (optional)
+%               gamma>1,        detects smaller modules
+%               0<=gamma<1,     detects larger modules
+%               gamma=1,        classic modularity (default)
+%       M0,
+%           initial community affiliation vector (optional)
+%       B,
+%           objective-function type or custom objective matrix (optional)
+%           'modularity',       modularity (default)
+%           'potts',            Potts-model Hamiltonian (for binary networks)
+%           'negative_sym',     symmetric treatment of negative weights
+%           'negative_asym',    asymmetric treatment of negative weights
+%           B,                  custom objective-function matrix
+%
+%           Note: see Rubinov and Sporns (2011) for a discussion of
+%           symmetric vs. asymmetric treatment of negative weights.
+%
+%   Outputs:
+%       M,
+%           community affiliation vector
+%       Q,
+%           optimized community-structure statistic (modularity by default)
+%
+%   Example:
+%       % Iterative community finetuning.
+%       % W is the input connection matrix.
+%       n  = size(W,1);             % number of nodes
+%       M  = 1:n;                   % initial community affiliations
+%       Q0 = -1; Q1 = 0;            % initialize modularity values
+%       while Q1-Q0>1e-5;           % while modularity increases
+%           Q0 = Q1;                % perform community detection
+%           [M, Q1] = community_louvain(W, [], M);
+%       end
+%
+%   References:
+%       Blondel et al. (2008)  J. Stat. Mech. P10008.
+%       Reichardt and Bornholdt (2006) Phys. Rev. E 74, 016110.
+%       Ronhovde and Nussinov (2008) Phys. Rev. E 80, 016109
+%       Sun et al. (2008) Europhysics Lett 86, 28004.
+%       Rubinov and Sporns (2011) Neuroimage 56:2068-79.
+%
+%   Mika Rubinov, U Cambridge 2015-2016
+
+%   Modification history
+%   2015: Original
+%   2016: Included generalization for negative weights.
+%         Enforced binary network input for Potts-model Hamiltonian.
+%         Streamlined code and expanded documentation.
+
+W=double(W);                                % convert to double format
+n=length(W);                                % get number of nodes
+s=sum(sum(W));                              % get sum of edges
+
+if ~exist('B','var') || isempty(B)
+    type_B = 'modularity';
+elseif ischar(B)
+    type_B = B;
+else
+    type_B = 0;
+    if exist('gamma','var') && ~isempty(gamma)
+        warning('Value of gamma is ignored in generalized mode.')
+    end
+end
+if ~exist('gamma','var') || isempty(gamma)
+    gamma = 1;
+end
+
+if strcmp(type_B,'negative_sym') || strcmp(type_B,'negative_asym')
+    W0 = W.*(W>0);                          %positive weights matrix
+    s0 = sum(sum(W0));                      %weight of positive links
+    B0 = W0-gamma*(sum(W0,2)*sum(W0,1))/s0; %positive modularity
+    
+    W1 =-W.*(W<0);                          %negative weights matrix
+    s1 = sum(sum(W1));                      %weight of negative links
+    if s1                                   %negative modularity
+        B1 = W1-gamma*(sum(W1,2)*sum(W1,1))/s1;
+    else
+        B1 = 0;
+    end
+elseif min(min(W))<-1e-10
+    err_string = [
+        'The input connection matrix contains negative weights.\nSpecify ' ...
+        '''negative_sym'' or ''negative_asym'' objective-function types.'];
+    error(sprintf(err_string))              %#ok<SPERR>
+end
+if strcmp(type_B,'potts') && any(any(W ~= logical(W)))
+    error('Potts-model Hamiltonian requires a binary W.')
+end
+
+if type_B
+    switch type_B
+        case 'modularity';      B = (W-gamma*(sum(W,2)*sum(W,1))/s)/s;
+        case 'potts';           B =  W-gamma*(~W);
+        case 'negative_sym';    B = B0/(s0+s1) - B1/(s0+s1);
+        case 'negative_asym';   B = B0/s0      - B1/(s0+s1);
+        otherwise;              error('Unknown objective function.');
+    end
+else                            % custom objective function matrix as input
+    B = double(B);
+    if ~isequal(size(W),size(B))
+        error('W and B must have the same size.')
+    end
+end
+if ~exist('M0','var') || isempty(M0)
+    M0=1:n;
+elseif numel(M0)~=n
+    error('M0 must contain n elements.')
+end
+
+[~,~,Mb] = unique(M0);
+M = Mb;
+
+B = (B+B.')/2;                                          % symmetrize modularity matrix
+Hnm=zeros(n,n);                                         % node-to-module degree
+for m=1:max(Mb)                                         % loop over modules
+    Hnm(:,m)=sum(B(:,Mb==m),2);
+end
+
+Q0 = -inf;
+Q = sum(B(bsxfun(@eq,M0,M0.')));                        % compute modularity
+first_iteration = true;
+while Q-Q0>1e-10
+    flag = true;                                        % flag for within-hierarchy search
+    while flag
+        flag = false;
+        for u=randperm(n)                               % loop over all nodes in random order
+            ma = Mb(u);                                 % current module of u
+            dQ = Hnm(u,:) - Hnm(u,ma) + B(u,u);
+            dQ(ma) = 0;                                 % (line above) algorithm condition
+            
+            [max_dQ,mb] = max(dQ);                      % maximal increase in modularity and corresponding module
+            if max_dQ>1e-10                             % if maximal increase is positive
+                flag = true;
+                Mb(u) = mb;                             % reassign module
+                
+                Hnm(:,mb) = Hnm(:,mb)+B(:,u);           % change node-to-module strengths
+                Hnm(:,ma) = Hnm(:,ma)-B(:,u);
+            end
+        end
+    end
+    [~,~,Mb] = unique(Mb);                              % new module assignments
+    
+    M0 = M;
+    if first_iteration
+        M=Mb;
+        first_iteration=false;
+    else
+        for u=1:n                                       % loop through initial module assignments
+            M(M0==u)=Mb(u);                             % assign new modules
+        end
+    end
+    
+    n=max(Mb);                                          % new number of modules
+    B1=zeros(n);                                        % new weighted matrix
+    for u=1:n
+        for v=u:n
+            bm=sum(sum(B(Mb==u,Mb==v)));                % pool weights of nodes in same module
+            B1(u,v)=bm;
+            B1(v,u)=bm;
+        end
+    end
+    B=B1;
+    
+    Mb=1:n;                                             % initial module assignments
+    Hnm=B;                                              % node-to-module strength
+    
+    Q0=Q;
+    Q=trace(B);                                         % compute modularity
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/consensus_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/consensus_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/consensus_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/consensus_und.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,96 @@
+function ciu = consensus_und(d,tau,reps)
+% CONSENSUS_UND      Consensus clustering
+%
+%   CIU = CONSENSUS(D,TAU,REPS) seeks a consensus partition of the 
+%   agreement matrix D. The algorithm used here is almost identical to the
+%   one introduced in Lancichinetti & Fortunato (2012): The agreement
+%   matrix D is thresholded at a level TAU to remove an weak elements. The
+%   resulting matrix is then partitions REPS number of times using the
+%   Louvain algorithm (in principle, any clustering algorithm that can
+%   handle weighted matrixes is a suitable alternative to the Louvain
+%   algorithm and can be substituted in its place). This clustering
+%   produces a set of partitions from which a new agreement is built. If
+%   the partitions have not converged to a single representative partition,
+%   the above process repeats itself, starting with the newly built
+%   agreement matrix.
+%
+%   NOTE: In this implementation, the elements of the agreement matrix must
+%   be converted into probabilities.
+%
+%   NOTE: This implementation is slightly different from the original
+%   algorithm proposed by Lanchichinetti & Fortunato. In its original
+%   version, if the thresholding produces singleton communities, those
+%   nodes are reconnected to the network. Here, we leave any singleton
+%   communities disconnected.
+%
+%   Inputs:     D,      agreement matrix with entries between 0 and 1
+%                       denoting the probability of finding node i in the
+%                       same cluster as node j
+%               TAU,    threshold which controls the resolution of the
+%                       reclustering
+%               REPS,   number of times that the clustering algorithm is
+%                       reapplied
+%
+%   Outputs:    CIU,    consensus partition
+%
+%   References: Lancichinetti & Fortunato (2012). Consensus clustering in
+%   complex networks. Scientific Reports 2, Article number: 336.
+%
+%   Richard Betzel, Indiana University, 2012
+%
+%   modified on 3/2014 to include "unique_partitions"
+
+n = length(d); flg = 1;
+while flg == 1
+    
+    flg = 0;
+    dt = d.*(d >= tau).*~eye(n);
+    if nnz(dt) == 0
+        ciu = (1:n)';
+    else
+        ci = zeros(n,reps);
+        for iter = 1:reps
+            ci(:,iter) = community_louvain(dt);
+        end
+        ci = relabel_partitions(ci);
+        ciu = unique_partitions(ci);
+        nu = size(ciu,2);
+        if nu > 1
+            flg = 1;
+            d = agreement(ci)./reps;
+        end
+    end
+    
+end
+
+function cinew = relabel_partitions(ci)
+[n,m] = size(ci);
+cinew = zeros(n,m);
+for i = 1:m
+    c = ci(:,i);
+    d = zeros(size(c));
+    count = 0;
+    while sum(d ~= 0) < n
+        count = count + 1;
+        ind = find(c,1,'first');
+        tgt = c(ind);
+        rep = c == tgt;
+        d(rep) = count;
+        c(rep) = 0;
+    end
+    cinew(:,i) = d;
+end
+
+function ciu = unique_partitions(ci)
+ci = relabel_partitions(ci);
+ciu = [];
+count = 0;
+c = 1:size(ci,2);
+while ~isempty(ci)
+    count = count + 1;
+    tgt = ci(:,1);
+    ciu = [ciu,tgt];                %#ok<AGROW>
+    dff = sum(abs(bsxfun(@minus,ci,tgt))) == 0;
+    ci(:,dff) = [];
+    c(dff) = [];
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/core_periphery_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/core_periphery_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/core_periphery_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/core_periphery_dir.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,88 @@
+function [C, q]=core_periphery_dir(W,gamm,C)
+%CORE_PERIPHERY_DIR     Core/periphery structure and core-ness statistic
+%
+%   C     = core_periphery_dir(W)
+%   [C,q] = core_periphery_dir(W,gamm,C0)
+%
+%   The optimal core/periphery subdivision is a partition of the network
+%   into two non-overlapping groups of nodes, a core group and a periphery
+%   group, in a way that maximizes the number/weight of within core-group
+%   edges, and minimizes the number/weight of within periphery-group edges.
+%
+%   The core-ness is a statistic which quantifies the goodness of the
+%   optimal core/periphery subdivision.
+%
+%   Input:      W       directed (weighted or binary) connection matrix.
+%               gamma,  core-ness resolution parameter (optional)
+%                       gamma>1     detects small core/large periphery
+%                       0<=gamma<1  detects large core/small periphery
+%                       default is gamma=1
+%
+%   Outputs:    C,      binary vector of optimal core structure
+%                       C = 1 represents nodes in the core
+%                       C = 0 represents nodes in the periphery
+%               q,      maximized core-ness statistic
+%
+%   Algorithm: A version of Kernighan-Lin algorithm for graph partitioning
+%   used in community detection (Newman, 2006) applied to optimize a
+%   core-structure objective described in Borgatti and Everett (2000).
+%
+%   Reference: Borgatti and Everett (2000) Soc Networks 21:375395.
+%              Newman (2006) Phys Rev E 74:036104, PNAS 23:8577-8582.
+%              Rubinov, Ypma et al. (2015) PNAS 112:10032-7
+%
+%   2015, Mika Rubinov, U Cambridge
+
+n = length(W);                              % number of nodes
+W = double(W);                              % convert from logical
+W(1:n+1:end) = 0;                           % clear diagonal
+if ~exist('gamm','var')
+    gamm = 1;
+end
+if ~exist('C','var')
+    C = (rand(1,n)<0.5);
+else
+    C = logical(reshape(C,1,n));
+end
+
+% Methodological note: cf. community detection, the core-detection
+% null model is not corrected for degree (to enable detection of hubs).
+s = sum(W(:));
+p = mean(W(:));
+b = W - gamm*p;
+B = (b+b.')/(2*s);                          % directed core-ness matrix
+q = sum(sum(B(C,C))) - sum(sum(B(~C,~C)));  % core-ness statistic
+
+f=1;                                        % loop flag
+while f
+    f=0;
+    Idx = 1:n;                              % initial node indices
+    Ct = C;
+    while any(Idx)
+        Qt = zeros(1,n);                    % check swaps of node indices
+        q0 = sum(sum(B(Ct,Ct))) - sum(sum(B(~Ct,~Ct)));
+        Qt( Ct) = q0 - 2*sum(B( Ct, :),2);
+        Qt(~Ct) = q0 + 2*sum(B(~Ct, :),2);
+        
+        %%% verification that the above update is equivalent to:
+        % for u=Idx
+        %     Ct(u) = ~Ct(u);
+        %     Qt(u) = sum(sum(B(Ct,Ct))) - sum(sum(B(~Ct,~Ct)));
+        %     Ct(u) = ~Ct(u);
+        % end
+        
+        max_Qt = max(Qt(Idx));              % make swap with maximal
+        u = find(abs(Qt(Idx)-max_Qt)<1e-10);% increase in core-ness
+        u = u(randi(numel(u)));
+        Ct(Idx(u)) = ~Ct(Idx(u));
+        Idx(u)=[];                          % remove index from consideration
+        
+        if max_Qt-q>1e-10                   % recompute core-ness statistic
+            f = 1;
+            C = Ct;
+            q = sum(sum(B(C,C))) - sum(sum(B(~C,~C)));
+        end
+    end
+end
+
+q = sum(sum(B(C,C))) - sum(sum(B(~C,~C)));  % return core-ness statistic
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/cycprob.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/cycprob.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/cycprob.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/cycprob.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,45 @@
+function [fcyc,pcyc] = cycprob(Pq)
+% CYCPROB       Cycle probability
+%
+%   [fcyc,pcyc] = cycprob(Pq);
+%
+%   Cycles are paths which begin and end at the same node. Cycle 
+%   probability for path length d, is the fraction of all paths of length 
+%   d-1 that may be extended to form cycles of length d.
+%
+%   Input:      Pq,     3D matrix, with Pq(i,j,q) = number of paths from 
+%                       'i' to 'j' of length 'q' (produced by 'findpaths')
+%
+%   Outputs:    fcyc,   fraction of all paths that are cycles for each path
+%                       length 'q'. 
+%               pcyc,   probability that a non-cyclic path of length 'q-1' 
+%                       can be extended to form a cycle of length 'q', for 
+%                       each path length 'q', 
+%
+%
+% Olaf Sporns, Indiana University, 2002/2007/2008
+
+
+% Note: fcyc(1) must be zero, as there cannot be cycles of length one.
+fcyc = zeros(1,size(Pq,3));
+for q=1:size(Pq,3)
+   if(sum(sum(Pq(:,:,q)))>0)
+      fcyc(q) = sum(diag(Pq(:,:,q)))/sum(sum(Pq(:,:,q)));
+   else
+      fcyc(q) = 0;
+   end;
+end;
+
+% Note: pcyc(1) is not defined (set to zero).
+% Note: pcyc(2) is equal to the fraction of reciprocal connections, 
+%       'frecip', delivered by 'reciprocal.m'.
+% Note: there are no non-cyclic paths of length N and no cycles of length N+1
+pcyc = zeros(1,size(Pq,3));
+for q=2:size(Pq,3)
+   if((sum(sum(Pq(:,:,q-1)))-sum(diag(Pq(:,:,q-1))))>0)
+      pcyc(q) = sum(diag(Pq(:,:,q)))/...
+                (sum(sum(Pq(:,:,q-1)))-sum(diag(Pq(:,:,q-1))));
+   else
+      pcyc(q) = 0;
+   end;
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_efficiency_measures.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_efficiency_measures.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_efficiency_measures.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_efficiency_measures.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,116 @@
+% This script loads 7 unweighted, undirected adjacency matrices
+% corresponding to a clique, chain, ring, 1D lattice, star, rich-club, and
+% bi-modular toy networks (all graphs have 50 nodes). The following
+% efficiency measures are computed for each graph:
+%
+%  - prob_SPL: probability of one particle traveling through shortest-paths
+%  - Erout: efficiency of routing -> based on shortest-paths
+%  - Ediff: efficiency of diffusion -> based on mean-first-passage-times
+%  - Eres: efficiency of resources -> based on number of particles
+%    necessary so that at least one particle taking shortest-paths with
+%    certain probability (lambda).
+%
+%  If you are using this efficiency package for your research, plase kindly
+%  cite the paper:
+%
+%  "Exploring the Morphospace of Communication Efficiency in Complex
+%  Networks" Goi J, Avena-Koenigsberger A, Velez de Mendizabal N, van den
+%  Heuvel M, Betzel RF and Sporns O. PLoS ONE. 2013
+%
+%  These examples and results correspond to Table 1 in the paper.
+%
+%  Joaquin Goi and Andrea Avena-Koenigsberger, IU Bloomington, 2012
+
+close all;
+clear all;
+clc;
+
+load demo_efficiency_measures_data.mat; % 7 adjacency matrices corresponding to the examples shown in Table 1 are loaded.
+lambda = 0.5; % this parameter is an input for the computation of Eres.
+
+% run and display efficiency measures for the 7 graphs
+disp(['    prob_SPL  ','  Erout  ','  Ediff  ','  Eres  '])
+
+fprintf('----- clique ----- \n')
+adj = clique;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- chain ----- \n')
+adj = chain;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- ring ----- \n')
+adj = ring;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- lattice1D ----- \n')
+adj = lattice1D;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- star ----- \n')
+adj = star;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- rich-club ----- \n')
+adj = rich_club;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
+
+fprintf('----- bi-modular ----- \n')
+adj = bi_modular;
+N = size(adj,1);
+EYE = logical(eye(N,N));
+SPL = distance_wei_floyd(adj);
+Erout = rout_efficiency(adj);
+Ediff = diffusion_efficiency(adj);
+[Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL);
+prob_SPL = mean(prob_SPL(~EYE));
+Eres = mean(Eres(~EYE));
+disp([prob_SPL,Erout,Ediff,Eres])
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_geometric.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_geometric.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_geometric.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_geometric.m	2019-03-03 19:42:52.000000000 +0100
@@ -0,0 +1,52 @@
+% Demonstration of generative model functions.
+%
+% See GENERATIVE_MODEL and EVALUATE_GENERATIVE_MODEL for further details
+% and interpretation.
+
+clear
+close all
+clc
+
+data = load('demo_generative_models_data');
+A     = data.A;
+Aseed = data.Aseed;
+D     = data.D;
+
+% get cardinality of network
+n = length(A);
+
+% set model type
+modeltype = 'sptl';
+
+% set whether the model is based on powerlaw or exponentials
+modelvar = [{'powerlaw'},{'powerlaw'}];
+
+% choose some model parameters
+nparams = 100;
+params = unifrnd(-10,0,nparams,1);
+
+% generate synthetic networks and energy for the neighbors model;
+[B,E,K] = evaluate_generative_model(Aseed,A,D,modeltype,modelvar,params);
+X = [E,K];
+
+% show scatterplot of parameter values versus energy and KS statistics
+names = [...
+    {'energy'},...
+    {'degree'},...
+    {'clustering'},...
+    {'betweenness'},...
+    {'edge length'}];
+
+f = figure(...
+    'units','inches',...
+    'position',[2,2,4,4]);
+for i = 1:size(X,2)
+    subplot(3,2,i);
+    scatter(params,X(:,i),100,X(:,i),'filled');
+    set(gca,...
+        'ylim',[0,1],...
+        'clim',[0,1]);
+    colormap(jet);
+    xlabel('geometric parameter, \eta');
+    ylabel(names{i});
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_neighbors.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_neighbors.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_neighbors.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/data_and_demos/demo_generative_models_neighbors.m	2019-03-03 19:42:52.000000000 +0100
@@ -0,0 +1,52 @@
+% Demonstration of generative model functions.
+%
+% See GENERATIVE_MODEL and EVALUATE_GENERATIVE_MODEL for further details
+% and interpretation.
+
+clear
+close all
+clc
+
+data = load('demo_generative_models_data');
+A     = data.A;
+Aseed = data.Aseed;
+D     = data.D;
+
+% get cardinality of network
+n = length(A);
+
+% set model type
+modeltype = 'matching';
+
+% set whether the model is based on powerlaw or exponentials
+modelvar = [{'powerlaw'},{'powerlaw'}];
+
+% choose some model parameters
+nparams = 100;
+params = [unifrnd(-10,0,nparams,1), unifrnd(-1,1,nparams,1)];
+
+% generate synthetic networks and energy for the neighbors model;
+[B,E,K] = evaluate_generative_model(Aseed,A,D,modeltype,modelvar,params);
+X = [E,K];
+
+% show scatterplot of parameter values versus energy and KS statistics
+names = [...
+    {'energy'},...
+    {'degree'},...
+    {'clustering'},...
+    {'betweenness'},...
+    {'edge length'}];
+
+f = figure(...
+    'units','inches',...
+    'position',[2,2,4,4]);
+for i = 1:size(X,2)
+    subplot(3,2,i);
+    scatter(params(:,1),params(:,2),100,X(:,i),'filled');
+    set(gca,...
+        'clim',[0,1]);
+    colormap(jet);
+    xlabel('geometric parameter, \eta');
+    ylabel('topological parameter, \gamma');
+    title(names{i});
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_dir.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,31 @@
+function [id,od,deg] = degrees_dir(CIJ)
+%DEGREES_DIR        Indegree and outdegree
+%
+%   [id,od,deg] = degrees_dir(CIJ);
+%
+%   Node degree is the number of links connected to the node. The indegree 
+%   is the number of inward links and the outdegree is the number of 
+%   outward links.
+%
+%   Input:      CIJ,    directed (binary/weighted) connection matrix
+%
+%   Output:     id,     node indegree
+%               od,     node outdegree
+%               deg,    node degree (indegree + outdegree)
+%
+%   Notes:  Inputs are assumed to be on the columns of the CIJ matrix.
+%           Weight information is discarded.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2006/2008
+
+
+% ensure CIJ is binary...
+CIJ = double(CIJ~=0);
+
+% compute degrees
+id = sum(CIJ,1);    % indegree = column sum of CIJ
+od = sum(CIJ,2)';   % outdegree = row sum of CIJ
+deg = id+od;        % degree = indegree+outdegree
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/degrees_und.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,22 @@
+function [deg] = degrees_und(CIJ)
+%DEGREES_UND        Degree
+%
+%   deg = degrees_und(CIJ);
+%
+%   Node degree is the number of links connected to the node.
+%
+%   Input:      CIJ,    undirected (binary/weighted) connection matrix
+%
+%   Output:     deg,    node degree
+%
+%   Note: Weight information is discarded.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2006/2008
+
+
+% ensure CIJ is binary...
+CIJ = double(CIJ~=0);
+
+deg = sum(CIJ);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/density_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/density_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/density_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/density_dir.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,24 @@
+function [kden,N,K] = density_dir(CIJ)
+% DENSITY_DIR        Density
+%
+%   kden = density_dir(CIJ);
+%   [kden,N,K] = density_dir(CIJ);
+%
+%   Density is the fraction of present connections to possible connections.
+%
+%   Input:      CIJ,    directed (weighted/binary) connection matrix
+%
+%   Output:     kden,   density
+%               N,      number of vertices
+%               K,      number of edges
+%
+%   Notes:  Assumes CIJ is directed and has no self-connections.
+%           Weight information is discarded.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+N = size(CIJ,1);
+K = nnz(CIJ);
+kden = K/(N^2-N);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/density_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/density_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/density_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/density_und.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,28 @@
+function [kden,N,K] = density_und(CIJ)
+% DENSITY_UND        Density
+%
+%   kden = density_und(CIJ);
+%   [kden,N,K] = density_und(CIJ);
+%
+%   Density is the fraction of present connections to possible connections.
+%
+%   Input:      CIJ,    undirected (weighted/binary) connection matrix
+%
+%   Output:     kden,   density
+%               N,      number of vertices
+%               K,      number of edges
+%
+%   Notes:  Assumes CIJ is undirected and has no self-connections.
+%           Weight information is discarded.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+
+% Modification history:
+% 2009-10: K fixed to sum over one half of CIJ [Tony Herdman, SFU]
+
+N = size(CIJ,1);
+K = nnz(triu(CIJ));
+kden = K/((N^2-N)/2);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/diffusion_efficiency.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/diffusion_efficiency.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/diffusion_efficiency.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/diffusion_efficiency.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,33 @@
+function [GEdiff,Ediff] = diffusion_efficiency(adj)
+% DIFFUSION_EFFICIENCY      Global mean and pair-wise diffusion efficiency
+%
+%   [GEdiff,Ediff] = diffusion_efficiency(adj);
+%
+%   The diffusion efficiency between nodes i and j is the inverse of the
+%   mean first passage time from i to j, that is the expected number of
+%   steps it takes a random walker starting at node i to arrive for the
+%   first time at node j. Note that the mean first passage time is not a
+%   symmetric measure -- mfpt(i,j) may be different from mfpt(j,i) -- and
+%   the pair-wise diffusion efficiency matrix is hence also not symmetric.
+%
+%
+%   Input:
+%       adj,    Weighted/Unweighted, directed/undirected adjacency matrix
+%
+%
+%   Outputs:
+%       GEdiff, Mean Global diffusion efficiency (scalar)
+%       Ediff,  Pair-wise diffusion efficiency (matrix)
+%
+%
+%   References: Goi J, et al (2013) PLoS ONE
+%
+%   Joaquin Goi and Andrea Avena-Koenigsberger, IU Bloomington, 2012
+
+
+n = size(adj,1);
+mfpt = mean_first_passage_time(adj);
+Ediff = 1./mfpt;
+Ediff(eye(n)>0) = 0;
+GEdiff = sum(Ediff(~eye(n)>0))/(n^2-n);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_bin.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,45 @@
+function D=distance_bin(A)
+%DISTANCE_BIN       Distance matrix
+%
+%   D = distance_bin(A);
+%
+%   The distance matrix contains lengths of shortest paths between all
+%   pairs of nodes. An entry (u,v) represents the length of shortest path 
+%   from node u to node v. The average shortest path length is the 
+%   characteristic path length of the network.
+%
+%   Input:      A,      binary directed/undirected connection matrix
+%
+%   Output:     D,      distance matrix
+%
+%   Notes: 
+%       Lengths between disconnected nodes are set to Inf.
+%       Lengths on the main diagonal are set to 0.
+%
+%   Algorithm: Algebraic shortest paths.
+%
+%
+%   Mika Rubinov, U Cambridge
+%   Jonathan Clayden, UCL
+%   2007-2013
+
+% Modification history:
+% 2007: Original (MR)
+% 2013: Bug fix, enforce zero distance for self-connections (JC)
+
+A=double(A~=0);                 %binarize and convert to double format
+
+l=1;                            %path length
+Lpath=A;                        %matrix of paths l
+D=A;                            %distance matrix
+
+Idx=true;
+while any(Idx(:))
+    l=l+1;
+    Lpath=Lpath*A;
+    Idx=(Lpath~=0)&(D==0);
+    D(Idx)=l;
+end
+
+D(~D)=inf;                      %assign inf to disconnected nodes
+D(1:length(A)+1:end)=0;         %clear diagonal
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,71 @@
+function [D,B]=distance_wei(L)
+% DISTANCE_WEI       Distance matrix (Dijkstra's algorithm)
+%
+%   D = distance_wei(L);
+%   [D,B] = distance_wei(L);
+%
+%   The distance matrix contains lengths of shortest paths between all
+%   pairs of nodes. An entry (u,v) represents the length of shortest path 
+%   from node u to node v. The average shortest path length is the 
+%   characteristic path length of the network.
+%
+%   Input:      L,      Directed/undirected connection-length matrix.
+%   *** NB: The length matrix L isn't the weights matrix W (see below) ***
+%
+%   Output:     D,      distance (shortest weighted path) matrix
+%               B,      number of edges in shortest weighted path matrix
+%
+%   Notes:
+%       The input matrix must be a connection-length matrix, typically
+%   obtained via a mapping from weight to length. For instance, in a
+%   weighted correlation network higher correlations are more naturally
+%   interpreted as shorter distances and the input matrix should
+%   consequently be some inverse of the connectivity matrix. 
+%       The number of edges in shortest weighted paths may in general 
+%   exceed the number of edges in shortest binary paths (i.e. shortest
+%   paths computed on the binarized connectivity matrix), because shortest 
+%   weighted paths have the minimal weighted distance, but not necessarily 
+%   the minimal number of edges.
+%       Lengths between disconnected nodes are set to Inf.
+%       Lengths on the main diagonal are set to 0.
+%
+%   Algorithm: Dijkstra's algorithm.
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2012.
+%   Rick Betzel and Andrea Avena, IU, 2012
+
+%Modification history
+%2007: original (MR)
+%2009-08-04: min() function vectorized (MR)
+%2012: added number of edges in shortest path as additional output (RB/AA)
+%2013: variable names changed for consistency with other functions (MR)
+
+n=length(L);
+D=inf(n);
+D(1:n+1:end)=0;                             %distance matrix
+B=zeros(n);                                 %number of edges matrix
+
+for u=1:n
+    S=true(1,n);                            %distance permanence (true is temporary)
+    L1=L;
+    V=u;
+    while 1
+        S(V)=0;                             %distance u->V is now permanent
+        L1(:,V)=0;                          %no in-edges as already shortest
+        for v=V
+            T=find(L1(v,:));                %neighbours of shortest nodes
+            [d,wi]=min([D(u,T);D(u,v)+L1(v,T)]);
+            D(u,T)=d;                       %smallest of old/new path lengths
+            ind=T(wi==2);                   %indices of lengthened paths
+            B(u,ind)=B(u,v)+1;              %increment no. of edges in lengthened paths
+        end
+
+        minD=min(D(u,S));
+        if isempty(minD)||isinf(minD)       %isempty: all nodes reached;
+            break,                          %isinf: some nodes cannot be reached
+        end;
+
+        V=find(D(u,:)==minD);
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei_floyd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei_floyd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei_floyd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/distance_wei_floyd.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,121 @@
+function [SPL,hops,Pmat] = distance_wei_floyd(D,transform)
+% DISTANCE_WEI_FLOYD        Distance matrix (Floyd-Warshall algorithm)
+%
+%   [SPL,hops,Pmat] = distance_wei_floyd(D,transform)
+%
+%   Computes the topological length of the shortest possible path
+%   connecting every pair of nodes in the network.
+%
+%   Inputs:
+%
+%       D,
+%           Weighted/unweighted directed/undirected 
+%           connection *weight* OR *length* matrix.
+%
+%       transform,
+%           If the input matrix is a connection *weight* matrix, specify a
+%           transform that map input connection weights to connection
+%           lengths. Two transforms are available.
+%               'log' -> l_ij = -log(w_ij)
+%               'inv' -> l_ij =    1/w_ij
+%
+%           If the input matrix is a connection *length* matrix, do not
+%           specify a transform (or specify an empty transform argument).
+%
+%
+%   Outputs:
+%
+%       SPL,
+%           Unweighted/Weighted shortest path-length matrix.
+%           If W is directed matrix, then SPL is not symmetric.
+%
+%       hops,
+%           Number of edges in the shortest path matrix. If W is
+%           unweighted, SPL and hops are identical.
+%
+%       Pmat,
+%           Elements {i,j} of this matrix indicate the next node in the
+%           shortest path between i and j. This matrix is used as an input
+%           argument for function 'retrieve_shortest_path.m', which returns
+%           as output the sequence of nodes comprising the shortest path
+%           between a given pair of nodes.
+%
+%
+%   Notes:
+%
+%       There may be more than one shortest path between any pair of nodes
+%       in the network. Non-unique shortest paths are termed shortest path
+%       degeneracies, and are most likely to occur in unweighted networks.
+%       When the shortest-path is degenerate, The elements of matrix Pmat
+%       correspond to the first shortest path discovered by the algorithm.
+%
+%       The input matrix may be either a connection weight matrix, or a
+%       connection length matrix. The connection length matrix is typically
+%       obtained with a mapping from weight to length, such that higher
+%       weights are mapped to shorter lengths (see above).
+%
+%
+%   Algorithm:  FloydWarshall Algorithm
+%
+%
+%   Andrea Avena-Koenigsberger, IU, 2012
+
+%   Modification history
+%   2016 - included transform variable that maps weights to lengths
+
+if exist('transform','var') && ~isempty(transform)
+    
+    switch transform
+        
+        case 'log'
+            
+            if any((D<0) & D>1)
+                error('connection-strengths must be in the interval [0,1) to use the transform -log(w_ij) \n')
+            else
+                SPL = -log(D);
+            end
+            
+        case 'inv'
+            
+            SPL = 1./D;
+            
+        otherwise
+            
+            error('Unexpected transform type. Only "log" and "inv" are accepted \n')
+    end
+    
+else    % the input is a connection lengths matrix.
+    SPL = D;
+    SPL(SPL == 0) = inf;
+end
+
+n=size(D,2);
+
+if nargout > 1
+    flag_find_paths = true;
+    hops = double(D ~= 0);
+    Pmat = 1:n;
+    Pmat = Pmat(ones(n,1),:);
+else
+    flag_find_paths = false;
+end
+
+for k=1:n
+    i2k_k2j = bsxfun(@plus, SPL(:,k), SPL(k,:));
+    
+    if flag_find_paths
+        path = bsxfun(@gt, SPL, i2k_k2j);
+        [i,j] = find(path);
+        hops(path) = hops(i,k) + hops(k,j)';
+        Pmat(path) = Pmat(i,k);
+    end
+    
+    SPL = min(SPL, i2k_k2j);
+end
+
+SPL(eye(n)>0)=0;
+
+if flag_find_paths
+    hops(eye(n)>0)=0;
+    Pmat(eye(n)>0)=0;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/diversity_coef_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/diversity_coef_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/diversity_coef_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/diversity_coef_sign.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,46 @@
+function [Hpos,Hneg] = diversity_coef_sign(W, Ci)
+%DIVERSITY_COEF_SIGN     Shannon-entropy based diversity coefficient
+%
+%   [Hpos Hneg] = diversity_coef_sign(W,Ci);
+%
+%   The Shannon-entropy based diversity coefficient measures the diversity
+%   of intermodular connections of individual nodes and ranges from 0 to 1.
+%
+%   Inputs:     W,      undirected connection matrix with positive and
+%                       negative weights
+%
+%               Ci,     community affiliation vector
+%
+%   Output:     Hpos,   diversity coefficient based on positive connections
+%               Hneg,   diversity coefficient based on negative connections
+%
+%   References: Shannon CE (1948) Bell Syst Tech J 27, 379-423.
+%               Rubinov and Sporns (2011) NeuroImage.
+%
+%
+%   2011-2012, Mika Rubinov, U Cambridge
+
+%   Modification History:
+%   Mar 2011: Original
+%   Sep 2012: Fixed treatment of nodes with no negative strength
+%             (thanks to Alex Fornito and Martin Monti)
+
+
+n = length(W);                                  %number of nodes
+m = max(Ci);                                    %number of modules
+
+Hpos = entropy(W.*(W>0));
+Hneg = entropy(-W.*(W<0));
+
+    function H = entropy(W_)
+        S = sum(W_,2);                          %strength
+        Snm = zeros(n,m);                       %node-to-module degree
+        for i = 1:m                             %loop over modules
+            Snm(:,i) = sum(W_(:,Ci==i),2);
+        end
+        pnm = Snm ./ S(:,ones(1,m));
+        pnm(isnan(pnm)) = 0;
+        pnm(~pnm) = 1;
+        H = -sum(pnm.*log(pnm),2)/log(m);
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_bin.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,68 @@
+function [EBC,BC]=edge_betweenness_bin(G)
+%EDGE_BETWEENNESS_BIN    Edge betweenness centrality
+%
+%   EBC = edge_betweenness_bin(A);
+%   [EBC BC] = edge_betweenness_bin(A);
+%
+%   Edge betweenness centrality is the fraction of all shortest paths in 
+%   the network that contain a given edge. Edges with high values of 
+%   betweenness centrality participate in a large number of shortest paths.
+%
+%   Input:      A,      binary (directed/undirected) connection matrix.
+%
+%   Output:     EBC,    edge betweenness centrality matrix.
+%               BC,     node betweenness centrality vector.
+%
+%   Note: Betweenness centrality may be normalised to the range [0,1] as
+%   BC/[(N-1)(N-2)], where N is the number of nodes in the network.
+%
+%   Reference: Brandes (2001) J Math Sociol 25:163-177.
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2012
+
+
+n=length(G);
+BC=zeros(n,1);                  %vertex betweenness
+EBC=zeros(n);                   %edge betweenness
+
+for u=1:n
+    D=false(1,n); D(u)=1;      	%distance from u
+    NP=zeros(1,n); NP(u)=1;     %number of paths from u
+    P=false(n);                 %predecessors
+    Q=zeros(1,n); q=n;          %order of non-increasing distance
+
+    Gu=G;
+    V=u;
+    while V
+        Gu(:,V)=0;              %remove remaining in-edges
+        for v=V
+            Q(q)=v; q=q-1;
+            W=find(Gu(v,:));                %neighbours of v
+            for w=W
+                if D(w)
+                    NP(w)=NP(w)+NP(v);      %NP(u->w) sum of old and new
+                    P(w,v)=1;               %v is a predecessor
+                else
+                    D(w)=1;
+                    NP(w)=NP(v);            %NP(u->w) = NP of new path
+                    P(w,v)=1;               %v is a predecessor
+                end
+            end
+        end
+        V=find(any(Gu(V,:),1));
+    end
+    if ~all(D)                              %if some vertices unreachable,
+        Q(1:q)=find(~D);                    %...these are first-in-line
+    end
+
+    DP=zeros(n,1);                          %dependency
+    for w=Q(1:n-1)
+        BC(w)=BC(w)+DP(w);
+        for v=find(P(w,:))
+            DPvw=(1+DP(w)).*NP(v)./NP(w);
+            DP(v)=DP(v)+DPvw;
+            EBC(v,w)=EBC(v,w)+DPvw;
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_betweenness_wei.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,82 @@
+function [EBC,BC]=edge_betweenness_wei(G)
+%EDGE_BETWEENNESS_WEI    Edge betweenness centrality
+%
+%   EBC = edge_betweenness_wei(L);
+%   [EBC BC] = edge_betweenness_wei(L);
+%
+%   Edge betweenness centrality is the fraction of all shortest paths in 
+%   the network that contain a given edge. Edges with high values of 
+%   betweenness centrality participate in a large number of shortest paths.
+%
+%   Input:      L,      Directed/undirected connection-length matrix.
+%
+%   Output:     EBC,    edge betweenness centrality matrix.
+%               BC,     nodal betweenness centrality vector.
+%
+%   Notes:
+%       The input matrix must be a connection-length matrix, typically
+%   obtained via a mapping from weight to length. For instance, in a
+%   weighted correlation network higher correlations are more naturally
+%   interpreted as shorter distances and the input matrix should
+%   consequently be some inverse of the connectivity matrix. 
+%       Betweenness centrality may be normalised to the range [0,1] as
+%   BC/[(N-1)(N-2)], where N is the number of nodes in the network.
+%
+%   Reference: Brandes (2001) J Math Sociol 25:163-177.
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2012
+
+
+n=length(G);
+% E=find(G); G(E)=1./G(E);        %invert weights
+BC=zeros(n,1);                  %vertex betweenness
+EBC=zeros(n);                   %edge betweenness
+
+for u=1:n
+    D=inf(1,n); D(u)=0;         %distance from u
+    NP=zeros(1,n); NP(u)=1;     %number of paths from u
+    S=true(1,n);                %distance permanence (true is temporary)
+    P=false(n);                 %predecessors
+    Q=zeros(1,n); q=n;          %order of non-increasing distance
+
+    G1=G;
+    V=u;
+    while 1
+        S(V)=0;                 %distance u->V is now permanent
+        G1(:,V)=0;              %no in-edges as already shortest
+        for v=V
+            Q(q)=v; q=q-1;
+            W=find(G1(v,:));                %neighbours of v
+            for w=W
+                Duw=D(v)+G1(v,w);           %path length to be tested
+                if Duw<D(w)                 %if new u->w shorter than old
+                    D(w)=Duw;
+                    NP(w)=NP(v);            %NP(u->w) = NP of new path
+                    P(w,:)=0;
+                    P(w,v)=1;               %v is the only predecessor
+                elseif Duw==D(w)            %if new u->w equal to old
+                    NP(w)=NP(w)+NP(v);      %NP(u->w) sum of old and new
+                    P(w,v)=1;               %v is also a predecessor
+                end
+            end
+        end
+
+        minD=min(D(S));
+        if isempty(minD), break             %all nodes reached, or
+        elseif isinf(minD)                  %...some cannot be reached:
+            Q(1:q)=find(isinf(D)); break	%...these are first-in-line
+        end
+        V=find(D==minD);
+    end
+
+    DP=zeros(n,1);                          %dependency
+    for w=Q(1:n-1)
+        BC(w)=BC(w)+DP(w);
+        for v=find(P(w,:))
+            DPvw=(1+DP(w)).*NP(v)./NP(w);
+            DP(v)=DP(v)+DPvw;
+            EBC(v,w)=EBC(v,w)+DPvw;
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,48 @@
+function [EC,ec,degij] = edge_nei_overlap_bd(CIJ)
+% EDGE_NEI_OVERLAP_BD     Overlap amongst neighbors of two adjacent nodes
+%
+%   [EC,ec,degij] = edge_nei_bd(CIJ);
+%
+%   This function determines the neighbors of two nodes that are linked by 
+%   an edge, and then computes their overlap.  Connection matrix must be
+%   binary and directed.  Entries of 'EC' that are 'inf' indicate that no
+%   edge is present.  Entries of 'EC' that are 0 denote "local bridges",
+%   i.e. edges that link completely non-overlapping neighborhoods.  Low
+%   values of EC indicate edges that are "weak ties".
+%
+%   If CIJ is weighted, the weights are ignored. Neighbors of a node can be
+%   linked by incoming, outgoing, or reciprocal connections.
+%
+%   Inputs:     CIJ,      directed (binary/weighted) connection matrix
+%  
+%   Outputs:    EC,     edge neighborhood overlap matrix
+%               ec,     edge neighborhood overlap per edge, in vector format
+%               degij,  degrees of node pairs connected by each edge
+%
+%   Reference:
+%
+%       Easley and Kleinberg (2010) Networks, Crowds, and Markets. 
+%           Cambridge University Press, Chapter 3
+%
+%   Olaf Sporns, Indiana University, 2012
+
+[ik,jk,ck] = find(CIJ);
+lel = length(ck);
+N = size(CIJ,1);
+
+[~,~,deg] = degrees_dir(CIJ);
+
+ec = zeros(1,lel);
+degij = zeros(2,lel);
+for e=1:lel
+    neiik = setdiff(union(find(CIJ(ik(e),:)),find(CIJ(:,ik(e))')),[ik(e) jk(e)]);
+    neijk = setdiff(union(find(CIJ(jk(e),:)),find(CIJ(:,jk(e))')),[ik(e) jk(e)]);
+    ec(e) = length(intersect(neiik,neijk))/length(union(neiik,neijk));
+    degij(:,e) = [deg(ik(e)) deg(jk(e))];
+end;
+
+ff = find(CIJ);
+EC = 1./zeros(N);
+EC(ff) = ec;                            %#ok<FNDSB>
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/edge_nei_overlap_bu.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,45 @@
+function [EC,ec,degij] = edge_nei_overlap_bu(CIJ)
+% EDGE_NEI_OVERLAP_BU        Overlap amongst neighbors of two adjacent nodes
+%
+%   [EC,ec,degij] = edge_nei_bu(CIJ);
+%
+%   This function determines the neighbors of two nodes that are linked by 
+%   an edge, and then computes their overlap.  Connection matrix must be
+%   binary and directed.  Entries of 'EC' that are 'inf' indicate that no
+%   edge is present.  Entries of 'EC' that are 0 denote "local bridges", i.e.
+%   edges that link completely non-overlapping neighborhoods.  Low values
+%   of EC indicate edges that are "weak ties".
+%
+%   If CIJ is weighted, the weights are ignored.
+%
+%   Inputs:     CIJ,    undirected (binary/weighted) connection matrix
+%  
+%   Outputs:    EC,     edge neighborhood overlap matrix
+%               ec,     edge neighborhood overlap per edge, in vector format
+%               degij,  degrees of node pairs connected by each edge
+%
+%   Reference: Easley and Kleinberg (2010) Networks, Crowds, and Markets. 
+%              Cambridge University Press, Chapter 3.
+%
+%   Olaf Sporns, Indiana University, 2012
+
+[ik,jk,ck] = find(CIJ);
+lel = length(ck);
+N = size(CIJ,1);
+
+[deg] = degrees_und(CIJ);
+
+ec = zeros(1,lel);
+degij = zeros(2,lel);
+for e=1:lel
+    neiik = setdiff(union(find(CIJ(ik(e),:)),find(CIJ(:,ik(e))')),[ik(e) jk(e)]);
+    neijk = setdiff(union(find(CIJ(jk(e),:)),find(CIJ(:,jk(e))')),[ik(e) jk(e)]);
+    ec(e) = length(intersect(neiik,neijk))/length(union(neiik,neijk));
+    degij(:,e) = [deg(ik(e)) deg(jk(e))];
+end;
+
+ff = find(CIJ);
+EC = 1./zeros(N);
+EC(ff) = ec;                        %#ok<FNDSB>
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_bin.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,76 @@
+function E=efficiency_bin(A,local)
+%EFFICIENCY_BIN     Global efficiency, local efficiency.
+%
+%   Eglob = efficiency_bin(A);
+%   Eloc = efficiency_bin(A,1);
+%
+%   The global efficiency is the average of inverse shortest path length,
+%   and is inversely related to the characteristic path length.
+%
+%   The local efficiency is the global efficiency computed on the
+%   neighborhood of the node, and is related to the clustering coefficient.
+%
+%   Inputs:     A,              binary undirected or directed connection matrix
+%               local,          optional argument
+%                                   local=0 computes global efficiency (default)
+%                                   local=1 computes local efficiency
+%
+%   Output:     Eglob,          global efficiency (scalar)
+%               Eloc,           local efficiency (vector)
+%
+%
+%   Algorithm: algebraic path count
+%
+%   Reference: Latora and Marchiori (2001) Phys Rev Lett 87:198701.
+%              Fagiolo (2007) Phys Rev E 76:026107.
+%              Rubinov M, Sporns O (2010) NeuroImage 52:1059-69
+%
+%
+%   Mika Rubinov, U Cambridge
+%   Jonathan Clayden, UCL
+%   2008-2013
+
+% Modification history:
+% 2008: Original (MR)
+% 2013: Bug fix, enforce zero distance for self-connections (JC)
+% 2013: Local efficiency generalized to directed networks
+
+n=length(A);                                %number of nodes
+A(1:n+1:end)=0;                             %clear diagonal
+A=double(A~=0);                             %enforce double precision
+
+if exist('local','var') && local            %local efficiency
+    E=zeros(n,1);    
+    for u=1:n
+        V=find(A(u,:)|A(:,u).');            %neighbors
+        sa=A(u,V)+A(V,u).';                 %symmetrized adjacency vector
+        e=distance_inv(A(V,V));             %inverse distance matrix
+        se=e+e.';                           %symmetrized inverse distance matrix
+        numer=sum(sum((sa.'*sa).*se))/2;    %numerator
+        if numer~=0
+            denom=sum(sa).^2 - sum(sa.^2);  %denominator
+            E(u)=numer/denom;               %local efficiency
+        end
+    end
+else                                        %global efficiency
+    e=distance_inv(A);
+    E=sum(e(:))./(n^2-n);
+end
+
+
+function D=distance_inv(A_)
+l=1;                                        %path length
+Lpath=A_;                                   %matrix of paths l
+D=A_;                                       %distance matrix
+n_=length(A_);
+
+Idx=true;
+while any(Idx(:))
+    l=l+1;
+    Lpath=Lpath*A_;
+    Idx=(Lpath~=0)&(D==0);
+    D(Idx)=l;
+end
+
+D(~D | eye(n_))=inf;                        %assign inf to disconnected nodes and to diagonal
+D=1./D;                                     %invert distance
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/efficiency_wei.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,136 @@
+function E = efficiency_wei(W, local)
+%EFFICIENCY_WEI     Global efficiency, local efficiency.
+%
+%   Eglob = efficiency_wei(W);
+%   Eloc = efficiency_wei(W,2);
+%
+%   The global efficiency is the average of inverse shortest path length,
+%   and is inversely related to the characteristic path length.
+%
+%   The local efficiency is the global efficiency computed on the
+%   neighborhood of the node, and is related to the clustering coefficient.
+%
+%   Inputs:     W,
+%                   weighted undirected or directed connection matrix
+%
+%               local,
+%                   optional argument
+%                   local=0  computes the global efficiency (default).
+%                  	local=1  computes the original version of the local
+%                               efficiency.
+%               	local=2  computes the modified version of the local
+%                               efficiency (recommended, see below). 
+%
+%   Output:     Eglob,
+%                   global efficiency (scalar)
+%               Eloc,
+%                   local efficiency (vector)
+%
+%   Notes:
+%       The  efficiency is computed using an auxiliary connection-length
+%   matrix L, defined as L_ij = 1/W_ij for all nonzero L_ij; This has an
+%   intuitive interpretation, as higher connection weights intuitively
+%   correspond to shorter lengths.
+%       The weighted local efficiency broadly parallels the weighted
+%   clustering coefficient of Onnela et al. (2005) and distinguishes the
+%   influence of different paths based on connection weights of the
+%   corresponding neighbors to the node in question. In other words, a path
+%   between two neighbors with strong connections to the node in question
+%   contributes more to the local efficiency than a path between two weakly
+%   connected neighbors. Note that the original weighted variant of the
+%   local efficiency (described in Rubinov and Sporns, 2010) is not a
+%   true generalization of the binary variant, while the modified variant
+%   (described in Wang et al., 2016) is a true generalization.
+%       For ease of interpretation of the local efficiency it may be
+%   advantageous to rescale all weights to lie between 0 and 1.
+%
+%   Algorithm:  Dijkstra's algorithm
+%
+%   References: Latora and Marchiori (2001) Phys Rev Lett 87:198701.
+%               Onnela et al. (2005) Phys Rev E 71:065103
+%               Fagiolo (2007) Phys Rev E 76:026107.
+%               Rubinov M, Sporns O (2010) NeuroImage 52:1059-69
+%               Wang Y et al. (2016) Neural Comput 21:1-19.
+%
+%   Mika Rubinov, U Cambridge/Janelia HHMI, 2011-2017
+
+%Modification history
+% 2011: Original (based on efficiency.m and distance_wei.m)
+% 2013: Local efficiency generalized to directed networks
+% 2017: Added the modified local efficiency and updated documentation.
+
+n = length(W);                                              % number of nodes
+ot = 1 / 3;                                                 % one third
+
+L = W;                                                      % connection-length matrix
+A = W > 0;                                                  % adjacency matrix
+L(A) = 1 ./ L(A);
+A = double(A);
+
+if exist('local','var') && local                            % local efficiency
+    E = zeros(n, 1);
+    cbrt_W = W.^ot;
+    switch local
+        case 1
+            for u = 1:n
+                V  = find(A(u, :) | A(:, u).');             % neighbors
+                sw = cbrt_W(u, V) + cbrt_W(V, u).';       	% symmetrized weights vector
+                di = distance_inv_wei(L(V, V));             % inverse distance matrix
+                se = di.^ot + di.'.^ot;                     % symmetrized inverse distance matrix
+                numer = (sum(sum((sw.' * sw) .* se)))/2;   	% numerator
+                if numer~=0
+                    sa = A(u, V) + A(V, u).';              	% symmetrized adjacency vector
+                    denom = sum(sa).^2 - sum(sa.^2);        % denominator
+                    E(u) = numer / denom;                   % local efficiency
+                end
+            end
+        case 2
+            cbrt_L = L.^ot;
+            for u = 1:n
+                V  = find(A(u, :) | A(:, u).');            	% neighbors
+                sw = cbrt_W(u, V) + cbrt_W(V, u).';       	% symmetrized weights vector
+                di = distance_inv_wei(cbrt_L(V, V));      	% inverse distance matrix
+                se = di + di.';                             % symmetrized inverse distance matrix
+                numer=(sum(sum((sw.' * sw) .* se)))/2;      % numerator
+                if numer~=0
+                    sa = A(u, V) + A(V, u).';             	% symmetrized adjacency vector
+                    denom = sum(sa).^2 - sum(sa.^2);        % denominator
+                    E(u) = numer / denom;                 	% local efficiency
+                end
+            end
+    end
+else
+    di = distance_inv_wei(L);
+    E = sum(di(:)) ./ (n^2 - n);                         	% global efficiency
+end
+
+
+function D=distance_inv_wei(W_)
+
+n_=length(W_);
+D=inf(n_);                                                  % distance matrix
+D(1:n_+1:end)=0;
+
+for u=1:n_
+    S=true(1,n_);                                           % distance permanence (true is temporary)
+    W1_=W_;
+    V=u;
+    while 1
+        S(V)=0;                                             % distance u->V is now permanent
+        W1_(:,V)=0;                                         % no in-edges as already shortest
+        for v=V
+            T=find(W1_(v,:));                               % neighbours of shortest nodes
+            D(u,T)=min([D(u,T);D(u,v)+W1_(v,T)]);           % smallest of old/new path lengths
+        end
+        
+        minD=min(D(u,S));
+        if isempty(minD)||isinf(minD)                       % isempty: all nodes reached;
+            break,                                          % isinf: some nodes cannot be reached
+        end;
+        
+        V=find(D(u,:)==minD);
+    end
+end
+
+D=1./D;                                                     % invert distance
+D(1:n_+1:end)=0;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/eigenvector_centrality_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/eigenvector_centrality_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/eigenvector_centrality_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/eigenvector_centrality_und.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,37 @@
+function   v = eigenvector_centrality_und(CIJ)
+%EIGENVECTOR_CENTRALITY_UND      Spectral measure of centrality
+%
+%   v = eigenvector_centrality_und(CIJ)
+%
+%   Eigenector centrality is a self-referential measure of centrality:
+%   nodes have high eigenvector centrality if they connect to other nodes
+%   that have high eigenvector centrality. The eigenvector centrality of
+%   node i is equivalent to the ith element in the eigenvector 
+%   corresponding to the largest eigenvalue of the adjacency matrix.
+%
+%   Inputs:     CIJ,        binary/weighted undirected adjacency matrix.
+%
+%   Outputs:      v,        eigenvector associated with the largest
+%                           eigenvalue of the adjacency matrix CIJ.
+%
+%   Reference: Newman, MEJ (2002). The mathematics of networks.
+%
+%   Contributors:
+%   Xi-Nian Zuo, Chinese Academy of Sciences, 2010
+%   Rick Betzel, Indiana University, 2012
+%   Mika Rubinov, University of Cambridge, 2015
+
+%   MODIFICATION HISTORY
+%   2010/2012: original (XNZ, RB)
+%   2015: ensure the use of leading eigenvector (MR)
+
+
+n = length(CIJ);
+if n < 1000
+    [V,D] = eig(CIJ);
+else
+    [V,D] = eigs(sparse(CIJ));
+end
+[~,idx] = max(diag(D));
+ec = abs(V(:,idx));
+v = reshape(ec, length(ec), 1);
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/erange.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/erange.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/erange.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/erange.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,45 @@
+function  [Erange,eta,Eshort,fs] = erange(CIJ)
+%ERANGE     Shortcuts
+%
+%   [Erange,eta,Eshort,fs] = erange(CIJ);
+%
+%   Shorcuts are central edges which significantly reduce the
+%   characteristic path length in the network.
+%
+%   Input:      CIJ,        binary directed connection matrix
+%
+%   Outputs:    Erange,     range for each edge, i.e. the length of the
+%                           shortest path from i to j for edge c(i,j) AFTER
+%                           the edge has been removed from the graph.
+%               eta         average range for entire graph.
+%               Eshort      entries are ones for shortcut edges.
+%               fs          fraction of shortcuts in the graph.
+%
+%   Follows the treatment of 'shortcuts' by Duncan Watts
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+
+N = size(CIJ,1);
+K = length(nonzeros(CIJ));
+Erange = zeros(N,N);
+[i,j] = find(CIJ==1);
+
+for c=1:length(i)
+    CIJcut = CIJ;
+    CIJcut(i(c),j(c)) = 0;
+    [~, D] = reachdist(CIJcut);
+    Erange(i(c),j(c)) = D(i(c),j(c));
+end;
+
+% average range (ignore Inf)
+eta = sum(Erange((Erange>0)&(Erange<Inf)))/length(Erange((Erange>0)&(Erange<Inf)));
+
+% Original entries of D are ones, thus entries of Erange
+% must be two or greater.
+% If Erange(i,j) > 2, then the edge is a shortcut.
+% 'fshort' is the fraction of shortcuts over the entire graph.
+
+Eshort = Erange>2;
+fs = length(nonzeros(Eshort))/K;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/evaluate_generative_model.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/evaluate_generative_model.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/evaluate_generative_model.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/evaluate_generative_model.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,100 @@
+function [B,E,K] = evaluate_generative_model(A,Atgt,D,modeltype,modelvar,params)
+% EVALUATE_GENERATIVE_MODEL     Generation and evaluation of synthetic networks
+%
+%   [B,E,K] = EVALUATE_GENERATIVE_MODEL(A,Atgt,D,m,modeltype,modelvar,params) 
+%
+%   Generates synthetic networks and evaluates their energy function (see
+%   below) using the models described in the study by Betzel et al (2016)
+%   in Neuroimage.
+%
+%   Inputs:
+%           A,          binary network of seed connections
+%           Atgt,       binary network against which synthetic networks are
+%                       compared
+%           D,          Euclidean distance/fiber length matrix
+%           m,          number of connections that should be present in
+%                       final synthetic network
+%           modeltype,  specifies the generative rule (see below)
+%           modelvar,   specifies whether the generative rules are based on
+%                       power-law or exponential relationship
+%                       ({'powerlaw'}|{'exponential})
+%           params,     either a vector (in the case of the geometric
+%                       model) or a matrix (for all other models) of
+%                       parameters at which the model should be evaluated.
+%
+%   Outputs:
+%           B,          m x number of networks matrix of connections
+%           E,          energy for each synthetic network
+%           K,          Kolmogorov-Smirnov statistics for each synthetic
+%                       network.
+%
+%   Full list of model types:
+%   (each model type realizes a different generative rule)
+%
+%       1.  'sptl'          spatial model
+%       2.  'neighbors'     number of common neighbors
+%       3.  'matching'      matching index
+%       4.  'clu-avg'       average clustering coeff.
+%       5.  'clu-min'       minimum clustering coeff.
+%       6.  'clu-max'       maximum clustering coeff.
+%       7.  'clu-diff'      difference in clustering coeff.
+%       8.  'clu-prod'      product of clustering coeff.
+%       9.  'deg-avg'       average degree
+%       10. 'deg-min'       minimum degree
+%       11. 'deg-max'       maximum degree
+%       12. 'deg-diff'      difference in degree
+%       13. 'deg-prod'      product of degree
+%
+%   Note: Energy is calculated in exactly the same way as in Betzel et
+%   al (2016). There are four components to the energy are KS statistics
+%   comparing degree, clustering coefficient, betweenness centrality, and 
+%   edge length distributions. Energy is calculated as the maximum across
+%   all four statistics.
+%
+%   Reference: Betzel et al (2016) Neuroimage 124:1054-64.
+%
+%   Richard Betzel, Indiana University/University of Pennsylvania, 2015
+
+m = nnz(Atgt)/2;
+n = length(Atgt);
+x = cell(4,1);
+x{1} = sum(Atgt,2);
+x{2} = clustering_coef_bu(Atgt);
+x{3} = betweenness_bin(Atgt)';
+x{4} = D(triu(Atgt,1) > 0);
+
+B = generative_model(A,D,m,modeltype,modelvar,params);
+nB = size(B,2);
+
+K = zeros(nB,4);
+for iB = 1:nB
+    b = zeros(n);
+    b(B(:,iB)) = 1;
+    b = b + b';
+    y = cell(4,1);
+    y{1} = sum(b,2);
+    y{2} = clustering_coef_bu(b);
+    y{3} = betweenness_bin(b)';
+    y{4} = D(triu(b,1) > 0);
+    for j = 1:4
+        K(iB,j) = fcn_ks(x{j},y{j});
+    end
+end
+E = max(K,[],2);
+
+
+function kstat = fcn_ks(x1,x2)
+binEdges    =  [-inf ; sort([x1;x2]) ; inf];
+
+binCounts1  =  histc (x1 , binEdges, 1);
+binCounts2  =  histc (x2 , binEdges, 1);
+
+sumCounts1  =  cumsum(binCounts1)./sum(binCounts1);
+sumCounts2  =  cumsum(binCounts2)./sum(binCounts2);
+
+sampleCDF1  =  sumCounts1(1:end-1);
+sampleCDF2  =  sumCounts2(1:end-1);
+
+deltaCDF  =  abs(sampleCDF1 - sampleCDF2);
+kstat = max(deltaCDF);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/find_motif34.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/find_motif34.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/find_motif34.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/find_motif34.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,50 @@
+function M=find_motif34(m,n)
+%FIND_MOTIF34       Motif legend
+%
+%   Motif_matrices = find_motif34(Motif_id,Motif_class);
+%   Motif_id = find_motif34(Motif_matrix);
+%
+%   This function returns all motif isomorphs for a given motif id and 
+%   class (3 or 4). The function also returns the motif id for a given
+%   motif matrix
+%
+%   1. Input:       Motif_id,           e.g. 1 to 13, if class is 3
+%                   Motif_class,        number of nodes, 3 or 4.
+%
+%      Output:      Motif_matrices,     all isomorphs for the given motif
+%
+%   2. Input:       Motif_matrix        e.g. [0 1 0; 0 0 1; 1 0 0]
+%
+%      Output       Motif_id            e.g. 1 to 13, if class is 3
+%
+%
+%Mika Rubinov, UNSW, 2007-2008
+
+persistent M3 ID3 M4 ID4
+
+if isscalar(m)
+    if n==3
+        if isempty(ID3)
+            load motif34lib M3 ID3;
+        end
+        ind=find(ID3==m).';
+        M=zeros(3,3,length(ind));
+        for i=1:length(ind)
+            M(:,:,i)=reshape([0 M3(ind(i),1:3) 0 ...
+                M3(ind(i),4:6) 0],3,3);
+        end
+    elseif n==4
+        if isempty(ID4)
+            load motif34lib M4 ID4;
+        end
+        ind=find(ID4==m).';
+        M=zeros(4,4,length(ind));
+        for i=1:length(ind)
+            M(:,:,i)=reshape([0 M4(ind(i),1:4) 0 ...
+                M4(ind(i),5:8) 0 M4(ind(i),9:12) 0],4,4);
+        end
+    end
+else
+    n=size(m,1);
+    M=eval(['find(motif' int2str(n) 'struct_bin(m))']);
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/findpaths.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/findpaths.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/findpaths.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/findpaths.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,158 @@
+function [Pq,tpath,plq,qstop,allpths,util] = findpaths(CIJ,sources,qmax,savepths)
+%FINDPATHS      Network paths
+%
+%   [Pq,tpath,plq,qstop,allpths,util] = findpaths(CIJ,sources,qmax,savepths);
+%
+%   Paths are sequences of linked nodes, that never visit a single node
+%   more than once. This function finds all paths that start at a set of 
+%   source nodes, up to a specified length. Warning: very memory-intensive.
+%
+%   Inputs:     CIJ,        binary (directed/undirected) connection matrix
+%               qmax,       maximal path length
+%               sources,    source units from which paths are grown
+%               savepths,   set to 1 if all paths are to be collected in
+%                           'allpths'
+%
+%   Outputs:    Pq,         3D matrix, with P(i,j,q) = number of paths from
+%                           'i' to 'j' of length 'q'.
+%               tpath,      total number of paths found (lengths 1 to 'qmax')
+%               plq,        path length distribution as a function of 'q'
+%               qstop,      path length at which 'findpaths' is stopped
+%               allpths,    a matrix containing all paths up to 'qmax'
+%               util,       node use index
+%
+%   Note that Pq(:,:,N) can only carry entries on the diagonal, as all
+%   "legal" paths of length N-1 must terminate.  Cycles of length N are
+%   possible, with all vertices visited exactly once (except for source and
+%   target). 'qmax = N' can wreak havoc (due to memory problems).
+%
+%   Note: Weights are discarded.
+%   Note: I am certain that this algorithm is rather inefficient -
+%   suggestions for improvements are welcome.
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008/2010
+
+%   2010 version:
+%   -- a bug affecting the calculation of 'util' was fixed -- thanks to
+%      Steve Williams
+%   -- better pre-allocation for 'npths'
+%   -- note that this code assumes a directed graph as input - calculation
+%      of paths and 'util' indices can be easily adapted to undirected
+%      graphs. 
+
+% ensure CIJ is binary...
+CIJ = double(CIJ~=0);
+
+% initialize some variables
+N = size(CIJ,1); K = sum(sum(CIJ));
+pths = [];
+Pq = zeros(N,N,qmax);
+util = zeros(N,qmax);
+
+% this code is for pathlength = 1
+% paths are seeded from 'sources'
+q = 1;
+for j=1:N
+    for i=1:length(sources)
+        is = sources(i);
+        if (CIJ(is,j) == 1)
+            pths = [pths [is j]'];
+        end;
+    end;
+end;
+
+% calculate the use index per vertex (for paths of length 1)
+util(1:N,q) = util(1:N,q)+hist(reshape(pths,1,size(pths,1)*size(pths,2)),1:N)';
+% now enter the found paths of length 1 into the pathmatrix Pq
+for np=1:size(pths,2)
+    Pq(pths(1,np),pths(q+1,np),q) = Pq(pths(1,np),pths(q+1,np),q) + 1;
+end;
+
+% begin saving all paths
+if (savepths==1)
+    allpths = pths;
+end;
+if (savepths~=1)
+    allpths = [];
+end;
+
+% initialize
+npthscnt = K;
+
+% "big loop" for all other pathlengths 'q'
+% ----------------------------------------------------------------------
+for q=2:qmax
+
+    % to keep track of time...
+    disp(['current pathlength (q) = ',num2str(q),'   number of paths so far (up to q-1)= ',num2str(sum(sum(sum(Pq))))])
+
+    % old paths are now in 'pths'
+    % new paths are about to be collected in 'npths'
+    % estimate needed allocation for new paths
+    len_npths = min(ceil(1.1*npthscnt*K/N),100000000);
+    npths = zeros(q+1,len_npths);
+
+    % find the unique set of endpoints of 'pths'
+    endp = unique(pths(q,:));
+    npthscnt = 0;
+
+    for ii=1:length(endp)  % set of endpoints of previous paths
+        i = endp(ii);
+        % in 'pb' collect all previous paths with 'i' as their endpoint
+        [pa,pb] = find(pths(q,:) == i);
+        % find the outgoing connections from 'i' ("breadth-first")
+        nendp = find(CIJ(i,:)==1);
+        % if 'i' is not a dead end
+        if (~isempty(nendp))
+            for jj=1:length(nendp)   % endpoints of next edge
+                j = nendp(jj);
+                % find new paths - only "legal" ones, i.e. no vertex is visited twice
+                pb_temp = pb(sum(j==pths(2:q,pb),1)==0);
+                % add new paths to 'npths'
+                npths(:,npthscnt+1:npthscnt+length(pb_temp)) = [pths(:,pb_temp)' ones(length(pb_temp),1)*j]';
+                npthscnt = npthscnt+length(pb_temp);
+                % count new paths and add the number to 'P'
+                Pq(1:N,j,q) = Pq(1:N,j,q)+(hist(pths(1,pb_temp),1:N))';
+            end;
+        end;
+    end;
+
+    % note: 'npths' now contains a list of all the paths of length 'q'
+    if (len_npths>npthscnt)
+        npths = npths(:,1:npthscnt);
+    end;
+
+    % append the matrix of all paths
+    if (savepths==1)
+        allpths = [allpths; zeros(1,size(allpths,2))];
+        allpths = [allpths npths(:,1:npthscnt)];
+    end;
+
+    % calculate the use index per vertex (correct for cycles, count
+    % source/target only once)
+    util(1:N,q) = util(1:N,q) + hist(reshape(npths(:,1:npthscnt),1,size(npths,1)*npthscnt),1:N)' - diag(Pq(:,:,q));
+    % eliminate cycles from "making it" to the next level, so that
+    % 'pths' contains all the paths that have a chance of being continued
+    if (~isempty(npths))
+        pths = npths(:,npths(1,:)~=npths(q+1,:));
+    else
+        pths = [];
+    end;
+
+    % if there are no 'pths' paths left, end the search
+    if (isempty(pths))
+        qstop = q;
+        tpath = sum(sum(sum(Pq)));
+        plq = reshape(sum(sum(Pq)),1,qmax);
+        return;
+    end;
+
+end;  % q
+% ----------------------------------------------------------------------
+qstop = q;
+
+% total number of paths
+tpath = sum(sum(sum(Pq)));
+
+% path length distribution
+plq = reshape(sum(sum(Pq)),1,qmax);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/findwalks.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/findwalks.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/findwalks.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/findwalks.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,41 @@
+function [Wq,twalk,wlq] = findwalks(CIJ)
+%FINDWALKS      Network walks
+%
+%   [Wq,twalk,wlq] = findwalks(CIJ);
+%
+%   Walks are sequences of linked nodes, that may visit a single node more
+%   than once. This function finds the number of walks of a given length, 
+%   between any two nodes.
+%
+%   Input:      CIJ         binary (directed/undirected) connection matrix
+%
+%   Outputs:    Wq          3D matrix, Wq(i,j,q) is the number of walks
+%                           from 'i' to 'j' of length 'q'.
+%               twalk       total number of walks found
+%               wlq         walk length distribution as function of 'q'
+%
+%   Notes: Wq grows very quickly for larger N,K,q. Weights are discarded.
+%
+%   Algorithm: algebraic path count
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+% ensure CIJ is binary...
+CIJ = double(CIJ~=0);
+
+N = size(CIJ,1);
+Wq = zeros(N,N,N);
+CIJpwr = CIJ;
+Wq(:,:,1) = CIJ;
+for q=2:N
+   CIJpwr = CIJpwr*CIJ;
+   Wq(:,:,q) = CIJpwr;
+end;
+
+% total number of walks
+twalk = sum(sum(sum(Wq)));
+
+% walk length distribution
+wlq = reshape(sum(sum(Wq)),1,N);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/flow_coef_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/flow_coef_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/flow_coef_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/flow_coef_bd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,51 @@
+function  [fc,FC,total_flo] = flow_coef_bd(CIJ)
+%FLOW_COEF_BD        Node-wise flow coefficients
+%
+%   [hc,HC,total_flo] = flow_coef_bd(CIJ)
+%
+%   Computes the flow coefficient for each node and averaged over the
+%   network, as described in Honey et al. (2007) PNAS. The flow coefficient
+%   is similar to betweenness centrality, but works on a local
+%   neighborhood. It is mathematically related to the clustering
+%   coefficient  (cc) at each node as, fc+cc <= 1.
+%
+%   input:      CIJ,	connection/adjacency matrix (binary, directed)
+%   output:     fc,     flow coefficient for each node
+%               FC,     average flow coefficient over the network
+%        total_flo,     number of paths that "flow" across the central node
+%
+%   Reference:  Honey et al. (2007) Proc Natl Acad Sci U S A
+%
+%   Olaf Sporns, Indiana University, 2007/2010/2012
+
+N = size(CIJ,1);
+
+% initialize ...
+fc        = zeros(1,N);
+total_flo = fc;
+max_flo   = fc;
+
+% loop over nodes
+for v=1:N
+    % find neighbors - note: treats incoming and outgoing connections as equal
+    [nb] = find(CIJ(v,:) + CIJ(:,v)');
+    fc(v) = 0;
+    if (~isempty(nb))
+        CIJflo = -CIJ(nb,nb);
+        for i=1:length(nb)
+            for j=1:length(nb)
+                if((CIJ(nb(i),v))==1)&&(CIJ(v,nb(j))==1)
+                    CIJflo(i,j) = CIJflo(i,j) + 1;
+                end;
+            end;
+        end;
+        total_flo(v) = sum(sum(double(CIJflo==1).*~eye(length(nb))));
+        max_flo(v) = length(nb)^2-length(nb);
+        fc(v) = total_flo(v)/max_flo(v);
+    end;
+end;
+
+% handle nodes that are NaNs
+fc(isnan(fc)) = 0;
+
+FC = mean(fc);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/gateway_coef_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/gateway_coef_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/gateway_coef_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/gateway_coef_sign.m	2019-03-03 20:22:02.000000000 +0100
@@ -0,0 +1,83 @@
+function [GWpos,GWneg] = gateway_coef_sign(W,Ci,centtype)
+
+%   Gateway coefficient
+%
+%   [Gpos,Gneg] = gateway_coef_sign(W,Ci,centtype);
+%
+%   Gateway coefficient is a variant of participation coefficient. Similar
+%   to participation coefficient, gateway coefficient measures the
+%   diversity of intermodular connections of individual nodes, but this is
+%   weighted by how critical these connections are to intermodular
+%   connectivity (e.g., if a node is the only connection between it's
+%   module and another module, it will have a higher gateway coefficient).
+%
+%   Inputs:     W,        undirected connection matrix with positive and
+%                         negative weights
+%
+%               Ci,       community affiliation vector
+%
+%               centtype, centrality measure to use
+%                         1 = Node Strength
+%                         2 = Betweenness Centrality
+%
+%   Output:     Gpos,     gateway coefficient for positive weights
+%               Gneg,     gateway coefficient for negative weights
+%
+%   Reference: Vargas ER, Wahl LM. Eur Phys J B (2014) 87:1-10.
+%
+%   Jeff Spielberg, University of Delaware
+
+%   Modification History:
+%   May 2015:  Original (originally adapted from participation_coef_sign.m)
+%   July 2018: Bugfix, change in how weighted matrices are handled,
+%              improvements for efficiency, additional line documentation
+
+[~,~,Ci]       = unique(Ci);                                        % Remap module indices to consecutive numbers
+n              = length(W);                                         % Number of nodes
+W(1:(n+1):end) = 0;                                                 % Ensure diagonal is zero
+GWpos          = gcoef(W.*(W>0));                                   % Compute gateway coefficient for positive weights
+GWneg          = gcoef(-W.*(W<0));                                  % Compute gateway coefficient for negative weights
+
+    function GW = gcoef(W_)
+        k    = sum(W_,2);                                           % Compute node strength
+        Gc   = (W_~=0)*diag(Ci);                                    % Create neighbor community affiliation matrix
+        nmod = max(Ci);                                             % Find # of modules
+        ks   = zeros(n,nmod);                                       % Preallocate space
+        kjs  = zeros(n,nmod);                                       % Preallocate space
+        cs   = zeros(n,nmod);                                       % Preallocate space
+        switch centtype                                             % Which centrality measure to use?
+            case 1                                                  % Node Strength
+                cent = sum(W_,2);
+            case 2                                                  % Betweenness Centrality
+                L    = weight_conversion(W_,'lengths');
+                cent = betweenness_wei(L);
+        end
+        mcn = 0;                                                    % Set max summed centrality per module to 0
+        for i = 1:nmod                                              % For each module
+            if sum(cent(Ci==i))>mcn                                 % If current module has a higher sum
+                mcn = sum(cent(Ci==i));                             % Reassign value
+            end
+            ks(:,i) = sum(W_.*(Gc==i),2);                           % Compute the total weight of the connections per node to each module
+        end
+        for i = 1:nmod                                              % For each module
+            if sum(Ci==i)>1                                         % If there is more than 1 node in a module
+                kjs(Ci==i,:) = ones(sum(Ci==i),1)*sum(ks(Ci==i,:)); % Compute total module-module connections
+                kjs(Ci==i,i) = kjs(Ci==i,i)/2;                      % Account for redundancy due to double counting within-network work weights
+            end
+        end
+        for i = 1:n                                                 % For each node
+            if k(i)>0                                               % If node is connected
+                for ii = 1:nmod                                     % For each module
+                    cs(i,ii) = sum(cent((Ci.*(W_(:,i)>0))==ii));    % Sum of centralities of neighbors of a node within a module
+                end
+            end
+        end
+        ksm           = ks./kjs;                                    % Normalize by total connections
+        ksm(kjs==0)   = 0;                                          % Account for division by 0
+        csm           = cs./mcn;                                    % Normalize by max summed centrality
+        gs            = (1-(ksm.*csm)).^2;                          % Calculate total weighting
+        GW            = 1-sum((ks.^2)./(k.^2).*gs,2);               % Compute gateway coefficient
+        GW(isnan(GW)) = 0;                                          % Account for division by 0
+        GW(~GW) = 0;                                                % Set to 0 if no neighbors
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/generate_fc.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/generate_fc.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/generate_fc.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/generate_fc.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,241 @@
+function [FCpre,pred_data,Fcorr] = generate_fc(SC,beta,ED,pred_var,model,FC)
+%   GENERATE_FC     Generation of synthetic functional connectivity matrices
+%
+%   [FCpre,pred_data,Fcorr] = generate_fc(SC,beta,ED,{'SPLwei_log','SIwei_log'},FC)
+%   [FCpre,pred_data] = generate_fc(SC,beta,[],{'SPLwei_log','SIwei_log'})
+%
+%   Uses a vector beta of regression coefficients from the model
+%   FC = pred_var*beta to predict FC. pred_var are structural-based network
+%   measures derived from the structural connectivity network.
+%
+%   Inputs:
+%
+%       SC,
+%           Weighted/unweighted undirected NxN Structural Connectivity matrix.
+%
+%       beta,
+%           Regression coefficients (vector). These may be obtained as an
+%           output parameter from function predict_fc.m
+%
+%       ED,
+%           Euclidean distance matrix or upper triangular vector of the
+%           matrix (optional)
+%
+%       pred_var,
+%           Set of M predictors. These can be given as an KxM array where
+%           K = ((N*(N-1))/2) and M is the number of predictors.
+%           Alternatively, pred_var can be a cell with the names of network
+%           measures to be used as predictors. Accepted network measure
+%           names are:
+%               SPLbin        - Shortest-path length (binary)
+%               SPLwei_inv    - Shortest-path length computed with an inv transform
+%             	SPLwei_log    - Shortest-path length computed with a log transform
+%             	SPLdist       - Shortest-path length computed with no transform
+%              	SIbin         - Search Information of binary shortest-paths
+%             	SIwei_inv     - Search Information of shortest-paths computed with an inv transform
+%           	SIwei_log     - Search Information of shortest-paths computed with a log transform
+%              	SIdist        - Search Information of shortest-paths computed with no transform
+%              	T             - Path Transitivity
+%              	deltaMFPT     - Column-wise z-scored mean first passage time
+%               neighOverlap  - Neighborhood Overlap
+%              	MI            - Matching Index
+%
+%           Predictors must be specified in the order that matches the
+%           given beta values.
+%
+%      	model,
+%           Specifies the order of the regression model used within
+%           matlab's function regstats.m. 'model' can be any option
+%           accepted by matlab's regstats.m function (e.g.'linear',
+%           'interaction' 'quadratic', etc). If no model is specified,
+%           'linear' is the default.
+%
+%    	FC,
+%           Functional connections. FC can be a NxN symmetric matrix or a
+%           ((N*(N-1))/2) x 1 vector containing the upper triangular
+%           elements of the square FC matrix (excluding diagonal elements).
+%           This argument is optional and only used to compute the
+%           correlation between the predicted FC and empirical FC.
+%
+%
+%   Outputs:
+%
+%       FCpre,
+%           Predicted NxN Functional Connectivity matrix
+%
+%      	pred_data,
+%           KxM array of predictors.
+%
+%       FCcorr,
+%           Pearson Correlation between FCpred and FC
+%
+%
+%   Reference: Goi et al. (2014) PNAS 111: 833838
+%
+%
+%   Andrea Avena-Koenigsberger, Joaquin Goi and Olaf Sporns; IU Bloomington, 2016
+
+
+[b1,b2] = size(beta);
+if b1 == 1 && b2 >= b1
+    beta = beta';  % beta must be a column vector
+elseif b1 > 1 && b2 > 1
+    error('beta must be a vector of scalar regression coefficients')
+end
+
+pred_names = {'SPLbin','SPLwei_inv','SPLwei_log','SPLdist','SIbin',...
+    'SIwei_inv','SIwei_log','SIdist','T','deltaMFPT','neighOverlap','MI'};
+
+% select model
+if ~exist('model','var') || isempty(model)
+    model = 'linear';
+end
+
+N = size(SC,1);
+indx = find(triu(ones(N),1));
+
+if ~exist('pred_var','var') && ~isempty(ED)
+    pred_var = {'ED','SPLwei_log','SI','T'};
+    flag_var_names = true;
+    flag_ED = true;
+elseif ~exist('pred_var','var') && isempty(ED)
+    pred_var = {'SPLwei_log','SI','T'};
+    flag_var_names = true;
+elseif exist('pred_var','var') && ~isnumeric(pred_var) && ~isempty(ED)
+    flag_var_names = true;
+    flag_ED = true;
+elseif exist('pred_var','var') && ~isnumeric(pred_var) && isempty(ED)
+    flag_var_names = true;
+    flag_ED = false;
+elseif exist('pred_var','var') && isnumeric(pred_var) && ~isempty(ED)
+    flag_var_names = false;
+    flag_ED = true;
+elseif exist('pred_var','var') && isnumeric(pred_var) && isempty(ED)
+    flag_var_names = false;
+    flag_ED = false;
+else
+    err_str = '"pred_var" must be an KxM array of M predictors, or any of the following graph-measure names:';
+    s1 = sprintf('SPLbin - Shortest-path length (binary) \n');
+    s2 = sprintf('SPLwei_inv - Shortest-path length computed with an inv transform \n');
+    s3 = sprintf('SPLwei_log - Shortest-path length computed with a log transform \n');
+    s4 = sprintf('SPLdist - Shortest-path length computed with no transform \n');
+    s5 = sprintf('SIbin - Search Information of binary shortest-paths \n');
+    s6 = sprintf('SIwei_inv - Search Information of shortest-paths computed with an inv transform \n');
+    s7 = sprintf('SIwei_log - Search Information of shortest-paths computed with a log transform \n');
+    s8 = sprintf('SIdist - Search Information of shortest-paths computed with no transform \n');
+    s9 = sprintf('T - Path Transitivity \n');
+    s10 = sprintf('deltaMFPT - Column-wise z-scored mean first passage time \n');
+    s11 = sprintf('neighOverlap - Neighborhood Overlap \n');
+    s12 = sprintf('MI - Matching Index \n');
+    error('%s \n %s %s %s %s %s %s %s %s %s %s %s %s',err_str,s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12);
+end
+
+if flag_ED
+    [n1,n2] = size(ED);
+    if n1 == n2 && n1 == N
+        % square ED matrix
+        pred_data = ED(indx);
+    elseif n1 == length(indx) && n2 == 1
+        % ED is already an upper-triangle vector
+        pred_data = ED;
+    else
+        error('ED must be square matrix or a vector containing the upper triangle of the square ED matrix \n')
+    end
+else
+    pred_data = [];
+end
+
+
+if flag_var_names
+    fprintf('\n----------------------');
+    fprintf('\n Selected predictors: \n');
+    ind2start = size(pred_data,2);
+    pred_data = [pred_data,zeros(length(indx),length(pred_var))];
+    
+    for v = 1:length(pred_var)
+        var_ind = find(strcmp(pred_var{v},pred_names));
+        switch var_ind
+            
+            case 1   %SPLbin
+                fprintf('Shortest-path length (binary) \n\n');
+                data = distance_wei_floyd(double(SC>0));
+            case 2   %SPLwei_inv
+                fprintf('Shortest-path length computed with an inv transform \n');
+                data = distance_wei_floyd(SC,'inv');
+            case 3   %SPLwei_log
+                fprintf('Shortest-path length computed with a log transform \n');
+                data = distance_wei_floyd(SC,'log');
+            case 4   %SPLdist
+                fprintf('Shortest-path length computed with no transform \n');
+                data = distance_wei_floyd(SC);
+            case 5   %SIbin
+                fprintf('Search Information of binary shortest-paths \n');
+                data = search_information(double(SC>0));
+                data = data + data';
+            case 6   %SIwei_inv
+                fprintf('Search Information of shortest-paths computed with an inv transform \n');
+                data = search_information(SC,'inv');
+                data = data + data';
+            case 7   %SIwei_log
+                fprintf('Search Information of shortest-paths computed with a log transform \n');
+                data = search_information(SC,'log');
+                data = data + data';
+            case 8   %SIdist
+                fprintf('Search Information of shortest-paths computed with no transform \n');
+                data = search_information(SC);
+                data = data + data';
+            case 9   %T
+                fprintf('Path Transitivity \n');
+                data = path_transitivity(double(SC>0));
+            case 10  %deltaMFPT
+                fprintf('Column-wise z-scored mean first passage time \n');
+                mfpt = mean_first_passage_time(SC);
+                deltamfpt = zscore(mfpt,[],1);
+                data = deltamfpt+deltamfpt';
+            case 11  %neighOverlap
+                fprintf('Neighborhood Overlap \n');
+                data = double(SC>0) * double(SC>0)';
+            case 12  %MI
+                fprintf('Matching Index \n');
+                data = matching_ind(SC);
+            otherwise
+                error('This is not an accepted predictor. See list of available predictors \n')
+        end
+        pred_data(:,ind2start+v) = data(indx);
+    end
+else
+    if size(pred_var,1) == length(indx)
+        pred_data = [pred_data,pred_var];
+    else
+        error('Custom predictors must be provided as KxM array of M predictors \n');
+    end
+end
+
+pred_data = x2fx(pred_data,model);
+
+if size(pred_data,2) == size(beta,1)
+    Y = pred_data*beta;
+    FCpre = zeros(N);
+    FCpre(indx) = Y;
+    FCpre = FCpre+FCpre';
+end
+
+if nargin == 6 && ~isempty(FC)
+    flag_nan_corr = false;
+    [n1,n2] = size(FC);
+    if n1 == n2 && n1 == N
+        % square FC matrix
+        FCemp = FC(indx);
+    elseif n1 == length(indx) && n2 == 1
+        % FC is already an upper-triangle vector
+        FCemp = FC;
+    else
+        warning('FC must be square matrix or a vector containing the upper triangle (no diagonal elements) of the square FC matrix \n')
+        flag_nan_corr = true;
+    end
+    if ~flag_nan_corr
+        Fcorr = corr(Y,FCemp);
+    else
+        Fcorr = nan;
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/generative_model.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/generative_model.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/generative_model.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/generative_model.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,897 @@
+function b = generative_model(A,D,m,modeltype,modelvar,params,epsilon)
+% GENERATIVE_MODEL          Run generative model code
+%
+%   B = GENERATIVE_MODEL(A,D,m,modeltype,modelvar,params)
+%
+%   Generates synthetic networks using the models described in the study by
+%   Betzel et al (2016) in Neuroimage.
+%
+%   Inputs:
+%           A,          binary network of seed connections
+%           D,          Euclidean distance/fiber length matrix
+%           m,          number of connections that should be present in
+%                       final synthetic network
+%           modeltype,  specifies the generative rule (see below)
+%           modelvar,   specifies whether the generative rules are based on
+%                       power-law or exponential relationship
+%                       ({'powerlaw'}|{'exponential})
+%           params,     either a vector (in the case of the geometric
+%                       model) or a matrix (for all other models) of
+%                       parameters at which the model should be evaluated.
+%           epsilon,    the baseline probability of forming a particular
+%                       connection (should be a very small number
+%                       {default = 1e-5}).
+%
+%   Output:
+%           B,          m x number of networks matrix of connections
+%
+%
+%   Full list of model types:
+%   (each model type realizes a different generative rule)
+%
+%       1.  'sptl'          spatial model
+%       2.  'neighbors'     number of common neighbors
+%       3.  'matching'      matching index
+%       4.  'clu-avg'       average clustering coeff.
+%       5.  'clu-min'       minimum clustering coeff.
+%       6.  'clu-max'       maximum clustering coeff.
+%       7.  'clu-diff'      difference in clustering coeff.
+%       8.  'clu-prod'      product of clustering coeff.
+%       9.  'deg-avg'       average degree
+%       10. 'deg-min'       minimum degree
+%       11. 'deg-max'       maximum degree
+%       12. 'deg-diff'      difference in degree
+%       13. 'deg-prod'      product of degree
+%
+%
+%   Example usage:
+%
+%       load demo_generative_models_data
+%
+%       % get number of bi-directional connections
+%       m = nnz(A)/2;
+% 
+%       % get cardinality of network
+%       n = length(A);
+% 
+%       % set model type
+%       modeltype = 'neighbors';
+% 
+%       % set whether the model is based on powerlaw or exponentials
+%       modelvar = [{'powerlaw'},{'powerlaw'}];
+% 
+%       % choose some model parameters
+%       params = [-2,0.2; -5,1.2; -1,1.5];
+%       nparams = size(params,1);
+% 
+%       % generate synthetic networks
+%       B = generative_model(Aseed,D,m,modeltype,modelvar,params);
+%
+%       % store them in adjacency matrix format
+%       Asynth = zeros(n,n,nparams);
+%       for i = 1:nparams; 
+%           a = zeros(n); a(B(:,i)) = 1; a = a + a'; 
+%           Asynth(:,:,i) = a; 
+%       end
+%
+%   Reference: Betzel et al (2016) Neuroimage 124:1054-64.
+%
+%   Richard Betzel, Indiana University/University of Pennsylvania, 2015
+
+if ~exist('epsilon','var')
+    epsilon = 1e-5;
+end
+
+n = length(D);
+nparams = size(params,1);
+b = zeros(m,nparams);
+
+switch modeltype
+    
+    case 'clu-avg'
+        clu = clustering_coef_bu(A);
+        Kseed = bsxfun(@plus,clu(:,ones(1,n)),clu')/2;
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_clu_avg(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'clu-diff'
+        clu = clustering_coef_bu(A);
+        Kseed = abs(bsxfun(@minus,clu(:,ones(1,n)),clu'));
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_clu_diff(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'clu-max'
+        clu = clustering_coef_bu(A);
+        Kseed = bsxfun(@max,clu(:,ones(1,n)),clu');
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_clu_max(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'clu-min'
+        clu = clustering_coef_bu(A);
+        Kseed = bsxfun(@min,clu(:,ones(1,n)),clu');
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_clu_min(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'clu-prod'
+        clu = clustering_coef_bu(A);
+        Kseed = clu*clu';
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_clu_prod(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'deg-avg'
+        kseed = sum(A,2);
+        Kseed = bsxfun(@plus,kseed(:,ones(1,n)),kseed')/2;
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_deg_avg(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'deg-diff'
+        kseed = sum(A,2);
+        Kseed = abs(bsxfun(@minus,kseed(:,ones(1,n)),kseed'));
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_deg_diff(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'deg-max'
+        kseed = sum(A,2);
+        Kseed = bsxfun(@max,kseed(:,ones(1,n)),kseed');
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_deg_max(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'deg-min'
+        kseed = sum(A,2);
+        Kseed = bsxfun(@min,kseed(:,ones(1,n)),kseed');
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_deg_min(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'deg-prod'
+        kseed = sum(A,2);
+        Kseed = (kseed*kseed').*~eye(n);
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_deg_prod(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'neighbors'
+        Kseed = (A*A).*~eye(n);
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_nghbrs(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'matching'
+        Kseed = matching_ind(A);
+        Kseed = Kseed + Kseed';
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            gam = params(iparam,2);
+            b(:,iparam) = fcn_matching(A,Kseed,D,m,eta,gam,modelvar,epsilon);
+        end
+        
+    case 'sptl'
+        for iparam = 1:nparams
+            eta = params(iparam,1);
+            b(:,iparam) = fcn_sptl(A,D,m,eta,modelvar{1});
+        end
+        
+end
+
+function b = fcn_clu_avg(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+
+c = clustering_coef_bu(A);
+k = sum(A,2);
+
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    k([uu,vv]) = k([uu,vv]) + 1;
+    bu = A(uu,:);
+    su = A(bu,bu);
+    bv = A(vv,:);
+    sv = A(bv,bv);
+    bth = bu & bv;
+    c(bth) = c(bth) + 2./(k(bth).^2 - k(bth));
+    c(uu) = nnz(su)/(k(uu)*(k(uu) - 1));
+    c(vv) = nnz(sv)/(k(vv)*(k(vv) - 1));
+    c(k <= 1) = 0;
+    bth([uu,vv]) = true;
+    K(:,bth) = bsxfun(@plus,c(:,ones(1,sum(bth))),c(bth,:)')/2 + epsilon;
+    K(bth,:) = bsxfun(@plus,c(:,ones(1,sum(bth))),c(bth,:)')'/2 + epsilon;
+
+    switch mv2
+        case 'powerlaw'
+            Ff(bth,:) = Fd(bth,:).*((K(bth,:)).^gam);
+            Ff(:,bth) = Fd(:,bth).*((K(:,bth)).^gam);
+        case 'exponential'
+            Ff(bth,:) = Fd(bth,:).*exp((K(bth,:))*gam);
+            Ff(:,bth) = Fd(:,bth).*exp((K(:,bth))*gam);
+    end
+    Ff = Ff.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_clu_diff(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+
+c = clustering_coef_bu(A);
+k = sum(A,2);
+
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    k([uu,vv]) = k([uu,vv]) + 1;
+    bu = A(uu,:);
+    su = A(bu,bu);
+    bv = A(vv,:);
+    sv = A(bv,bv);
+    bth = bu & bv;
+    c(bth) = c(bth) + 2./(k(bth).^2 - k(bth));
+    c(uu) = nnz(su)/(k(uu)*(k(uu) - 1));
+    c(vv) = nnz(sv)/(k(vv)*(k(vv) - 1));
+    c(k <= 1) = 0;
+    bth([uu,vv]) = true;
+    K(:,bth) = abs(bsxfun(@minus,c(:,ones(1,sum(bth))),c(bth,:)')) + epsilon;
+    K(bth,:) = abs(bsxfun(@minus,c(:,ones(1,sum(bth))),c(bth,:)'))' + epsilon;
+
+    switch mv2
+        case 'powerlaw'
+            Ff(bth,:) = Fd(bth,:).*((K(bth,:)).^gam);
+            Ff(:,bth) = Fd(:,bth).*((K(:,bth)).^gam);
+        case 'exponential'
+            Ff(bth,:) = Fd(bth,:).*exp((K(bth,:))*gam);
+            Ff(:,bth) = Fd(:,bth).*exp((K(:,bth))*gam);
+    end
+    Ff = Ff.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_clu_max(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+
+c = clustering_coef_bu(A);
+k = sum(A,2);
+
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    k([uu,vv]) = k([uu,vv]) + 1;
+    bu = A(uu,:);
+    su = A(bu,bu);
+    bv = A(vv,:);
+    sv = A(bv,bv);
+    bth = bu & bv;
+    c(bth) = c(bth) + 2./(k(bth).^2 - k(bth));
+    c(uu) = nnz(su)/(k(uu)*(k(uu) - 1));
+    c(vv) = nnz(sv)/(k(vv)*(k(vv) - 1));
+    c(k <= 1) = 0;
+    bth([uu,vv]) = true;
+    K(:,bth) = bsxfun(@max,c(:,ones(1,sum(bth))),c(bth,:)') + epsilon;
+    K(bth,:) = bsxfun(@max,c(:,ones(1,sum(bth))),c(bth,:)')' + epsilon;
+
+    switch mv2
+        case 'powerlaw'
+            Ff(bth,:) = Fd(bth,:).*((K(bth,:)).^gam);
+            Ff(:,bth) = Fd(:,bth).*((K(:,bth)).^gam);
+        case 'exponential'
+            Ff(bth,:) = Fd(bth,:).*exp((K(bth,:))*gam);
+            Ff(:,bth) = Fd(:,bth).*exp((K(:,bth))*gam);
+    end
+    Ff = Ff.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_clu_min(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+
+c = clustering_coef_bu(A);
+k = sum(A,2);
+
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    k([uu,vv]) = k([uu,vv]) + 1;
+    bu = A(uu,:);
+    su = A(bu,bu);
+    bv = A(vv,:);
+    sv = A(bv,bv);
+    bth = bu & bv;
+    c(bth) = c(bth) + 2./(k(bth).^2 - k(bth));
+    c(uu) = nnz(su)/(k(uu)*(k(uu) - 1));
+    c(vv) = nnz(sv)/(k(vv)*(k(vv) - 1));
+    c(k <= 1) = 0;
+    bth([uu,vv]) = true;
+    K(:,bth) = bsxfun(@min,c(:,ones(1,sum(bth))),c(bth,:)') + epsilon;
+    K(bth,:) = bsxfun(@min,c(:,ones(1,sum(bth))),c(bth,:)')' + epsilon;
+
+    switch mv2
+        case 'powerlaw'
+            Ff(bth,:) = Fd(bth,:).*((K(bth,:)).^gam);
+            Ff(:,bth) = Fd(:,bth).*((K(:,bth)).^gam);
+        case 'exponential'
+            Ff(bth,:) = Fd(bth,:).*exp((K(bth,:))*gam);
+            Ff(:,bth) = Fd(:,bth).*exp((K(:,bth))*gam);
+    end
+    Ff = Ff.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_clu_prod(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+
+c = clustering_coef_bu(A);
+k = sum(A,2);
+
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    k([uu,vv]) = k([uu,vv]) + 1;
+    bu = A(uu,:);
+    su = A(bu,bu);
+    bv = A(vv,:);
+    sv = A(bv,bv);
+    bth = bu & bv;
+    c(bth) = c(bth) + 2./(k(bth).^2 - k(bth));
+    c(uu) = nnz(su)/(k(uu)*(k(uu) - 1));
+    c(vv) = nnz(sv)/(k(vv)*(k(vv) - 1));
+    c(k <= 1) = 0;
+    bth([uu,vv]) = true;
+    K(bth,:) = (c(bth,:)*c') + epsilon;
+    K(:,bth) = (c*c(bth,:)') + epsilon;
+    
+    switch mv2
+        case 'powerlaw'
+            Ff(bth,:) = Fd(bth,:).*((K(bth,:)).^gam);
+            Ff(:,bth) = Fd(:,bth).*((K(:,bth)).^gam);
+        case 'exponential'
+            Ff(bth,:) = Fd(bth,:).*exp((K(bth,:))*gam);
+            Ff(:,bth) = Fd(:,bth).*exp((K(:,bth))*gam);
+    end
+    Ff = Ff.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_deg_avg(A,K,D,m,eta,gam,modelvar,epsilon)
+n = length(D);
+mseed = nnz(A)/2;
+k = sum(A,2);
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+D = D(indx);
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+K = K + epsilon;
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+P = Fd.*Fk(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    w = [u(r),v(r)];
+    k(w) = k(w) + 1;
+    switch mv2
+        case 'powerlaw'
+            Fk(:,w) = [((k + k(w(1)))/2) + epsilon, ((k + k(w(2)))/2) + epsilon].^gam;
+            Fk(w,:) = ([((k + k(w(1)))/2) + epsilon, ((k + k(w(2)))/2) + epsilon].^gam)';
+        case 'exponential'
+            Fk(:,w) = exp([((k + k(w(1)))/2) + epsilon, ((k + k(w(2)))/2) + epsilon]*gam);
+            Fk(w,:) = exp([((k + k(w(1)))/2) + epsilon, ((k + k(w(2)))/2) + epsilon]*gam)';
+    end
+    P = Fd.*Fk(indx);
+    b(i) = r;
+    P(b(1:i)) = 0;
+end
+b = indx(b);
+
+function b = fcn_deg_diff(A,K,D,m,eta,gam,modelvar,epsilon)
+n = length(D);
+mseed = nnz(A)/2;
+k = sum(A,2);
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+D = D(indx);
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+K = K + epsilon;
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+P = Fd.*Fk(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    
+    w = [u(r),v(r)];
+    k(w) = k(w) + 1;
+    switch mv2
+        case 'powerlaw'
+            Fk(:,w) = (abs([k - k(w(1)), k - k(w(2))]) + epsilon).^gam;
+            Fk(w,:) = ((abs([k - k(w(1)), k - k(w(2))]) + epsilon).^gam)';
+        case 'exponential'
+            Fk(:,w) = exp((abs([k - k(w(1)), k - k(w(2))]) + epsilon)*gam);
+            Fk(w,:) = exp((abs([k - k(w(1)), k - k(w(2))]) + epsilon)*gam)';
+    end
+    P = Fd.*Fk(indx);
+    b(i) = r;
+    P(b(1:i)) = 0;
+end
+b = indx(b);
+
+function b = fcn_deg_min(A,K,D,m,eta,gam,modelvar,epsilon)
+n = length(D);
+mseed = nnz(A)/2;
+k = sum(A,2);
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+D = D(indx);
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+K = K + epsilon;
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+P = Fd.*Fk(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    w = [u(r),v(r)];
+    k(w) = k(w) + 1;
+    switch mv2
+        case 'powerlaw'
+            Fk(:,w) = [min(k,k(w(1))) + epsilon, min(k,k(w(2))) + epsilon].^gam;
+            Fk(w,:) = ([min(k,k(w(1))) + epsilon, min(k,k(w(2))) + epsilon].^gam)';
+        case 'exponential'
+            Fk(:,w) = exp([min(k,k(w(1))) + epsilon, min(k,k(w(2))) + epsilon]*gam);
+            Fk(w,:) = exp([min(k,k(w(1))) + epsilon, min(k,k(w(2))) + epsilon]*gam)';
+    end
+    P = Fd.*Fk(indx);
+    b(i) = r;
+    P(b(1:i)) = 0;
+end
+b = indx(b);
+
+function b = fcn_deg_max(A,K,D,m,eta,gam,modelvar,epsilon)
+n = length(D);
+mseed = nnz(A)/2;
+k = sum(A,2);
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+D = D(indx);
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+K = K + epsilon;
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+P = Fd.*Fk(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    w = [u(r),v(r)];
+    k(w) = k(w) + 1;
+    switch mv2
+        case 'powerlaw'
+            Fk(:,w) = [max(k,k(w(1))) + epsilon, max(k,k(w(2))) + epsilon].^gam;
+            Fk(w,:) = ([max(k,k(w(1))) + epsilon, max(k,k(w(2))) + epsilon].^gam)';
+        case 'exponential'
+            Fk(:,w) = exp([max(k,k(w(1))) + epsilon, max(k,k(w(2))) + epsilon]*gam);
+            Fk(w,:) = exp([max(k,k(w(1))) + epsilon, max(k,k(w(2))) + epsilon]*gam)';
+    end
+    P = Fd.*Fk(indx);
+    b(i) = r;
+    P(b(1:i)) = 0;
+end
+b = indx(b);
+
+function b = fcn_deg_prod(A,K,D,m,eta,gam,modelvar,epsilon)
+n = length(D);
+mseed = nnz(A)/2;
+k = sum(A,2);
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+D = D(indx);
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+K = K + epsilon;
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+P = Fd.*Fk(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    w = [u(r),v(r)];
+    k(w) = k(w) + 1;
+    switch mv2
+        case 'powerlaw'
+            Fk(:,w) = ([k*k(w(1)) + epsilon, k*k(w(2)) + epsilon].^gam);
+            Fk(w,:) = (([k*k(w(1)) + epsilon, k*k(w(2)) + epsilon].^gam)');
+        case 'exponential'
+            Fk(:,w) = exp([k*k(w(1)) + epsilon, k*k(w(2)) + epsilon]*gam);
+            Fk(w,:) = exp([k*k(w(1)) + epsilon, k*k(w(2)) + epsilon]*gam)';
+    end
+    P = Fd.*Fk(indx);
+    b(i) = r;
+    P(b(1:i)) = 0;
+end
+b = indx(b);
+
+function b = fcn_nghbrs(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+A = A > 0;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+%         gam = abs(gam);
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    x = A(uu,:);
+    y = A(:,vv);
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    K(uu,y) = K(uu,y) + 1;
+    K(y,uu) = K(y,uu) + 1;
+    K(vv,x) = K(vv,x) + 1;
+    K(x,vv) = K(x,vv) + 1;
+    switch mv2
+        case 'powerlaw'
+            Ff(uu,y) = Fd(uu,y).*(K(uu,y).^gam);
+            Ff(y,uu) = Ff(uu,y)';
+            Ff(vv,x) = Fd(vv,x).*(K(vv,x).^gam);
+            Ff(x,vv) = Ff(vv,x)';
+        case 'exponential'
+            Ff(uu,y) = Fd(uu,y).*exp(K(uu,y)*gam);
+            Ff(y,uu) = Ff(uu,y)';
+            Ff(vv,x) = Fd(vv,x).*exp(K(vv,x)*gam);
+            Ff(x,vv) = Ff(vv,x)';
+    end
+    Ff(A) = 0;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_matching(A,K,D,m,eta,gam,modelvar,epsilon)
+K = K + epsilon;
+n = length(D);
+mseed = nnz(A)/2;
+mv1 = modelvar{1};
+mv2 = modelvar{2};
+switch mv1
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+switch mv2
+    case 'powerlaw'
+        Fk = K.^gam;
+    case 'exponential'
+        Fk = exp(gam*K);
+end
+Ff = Fd.*Fk.*~A;
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Ff(indx);
+for ii = (mseed + 1):m
+    
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    uu = u(r);
+    vv = v(r);
+    
+    A(uu,vv) = 1;
+    A(vv,uu) = 1;
+    
+    updateuu = find(A*A(:,uu));
+    updateuu(updateuu == uu) = [];
+    updateuu(updateuu == vv) = [];
+    
+    updatevv = find(A*A(:,vv));
+    updatevv(updatevv == uu) = [];
+    updatevv(updatevv == vv) = [];
+    
+    c1 = [A(:,uu)', A(uu,:)];
+    for i = 1:length(updateuu)
+        j = updateuu(i);
+        c2 = [A(:,j)' A(j,:)];
+        use = ~(~c1&~c2);
+        use(uu) = 0;  use(uu+n) = 0;
+        use(j) = 0;  use(j+n) = 0;
+        ncon = sum(c1(use))+sum(c2(use));
+        if (ncon==0)
+            K(uu,j) = epsilon;
+            K(j,uu) = epsilon;
+        else
+            K(uu,j) = (2*(sum(c1(use)&c2(use))/ncon)) + epsilon;
+            K(j,uu) = K(uu,j);
+        end
+        
+    end
+    
+    c1 = [A(:,vv)', A(vv,:)];
+    for i = 1:length(updatevv)
+        j = updatevv(i);
+        c2 = [A(:,j)' A(j,:)];
+        use = ~(~c1&~c2);
+        use(vv) = 0;  use(vv+n) = 0;
+        use(j) = 0;  use(j+n) = 0;
+        ncon = sum(c1(use))+sum(c2(use));
+        if (ncon==0)
+            K(vv,j) = epsilon;
+            K(j,vv) = epsilon;
+        else
+            K(vv,j) = (2*(sum(c1(use)&c2(use))/ncon)) + epsilon;
+            K(j,vv) = K(vv,j);
+        end
+    end
+    switch mv2
+        case 'powerlaw'
+            Fk = K.^gam;
+        case 'exponential'
+            Fk = exp(gam*K);
+    end
+    Ff = Fd.*Fk.*~A;
+    P = Ff(indx);
+end
+b = find(triu(A,1));
+
+function b = fcn_sptl(A,D,m,eta,modelvar)
+n = length(D);
+mseed = nnz(A)/2;
+switch modelvar
+    case 'powerlaw'
+        Fd = D.^eta;
+    case 'exponential'
+        Fd = exp(eta*D);
+end
+[u,v] = find(triu(ones(n),1));
+indx = (v - 1)*n + u;
+P = Fd(indx).*~A(indx);
+b = zeros(m,1);
+b(1:mseed) = find(A(indx));
+for i = (mseed + 1):m
+    C = [0; cumsum(P)];
+    r = sum(rand*C(end) >= C);
+    b(i) = r;
+    P = Fd(indx);
+    P(b(1:i)) = 0;
+end
+b = indx(b);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/get_components.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/get_components.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/get_components.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/get_components.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,57 @@
+function [comps,comp_sizes] = get_components(adj)
+% GET_COMPONENTS        Connected components
+%
+%   [comps,comp_sizes] = get_components(adj);
+%
+%   Returns the components of an undirected graph specified by the binary and 
+%   undirected adjacency matrix adj. Components and their constitutent nodes are 
+%   assigned the same index and stored in the vector, comps. The vector, comp_sizes,
+%   contains the number of nodes beloning to each component.
+%
+%   Inputs:         adj,    binary and undirected adjacency matrix
+%
+%   Outputs:      comps,    vector of component assignments for each node
+%            comp_sizes,    vector of component sizes
+%
+%   Note: disconnected nodes will appear as components of size 1
+%
+%   J Goni, University of Navarra and Indiana University, 2009/2011
+
+if size(adj,1)~=size(adj,2)
+    error('this adjacency matrix is not square');
+end
+
+if ~any(adj-triu(adj))
+  adj = adj | adj';
+end
+
+%if main diagonal of adj do not contain all ones, i.e. autoloops
+if sum(diag(adj))~=size(adj,1)
+    
+    %the main diagonal is set to ones
+    adj = adj|speye(size(adj));
+end
+
+%Dulmage-Mendelsohn decomposition
+[~,p,~,r] = dmperm(adj); 
+
+%p indicates a permutation (along rows and columns)
+%r is a vector indicating the component boundaries
+
+% List including the number of nodes of each component. ith entry is r(i+1)-r(i)
+comp_sizes = diff(r);
+
+% Number of components found.
+num_comps = numel(comp_sizes);
+
+% initialization
+comps = zeros(1,size(adj,1)); 
+
+% first position of each component is set to one
+comps(r(1:num_comps)) = ones(1,num_comps); 
+
+% cumulative sum produces a label for each component (in a consecutive way)
+comps = cumsum(comps); 
+
+%re-order component labels according to adj.
+comps(p) = comps; 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/grid_communities.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/grid_communities.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/grid_communities.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/grid_communities.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,46 @@
+function [X,Y,indsort] = grid_communities(c)
+% GRID_COMMUNITIES       Outline communities along diagonal
+%
+%   [X Y INDSORT] = GRID_COMMUNITIES(C) takes a vector of community
+%   assignments C and returns three output arguments for visualizing the
+%   communities. The third is INDSORT, which is an ordering of the vertices
+%   so that nodes with the same community assignment are next to one
+%   another. The first two arguments are vectors that, when overlaid on the
+%   adjacency matrix using the PLOT function, highlight the communities.
+%
+%   Example:
+%
+%   >> load AIJ;                                % load adjacency matrix
+%   >> [C,Q] = modularity_louvain_und(AIJ);     % get community assignments
+%   >> [X,Y,INDSORT] = fcn_grid_communities(C); % call function
+%   >> imagesc(AIJ(INDSORT,INDSORT));           % plot ordered adjacency matrix
+%   >> hold on;                                 % hold on to overlay community visualization
+%   >> plot(X,Y,'r','linewidth',2);             % plot community boundaries
+%
+%   Inputs:     C,       community assignments
+%
+%   Outputs:    X,       x coor
+%               Y,       y coor
+%               INDSORT, indices
+%
+%   Richard Betzel, Indiana University, 2012
+%
+
+%#ok<*AGROW>
+
+nc = max(c);
+[c,indsort] = sort(c);
+
+X = [];
+Y = [];
+for i = 1:nc
+    ind = find(c == i);
+    if ~isempty(ind)
+        mn = min(ind) - 0.5;
+        mx = max(ind) + 0.5;
+        x = [mn mn mx mx mn NaN];
+        y = [mn mx mx mn mn NaN];
+        X = [X, x]; 
+        Y = [Y, y];
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/gtom.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/gtom.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/gtom.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/gtom.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,81 @@
+function gt = gtom(adj,numSteps)
+%GTOM       Generalized topological overlap measure
+%
+%   gt = gtom(adj,numSteps);
+%
+%   The m-th step generalized topological overlap measure (GTOM) quantifies
+%   the extent to which a pair of nodes have similar m-th step neighbors.
+%   Mth-step neighbors are nodes that are reachable by a path of at most
+%   length m.
+%
+%   This function computes the the M x M generalized topological overlap
+%   measure (GTOM) matrix for number of steps, numSteps. 
+%
+%   Inputs:       adj,    adjacency matrix (binary,undirected)
+%            numSteps,    number of steps
+%
+%   Outputs:       gt,    GTOM matrix
+%
+%   NOTE: When numSteps is equal to 1, GTOM is identical to the topological
+%   overlap measure (TOM) from reference [2]. In that case the 'gt' matrix
+%   records, for each pair of nodes, the fraction of neighbors the two
+%   nodes share in common, where "neighbors" are one step removed. As
+%   'numSteps' is increased, neighbors that are furter out are considered.
+%   Elements of 'gt' are bounded between 0 and 1.  The 'gt' matrix can be
+%   converted from a similarity to a distance matrix by taking 1-gt.
+%
+%   References: [1] Yip & Horvath (2007) BMC Bioinformatics 2007, 8:22
+%               [2] Ravasz et al (2002) Science 297 (5586), 1551.
+%
+%   J Goni, University of Navarra and Indiana University, 2009/2011
+
+%#ok<*ASGLU>
+
+%initial state for bm matrix;
+bm = adj;
+bmAux = bm;
+numNodes = size(adj,1);
+
+if (numSteps > numNodes)
+    disp('warning, reached maximum value for numSteps. numSteps reduced to adj-size')
+    numSteps = numNodes;
+end
+
+if (numSteps == 0)
+    %GTOM0
+    gt = adj;
+else
+    
+    for steps = 2:numSteps
+        for i = 1:numNodes
+            
+            %neighbours of node i
+            [neighRow,neighColumn] = find(bm(i,:)==1); 
+            
+            %neighbours of neighbours of node i
+            [neighNeighRow,neighNeighColumn] = find(bm(neighColumn,:)==1);
+            newNeigh = setdiff(unique(neighNeighColumn),i);
+            
+            %neighbours of neighbours of node i become considered node i neighbours
+            bmAux(i,newNeigh) = 1;
+            
+            %keep symmetry of matrix
+            bmAux(newNeigh,i) = 1;
+        end
+        %bm is updated with new step all at once
+        bm = bmAux;
+        
+    end
+    
+    clear bmAux newNeigh;
+    
+    %numerators of GTOM formula
+    numeratorMatrix = bm*bm + adj + speye(numNodes,numNodes);
+    
+    %vector containing degree of each node
+    bmSum=sum(bm);  
+    clear bm;
+    
+    denominatorMatrix = -adj + min(repmat(bmSum,numNodes,1),repmat(bmSum',1,numNodes)) + 1;
+    gt = numeratorMatrix ./ denominatorMatrix;
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/jdegree.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/jdegree.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/jdegree.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/jdegree.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,48 @@
+function [J,J_od,J_id,J_bl] = jdegree(CIJ)
+%JDEGREE        Joint degree distribution
+%
+%   [J,J_od,J_id,J_bl] = jdegree(CIJ);
+%
+%   This function returns a matrix in which the value of each element (u,v)
+%   corresponds to the number of nodes that have u outgoing connections 
+%   and v incoming connections.
+%
+%   Input:      CIJ,    directed (weighted/binary) connection matrix
+%
+%   Outputs:    J,      joint degree distribution matrix (shifted by one)
+%               J_od,   number of vertices with od>id.
+%               J_id,   number of vertices with id>od.
+%               J_bl,   number of vertices with id=od.
+%
+%   Note: Weights are discarded.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2006/2008
+
+
+% ensure CIJ is binary...
+CIJ = double(CIJ~=0);
+
+N = size(CIJ,1);
+
+id = sum(CIJ,1);    % indegree = column sum of CIJ
+od = sum(CIJ,2)';   % outdegree = row sum of CIJ
+
+% Create the joint degree distribution matrix
+% Note:  the matrix is shifted by one, to accomodate zero id and od in the first row/column.
+% Upper triangular part of the matrix has vertices with an excess of 
+%    outgoing edges (od>id)
+% Lower triangular part of the matrix has vertices with an excess of
+%    outgoing edges (id>od)
+% Main diagonal has units with id=od
+
+szJ = max(max(id,od))+1;
+J = zeros(szJ);
+
+for i=1:N
+   J(id(i)+1,od(i)+1) = J(id(i)+1,od(i)+1) + 1;
+end;
+
+J_od = sum(sum(triu(J,1)));
+J_id = sum(sum(tril(J,-1)));
+J_bl = sum(diag(J));
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,59 @@
+function [CIJkcore,kn,peelorder,peellevel] = kcore_bd(CIJ,k)
+%KCORE_BD       K-core
+%
+%   [CIJkcore,kn,peelorder,peellevel] = kcore_bd(CIJ,k);
+%
+%   The k-core is the largest subnetwork comprising nodes of degree at
+%   least k. This function computes the k-core for a given binary directed
+%   connection matrix by recursively peeling off nodes with degree lower
+%   than k, until no such nodes remain.
+%
+%   input:          CIJ,        connection/adjacency matrix (binary, directed)
+%                     k,        level of k-core
+%
+%   output:    CIJkcore,        connection matrix of the k-core.  This matrix
+%                               only contains nodes of degree at least k.
+%                    kn,        size of k-core
+%                    peelorder, indices in the order in which they were
+%                               peeled away during k-core decomposition
+%                    peellevel, corresponding level - nodes at the same
+%                               level have been peeled away at the same time
+%
+%   'peelorder' and 'peellevel' are similar the the k-core sub-shells
+%   described in Modha and Singh (2010).
+%
+%   References: e.g. Hagmann et al. (2008) PLoS Biology
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+%#ok<*ASGLU>
+%#ok<*AGROW>
+
+peelorder = [];
+peellevel = [];
+iter = 0;
+
+while 1 
+
+    % get degrees of matrix
+    [id,od,deg] = degrees_dir(CIJ); 
+
+    % find nodes with degree <k
+    ff = find((deg<k)&(deg>0));
+    
+    % if none found -> stop
+    if (isempty(ff)) break; end;            %#ok<SEPEX>
+
+    % peel away found nodes
+    iter = iter+1;
+    CIJ(ff,:) = 0;
+    CIJ(:,ff) = 0;
+    
+    peelorder = [peelorder; ff']; 
+    peellevel = [peellevel; iter.*ones(1,length(ff))'];
+    
+end;
+
+CIJkcore = CIJ;
+kn = sum(deg>0);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcore_bu.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,58 @@
+function [CIJkcore,kn,peelorder,peellevel] = kcore_bu(CIJ,k)
+%KCORE_BU       K-core
+%
+%   [CIJkcore,kn,peelorder,peellevel] = kcore_bu(CIJ,k);
+%
+%   The k-core is the largest subnetwork comprising nodes of degree at
+%   least k. This function computes the k-core for a given binary
+%   undirected connection matrix by recursively peeling off nodes with
+%   degree lower than k, until no such nodes remain.
+%
+%   input:          CIJ,        connection/adjacency matrix (binary, undirected)
+%                     k,        level of k-core
+%
+%   output:    CIJkcore,        connection matrix of the k-core.  This matrix
+%                               only contains nodes of degree at least k.
+%                    kn,        size of k-core
+%                    peelorder, indices in the order in which they were
+%                               peeled away during k-core decomposition
+%                    peellevel, corresponding level - nodes at the same
+%                               level were peeled away at the same time
+%
+%   'peelorder' and 'peellevel' are similar the the k-core sub-shells
+%   described in Modha and Singh (2010).
+%
+%   References: e.g. Hagmann et al. (2008) PLoS Biology
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+%#ok<*AGROW>
+
+peelorder = [];
+peellevel = [];
+iter = 0;
+
+while 1 
+
+    % get degrees of matrix
+    [deg] = degrees_und(CIJ);
+
+    % find nodes with degree <k
+    ff = find((deg<k)&(deg>0));
+    
+    % if none found -> stop
+    if (isempty(ff)) break; end;            %#ok<SEPEX>
+
+    % peel away found nodes
+    iter = iter+1;
+    CIJ(ff,:) = 0;
+    CIJ(:,ff) = 0;
+    
+    peelorder = [peelorder; ff']; 
+    peellevel = [peellevel; iter.*ones(1,length(ff))'];
+    
+end;
+
+CIJkcore = CIJ;
+kn = sum(deg>0);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,27 @@
+function  [coreness,kn] = kcoreness_centrality_bd(CIJ)
+%KCORENESS_CENTRALITY_BD       K-coreness centrality
+%
+%   [coreness,kn] = kcoreness_centrality_bd(CIJ)
+%
+%   The k-core is the largest subgraph comprising nodes of degree at least
+%   k. The coreness of a node is k if the node belongs to the k-core but
+%   not to the (k+1)-core. This function computes k-coreness of all nodes
+%   for a given binary directed connection matrix.
+%
+%   input:          CIJ,        connection/adjacency matrix (binary, directed)
+%
+%   output:    coreness,        node coreness.
+%                    kn,        size of k-core
+%
+%   References: e.g. Hagmann et al. (2008) PLoS Biology
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+N = size(CIJ,1);
+
+coreness = zeros(1,N); kn = zeros(1,N);
+for k=1:N
+    [CIJkcore,kn(k)] = kcore_bd(CIJ,k);
+    ss = sum(CIJkcore)>0;
+    coreness(ss) = k;
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/kcoreness_centrality_bu.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,34 @@
+function  [coreness,kn] = kcoreness_centrality_bu(CIJ)
+%KCORENESS_CENTRALITY_BU       K-coreness centrality
+%
+%   [coreness,kn] = kcoreness_centrality_bu(CIJ)
+%
+%   The k-core is the largest subgraph comprising nodes of degree at least
+%   k. The coreness of a node is k if the node belongs to the k-core but
+%   not to the (k+1)-core. This function computes the coreness of all nodes
+%   for a given binary undirected connection matrix.
+%
+%   input:          CIJ,        connection/adjacency matrix (binary, undirected)
+%
+%   output:    coreness,        node coreness.
+%                    kn,        size of k-core
+%
+%   References: e.g. Hagmann et al. (2008) PLoS Biology
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+N = size(CIJ,1);
+
+% determine if the network is undirected - if not, compute coreness on the
+% corresponding undirected network
+CIJund = CIJ+CIJ';
+if (any(CIJund(:)>1))
+    CIJ = double(CIJund>0);
+end;
+
+coreness = zeros(1,N); kn = zeros(1,N);
+for k=1:N
+    [CIJkcore,kn(k)] = kcore_bu(CIJ,k);
+    ss = sum(CIJkcore)>0;
+    coreness(ss) = k;
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,91 @@
+function [Rlatt,Rrp,ind_rp,eff] = latmio_dir(R,ITER,D)
+%LATMIO_DIR     Lattice with preserved in/out degree distribution
+%
+%   [Rlatt,Rrp,ind_rp,eff] = latmio_dir(R,ITER,D);
+%
+%   This function "latticizes" a directed network, while preserving the in-
+%   and out-degree distributions. In weighted networks, the function
+%   preserves the out-strength but not the in-strength distributions.
+%
+%   Input:      R,      directed (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%               D,      distance-to-diagonal matrix
+%
+%   Output:     Rlatt,  latticized network in original node ordering
+%               Rrp,    latticized network in node ordering used for
+%                       latticization
+%               ind_rp, node ordering used for latticization
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%               Sporns and Zwi (2004) Neuroinformatics 2:145
+%
+%   Mika Rubinov, UNSW, 2007-2010
+%   Olaf Sporns, IU, 2012
+
+n=size(R,1);
+ 
+% randomly reorder matrix
+ind_rp = randperm(n);
+R = R(ind_rp,ind_rp);
+
+% create 'distance to diagonal' matrix
+if nargin<3 %if D is not specified by user
+    D=zeros(n);
+    u=[0 min([mod(1:n-1,n);mod(n-1:-1:1,n)])];
+    for v=1:ceil(n/2)
+        D(n-v+1,:)=u([v+1:n 1:v]);
+        D(v,:)=D(n-v+1,n:-1:1);
+    end
+end
+%end create
+
+[i,j]=find(R);
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %lattice condition
+            if (D(a,b)*R(a,b)+D(c,d)*R(c,d))>=(D(a,d)*R(a,b)+D(c,b)*R(c,d))
+                R(a,d)=R(a,b); R(a,b)=0;
+                R(c,b)=R(c,d); R(c,d)=0;
+
+                j(e1) = d;          %reassign edge indices
+                j(e2) = b;
+                eff = eff+1;
+                break;
+            end %lattice condition
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
+
+% lattice in node order used for latticization
+Rrp = R;
+% reverse random permutation of nodes
+[~,ind_rp_reverse] = sort(ind_rp);
+Rlatt = Rrp(ind_rp_reverse,ind_rp_reverse);
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir_connected.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir_connected.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir_connected.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_dir_connected.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,119 @@
+function [Rlatt,Rrp,ind_rp,eff] = latmio_dir_connected(R,ITER,D)
+%LATMIO_DIR_CONNECTED     Lattice with preserved in/out degree distribution
+%
+%   [Rlatt,Rrp,ind_rp,eff] = latmio_dir_connected(R,ITER,D);
+%
+%   This function "latticizes" a directed network, while preserving the in-
+%   and out-degree distributions. In weighted networks, the function
+%   preserves the out-strength but not the in-strength distributions. The 
+%   function also ensures that the randomized network maintains
+%   connectedness, the ability for every node to reach every other node in
+%   the network. The input network for this function must be connected.
+%
+%   Input:      R,      directed (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%               D,      distance-to-diagonal matrix
+%
+%   Output:     Rlatt,  latticized network in original node ordering
+%               Rrp,    latticized network in node ordering used for
+%                       latticization
+%               ind_rp, node ordering used for latticization
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%               Sporns and Zwi (2004) Neuroinformatics 2:145
+%
+%   Mika Rubinov, UNSW, 2007-2010
+%   Olaf Sporns, Indiana University, 2012
+
+n=size(R,1);
+
+% randomly reorder matrix
+ind_rp = randperm(n);
+R = R(ind_rp,ind_rp);
+
+% create 'distance to diagonal' matrix
+if nargin<3 %if D is not specified by user
+    D=zeros(n);
+    u=[0 min([mod(1:n-1,n);mod(n-1:-1:1,n)])];
+    for v=1:ceil(n/2)
+        D(n-v+1,:)=u([v+1:n 1:v]);
+        D(v,:)=D(n-v+1,n:-1:1);
+    end
+end
+%end create
+
+[i,j]=find(R);
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        rewire=1;
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %lattice condition
+            if (D(a,b)*R(a,b)+D(c,d)*R(c,d))>=(D(a,d)*R(a,b)+D(c,b)*R(c,d))
+                %connectedness condition
+                if ~(any([R(a,c) R(d,b) R(d,c)]) && any([R(c,a) R(b,d) R(b,a)]))
+                    P=R([a c],:);
+                    P(1,b)=0; P(1,d)=1;
+                    P(2,d)=0; P(2,b)=1;
+                    PN=P;
+                    PN(1,a)=1; PN(2,c)=1;
+
+                    while 1
+                        P(1,:)=any(R(P(1,:)~=0,:),1);
+                        P(2,:)=any(R(P(2,:)~=0,:),1);
+                        P=P.*(~PN);
+                        PN=PN+P;
+                        if ~all(any(P,2))
+                            rewire=0;
+                            break
+                        elseif any(PN(1,[b c])) && any(PN(2,[d a]))
+                            break
+                        end
+                    end
+                end %connectedness testing
+
+                if rewire               %reassign edges
+                    R(a,d)=R(a,b); R(a,b)=0;
+                    R(c,b)=R(c,d); R(c,d)=0;
+
+                    j(e1) = d;          %reassign edge indices
+                    j(e2) = b;
+                    eff = eff+1;
+                    break;
+                end %edge reassignment
+            end %lattice condition
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
+
+% lattice in node order used for latticization
+Rrp = R;
+% reverse random permutation of nodes
+[~,ind_rp_reverse] = sort(ind_rp);
+Rlatt = Rrp(ind_rp_reverse,ind_rp_reverse);
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,109 @@
+function [Rlatt,Rrp,ind_rp,eff] = latmio_und(R,ITER,D)
+%LATMIO_UND     Lattice with preserved degree distribution
+%
+%   [Rlatt,Rrp,ind_rp,eff] = latmio_und(R,ITER,D);
+%
+%   This function "latticizes" an undirected network, while preserving the 
+%   degree distribution. The function does not preserve the strength 
+%   distribution in weighted networks.
+%
+%   Input:      R,      undirected (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%               D,      distance-to-diagonal matrix
+%
+%   Output:     Rlatt,  latticized network in original node ordering
+%               Rrp,    latticized network in node ordering used for
+%                       latticization
+%               ind_rp, node ordering used for latticization
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%               Sporns and Zwi (2004) Neuroinformatics 2:145
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Apr 2008: Edge c-d is flipped with 50% probability, allowing to explore
+%             all potential rewirings (Jonathan Power)
+%   Feb 2012: limit on number of attempts, distance-to-diagonal as input,
+%             count number of successful rewirings (Olaf Sporns)
+%   Feb 2012: permute node ordering on each run, to ensure lattices are
+%             shuffled across mutliple runs (Olaf Sporns)
+
+n=size(R,1);
+
+% randomly reorder matrix
+ind_rp = randperm(n);
+R = R(ind_rp,ind_rp);
+    
+% create 'distance to diagonal' matrix
+if nargin<3 %if D is not specified by user
+    D=zeros(n);
+    u=[0 min([mod(1:n-1,n);mod(n-1:-1:1,n)])];
+    for v=1:ceil(n/2)
+        D(n-v+1,:)=u([v+1:n 1:v]);
+        D(v,:)=D(n-v+1,n:-1:1);
+    end
+end
+%end create
+
+[i,j]=find(tril(R));
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)/2));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+
+        if rand>0.5
+            i(e2)=d; j(e2)=c; 	%flip edge c-d with 50% probability
+            c=i(e2); d=j(e2); 	%to explore all potential rewirings
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %lattice condition
+            if (D(a,b)*R(a,b)+D(c,d)*R(c,d))>=(D(a,d)*R(a,b)+D(c,b)*R(c,d))
+                R(a,d)=R(a,b); R(a,b)=0;
+                R(d,a)=R(b,a); R(b,a)=0;
+                R(c,b)=R(c,d); R(c,d)=0;
+                R(b,c)=R(d,c); R(d,c)=0;
+
+                j(e1) = d;          %reassign edge indices
+                j(e2) = b;
+                eff = eff+1;
+                break;
+            end %lattice condition
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
+
+% lattice in node order used for latticization
+Rrp = R;
+% reverse random permutation of nodes
+[~,ind_rp_reverse] = sort(ind_rp);
+Rlatt = Rrp(ind_rp_reverse,ind_rp_reverse);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und_connected.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und_connected.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und_connected.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/latmio_und_connected.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,136 @@
+function [Rlatt,Rrp,ind_rp,eff] = latmio_und_connected(R,ITER,D)
+%LATMIO_UND_CONNECTED     Lattice with preserved degree distribution
+%
+%   [Rlatt,Rrp,ind_rp,eff] = latmio_und_connected(R,ITER,D);
+%
+%   This function "latticizes" an undirected network, while preserving the 
+%   degree distribution. The function does not preserve the strength 
+%   distribution in weighted networks. The function also ensures that the 
+%   randomized network maintains connectedness, the ability for every node 
+%   to reach every other node in the network. The input network for this 
+%   function must be connected.
+%
+%   Input:      R,      undirected (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%               D,      distance-to-diagonal matrix
+%
+%   Output:     Rlatt,  latticized network in original node ordering
+%               Rrp,    latticized network in node ordering used for
+%                       latticization
+%               ind_rp, node ordering used for latticization
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%               Sporns and Zwi (2004) Neuroinformatics 2:145
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Apr 2008: Edge c-d is flipped with 50% probability, allowing to explore
+%             all potential rewirings (Jonathan Power)
+%   Feb 2012: limit on number of attempts, distance-to-diagonal as input,
+%             count number of successful rewirings (Olaf Sporns)
+%   Feb 2012: permute node ordering on each run, to ensure lattices are
+%             shuffled across mutliple runs (Olaf Sporns)
+
+n=size(R,1);
+
+% randomly reorder matrix
+ind_rp = randperm(n);
+R = R(ind_rp,ind_rp);
+
+% create 'distance to diagonal' matrix
+if nargin<3 %if D is not specified by user
+    D=zeros(n);
+    u=[0 min([mod(1:n-1,n);mod(n-1:-1:1,n)])];
+    for v=1:ceil(n/2)
+        D(n-v+1,:)=u([v+1:n 1:v]);
+        D(v,:)=D(n-v+1,n:-1:1);
+    end
+end
+%end create
+
+[i,j]=find(tril(R));
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)/2));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        rewire=1;
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+
+        if rand>0.5
+            i(e2)=d; j(e2)=c; 	%flip edge c-d with 50% probability
+            c=i(e2); d=j(e2); 	%to explore all potential rewirings
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %lattice condition
+            if (D(a,b)*R(a,b)+D(c,d)*R(c,d))>=(D(a,d)*R(a,b)+D(c,b)*R(c,d))
+                %connectedness condition
+                if ~(R(a,c) || R(b,d))
+                    P=R([a d],:);
+                    P(1,b)=0; P(2,c)=0;
+                    PN=P;
+                    PN(:,d)=1; PN(:,a)=1;
+
+                    while 1
+                        P(1,:)=any(R(P(1,:)~=0,:),1);
+                        P(2,:)=any(R(P(2,:)~=0,:),1);
+                        P=P.*(~PN);
+                        if ~all(any(P,2))
+                            rewire=0;
+                            break
+                        elseif any(any(P(:,[b c])))
+                            break
+                        end
+                        PN=PN+P;
+                    end
+                end %connectedness testing
+
+                if rewire               %reassign edges
+                    R(a,d)=R(a,b); R(a,b)=0;
+                    R(d,a)=R(b,a); R(b,a)=0;
+                    R(c,b)=R(c,d); R(c,d)=0;
+                    R(b,c)=R(d,c); R(d,c)=0;
+
+                    j(e1) = d;          %reassign edge indices
+                    j(e2) = b;
+                    eff = eff+1;
+                    break;
+                end %edge reassignment
+            end %lattice condition
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
+
+% lattice in node order used for latticization
+Rrp = R;
+% reverse random permutation of nodes
+[~,ind_rp_reverse] = sort(ind_rp);
+Rlatt = Rrp(ind_rp_reverse,ind_rp_reverse);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/link_communities.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/link_communities.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/link_communities.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/link_communities.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,150 @@
+function M=link_communities(W,type_clustering)
+%LINK_COMMUNITIES     Optimal overlapping community structure
+%
+%   M = link_communities(W)
+%   M = link_communities(W,'complete');
+%
+%   The optimal community structure is a subdivision of the network into
+%   groups of nodes which have a high number of within-group connections
+%   and a low number of between group connections.
+%
+%   This algorithm uncovers overlapping community structure via
+%   hierarchical clustering of network links. This algorith is generalized
+%   for weighted/directed/fully-connected networks.
+%
+%   Input:      W,                  directed (weighted or binary) connection matrix.
+%               type_clustering,    type of hierarchical clustering (optional)
+%                                       'single'        single-linkage (default)
+%                                       'complete'      complete-linkage
+%
+%   Output:     M,                  nodal community-affiliation matrix
+%                                   binary matrix of size CxN [communities x nodes]
+%
+%   NB: The algorithm can be slow and memory intensive.
+%
+%   Reference: Ahn, Bagrow and Lehmann (2010) Nature 466, 761764.
+%
+%   Mika Rubinov, U Cambridge, 2014-2015
+
+%% initialize
+
+n=size(W,1);                                                        % number of nodes
+W(1:n+1:end)=0;
+W=W./max(W(:));                                                     % normalize weights
+
+if ~exist('type_clustering','var')
+    type_clustering='single';
+end
+
+%% get node similarity
+
+W(1:n+1:end) = ( sum(W)/sum(W~=0) + sum(W.')/sum(W.'~=0) )/2;       % mean weight on diagonal
+No=sum(W.^2,2);                                                     % out-norm squared
+Ni=sum(W.^2,1);                                                     % in-norm squared
+
+Jo=zeros(n);                                                        % weighted in-Jaccard
+Ji=zeros(n);                                                        % weighted ou-Jaccard
+for b=1:n
+    for c=1:n
+        Do=W(b,:)*W(c,:).';
+        Jo(b,c)=Do./(No(b)+No(c)-Do);
+        
+        Di=W(:,b).'*W(:,c);
+        Ji(b,c)=Di./(Ni(b)+Ni(c)-Di);
+    end
+end
+
+%% get link similarity
+
+[A,B]=find( (W|W.') & triu(ones(n),1));
+m=length(A);
+Ln=zeros(m,2);                                                      % link nodes
+Lw=zeros(m,1);                                                      % link weights
+for i=1:m
+    Ln(i,:) = [A(i) B(i)];                                          % link nodes
+    Lw(i) = (W(A(i),B(i))+W(B(i),A(i)))/2;                          % link weight
+end
+
+ES=zeros(m,m,'single');                                             % link similarity
+for i=1:m
+    for j=1:m
+        if      Ln(i,1)==Ln(j,1); a=Ln(i,1); b=Ln(i,2); c=Ln(j,2);
+        elseif  Ln(i,1)==Ln(j,2); a=Ln(i,1); b=Ln(i,2); c=Ln(j,1);
+        elseif  Ln(i,2)==Ln(j,1); a=Ln(i,2); b=Ln(i,1); c=Ln(j,2);
+        elseif  Ln(i,2)==Ln(j,2); a=Ln(i,2); b=Ln(i,1); c=Ln(j,1);
+        else    continue
+        end
+        
+        ES(i,j) = (W(a,b)*W(a,c)*Ji(b,c) + W(b,a)*W(c,a)*Jo(b,c))/2;
+    end
+end
+ES(1:m+1:end)=0;
+
+%% perform hierarchical clustering
+
+C=zeros(m,m,'single');                                              % community affiliation matrix
+Nc=C; Mc=C; Dc=C;                                                   % communities nodes, links and density
+U=1:m;                                                              % initial community assignments
+C(1,:)=U;                                                           % as above, in the matrix
+
+for i=1:m-1; fprintf('hierarchy%8d\n',i)                            % hierarchy level
+    
+    % compute densities
+    for j=1:length(U)                                               % loop over communities
+        idx = C(i,:)==U(j);                                         % get link indices
+        links = sort(Lw(idx));                                      % sort link weights
+        nodes = sort(reshape(Ln(idx,:),2*nnz(idx),1));
+        nodes = nodes([true;nodes(2:end)~=nodes(1:end-1)]);         % get unique nodes
+        
+        nc = numel(nodes);                                          % community nodes
+        mc = sum(links);                                            % community weights
+        min_mc = sum(links(1:nc-1));                                % minimal weight
+        dc = (mc - min_mc) / (nc.*(nc-1)/2 - min_mc);               % community density
+        
+        Nc(i,j)=nc;
+        Mc(i,j)=mc;
+        Dc(i,j)=dc;
+    end
+    
+    % cluster
+    C(i+1,:)=C(i,:);                                                % copy current partition
+    [u1,u2]=find(ES(U,U)==max(max(ES(U,U))));                       % on this line MAXs MUST BE MAXs
+    
+    V=U(unique(sortrows(sort([u1 u2],2)),'rows'));                  % get unique links
+    for j=1:size(V,1)
+        switch type_clustering
+            case 'single';      x = max(ES(V(j,:),:),[],1);         % max -> single linkage
+            case 'complete';    x = min(ES(V(j,:),:),[],1);         % min -> complete linkage
+            otherwise; error('Unknown clustering type.');
+        end
+        ES(V(j,:),:) = [x;x];                                       % assign distances to whole clusters
+        ES(:,V(j,:)) = [x;x].';
+        ES(V(j,1),V(j,1)) = 0;                                      % clear diagonal
+        ES(V(j,2),V(j,2)) = 0;                                      % clear diagonal
+        
+        C(i+1,C(i+1,:)==V(j,2)) = V(j,1);                           % merge communities
+        V(V==V(j,2)) = V(j,1);                                      % merge indices
+    end
+    
+    U=unique(C(i+1,:));                                             % get unique communities
+    if numel(U)==1
+        break;
+    end
+end
+
+%%
+
+Dc(isnan(Dc))=0;
+[~,i]=max(sum(Dc.*Mc,2));                                           % get maximal density
+
+U=unique(C(i,:));                                                   % unique communities
+M=zeros(1,n);                                                       % nodal affiliations
+for j=1:length(U)
+    M(j,unique( Ln(C(i,:)==U(j),:)) )=1;
+end
+M=M(sum(M,2)>2,:);
+
+% M2=zeros(n);                                                      % two dimensional nodal affiliation
+% for i=1:size(M,1);
+%     M2=M2+(M(i,:).'*ones(1,n) & ones(n,1)*M(i,:));
+% end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/local_assortativity_wu_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/local_assortativity_wu_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/local_assortativity_wu_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/local_assortativity_wu_sign.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,44 @@
+function [loc_assort_pos,loc_assort_neg] = local_assortativity_wu_sign(W)
+%LOCAL_ASSORTATIVITY_WU_SIGN     Local Assortativity
+%
+%   [loc_assort_pos,loc_assort_neg] = local_assortativity_wu_sign(W);
+%
+%   Local Assortativity measures the extent to which nodes are connected to
+%   nodes of similar strength (vs. higher or lower strength). Adapted from
+%   Thedchanamoorthy et al. (2014)'s formula to allow weighted/signed 
+%   networks (node degree replaced with node strength). Note, output values 
+%   sum to total assortativity. 
+%
+%   Inputs:     W,        undirected connection matrix with positive and
+%                         negative weights
+%
+%   Output:     loc_assort_pos, local assortativity from positive weights
+%
+%               loc_assort_neg, local assortativity from negative weights
+%
+%   Reference: Thedchanamoorthy G, Piraveenan M, Kasthuriratna D, 
+%              Senanayake U. Proc Comp Sci (2014) 29:2449-2461.
+%
+%
+%   Jeff Spielberg, Boston University
+
+%   Modification History:
+%   May 2015: Original
+
+W(1:(size(W,1)+1):end) = 0;
+r_pos = assortativity_wei(W.*(W>0),0);
+r_neg = assortativity_wei(-W.*(W<0),0);
+[str_pos,str_neg] = strengths_und_sign(W);
+loc_assort_pos = nan(size(W,1),1);
+loc_assort_neg = nan(size(W,1),1);
+
+for curr_node = 1:size(W,1)
+    [~,j_pos] = find(W(curr_node,:)>0);
+    loc_assort_pos(curr_node,1) = sum(abs(str_pos(j_pos)-str_pos(curr_node)))/str_pos(curr_node);
+    
+    [~,j_neg] = find(W(curr_node,:)<0);
+    loc_assort_neg(curr_node,1) = sum(abs(str_neg(j_neg)-str_neg(curr_node)))/str_neg(curr_node);
+end
+
+loc_assort_pos = ((r_pos+1)/size(W,1))-(loc_assort_pos/sum(loc_assort_pos));
+loc_assort_neg = ((r_neg+1)/size(W,1))-(loc_assort_neg/sum(loc_assort_neg));
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/make_motif34lib.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/make_motif34lib.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/make_motif34lib.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/make_motif34lib.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,90 @@
+function make_motif34lib
+%MAKE_MOTIF34LIB        Auxiliary motif library function
+%
+%   make_motif34lib;
+%
+%   This function generates the motif34lib.mat library required for all
+%   other motif computations.
+%
+%
+%   Mika Rubinov, UNSW, 2007-2010
+
+%#ok<*ASGLU>
+
+[M3,M3n,ID3,N3]=motif3generate; 
+[M4,M4n,ID4,N4]=motif4generate;
+save motif34lib;
+
+function [M,Mn,ID,N]=motif3generate
+n=0;
+M=false(54,6);                  %isomorphs
+CL=zeros(54,6,'uint8');         %canonical labels (predecessors of IDs)
+cl=zeros(1,6,'uint8');
+for i=0:2^6-1                   %loop through all subgraphs
+    m=dec2bin(i);
+    m=[num2str(zeros(1,6-length(m)), '%d') m];  %#ok<AGROW>
+    G=str2num ([ ...
+        '0'   ' '  m(3)  ' '  m(5) ;
+        m(1)  ' '  '0'   ' '  m(6) ;
+        m(2)  ' '  m(4)  ' '  '0'   ]);         %#ok<ST2NM>
+    Ko=sum(G,2);
+    Ki=sum(G,1).';
+    if all(Ko|Ki)               %if subgraph weakly-connected
+        n=n+1;
+        cl(:)=sortrows([Ko Ki]).';
+        CL(n,:)=cl;             %assign motif label to isomorph
+        M(n,:)=G([2:4 6:8]);
+    end
+end
+[u1,u2,ID]=unique(CL,'rows');   %convert CLs into motif IDs
+
+%convert IDs into Sporns & Kotter classification
+id_mika=  [1  3  4  6  7  8  11];
+id_olaf= -[3  6  1 11  4  7   8];
+for id=1:length(id_mika)
+    ID(ID==id_mika(id))=id_olaf(id);
+end
+ID=abs(ID);
+
+[X,ind]=sortrows(ID);
+ID=ID(ind,:);               %sort IDs
+M=M(ind,:);                 %sort isomorphs
+N=sum(M,2);                 %number of edges
+Mn=uint32(sum(repmat(10.^(5:-1:0),size(M,1),1).*M,2));  %M as a single number
+
+function [M,Mn,ID,N]=motif4generate
+n=0;
+M=false(3834,12);               %isomorphs
+CL=zeros(3834,16,'uint8');      %canonical labels (predecessors of IDs)
+cl=zeros(1,16,'uint8');
+for i=0:2^12-1                  %loop through all subgraphs
+    m=dec2bin(i);
+    m=[num2str(zeros(1,12-length(m)), '%d') m];     %#ok<AGROW>
+    G=str2num ([ ...
+        '0'   ' '  m(4)  ' '  m(7)  ' '  m(10) ;
+        m(1)  ' '  '0'   ' '  m(8)  ' '  m(11) ;
+        m(2)  ' '  m(5)  ' '  '0'   ' '  m(12) ;
+        m(3)  ' '  m(6)  ' '  m(9)  ' '  '0'    ]); %#ok<ST2NM>
+    Gs=G+G.';
+    v=Gs(1,:);
+    for j=1:2
+        v=any(Gs(v~=0,:),1)+v;
+    end
+    if v                        %if subgraph weakly connected
+        n=n+1;
+        G2=(G*G)~=0;
+        Ko=sum(G,2);
+        Ki=sum(G,1).';
+        Ko2=sum(G2,2);
+        Ki2=sum(G2,1).';
+        cl(:)=sortrows([Ki Ko Ki2 Ko2]).';
+        CL(n,:)=cl;             %assign motif label to isomorph
+        M(n,:)=G([2:5 7:10 12:15]);
+    end
+end
+[u1,u2,ID]=unique(CL,'rows');   %convert CLs into motif IDs
+[X,ind]=sortrows(ID);   
+ID=ID(ind,:);                   %sort IDs
+M=M(ind,:);                     %sort isomorphs
+N=sum(M,2);                     %number of edges
+Mn=uint64(sum(repmat(10.^(11:-1:0),size(M,1),1).*M,2)); %M as a single number
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makeevenCIJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makeevenCIJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makeevenCIJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makeevenCIJ.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,75 @@
+function  [CIJ] = makeevenCIJ(N,K,sz_cl)
+%MAKEEVENCIJ        Synthetic modular small-world network
+%
+%   CIJ = makeevenCIJ(N,K,sz_cl);
+%
+%   This function generates a random, directed network with a specified 
+%   number of fully connected modules linked together by evenly distributed
+%   remaining random connections.
+%
+%   Inputs:     N,      number of vertices (must be power of 2)
+%               K,      number of edges
+%               sz_cl,  size of clusters (power of 2)
+%
+%   Outputs:    CIJ,    connection matrix
+%
+%   Notes:  N must be a power of 2.
+%           A warning is generated if all modules contain more edges than K.
+%           Cluster size is 2^sz_cl;
+%
+%
+%   Olaf Sporns, Indiana University, 2005/2007
+
+% compute number of hierarchical levels and adjust cluster size
+mx_lvl = floor(log2(N));
+sz_cl = sz_cl-1;
+
+% make a stupid little template
+t = ones(2).*2;
+
+% check N against number of levels
+Nlvl = 2^mx_lvl;
+if (Nlvl~=N) 
+    disp('Warning: N must be a power of 2'); 
+end;
+N = Nlvl;
+
+% create hierarchical template
+for lvl=1:mx_lvl-1
+   CIJ = ones(2^(lvl+1),2^(lvl+1));
+   group1 = 1:size(CIJ,1)/2;
+   group2 = size(CIJ,1)/2+1:size(CIJ,1);
+   CIJ(group1,group1) = t;
+   CIJ(group2,group2) = t;
+   CIJ = CIJ+ones(size(CIJ,1),size(CIJ,1));
+   t = CIJ;
+end;
+s = size(CIJ,1);
+CIJ = CIJ-ones(s,s)-mx_lvl.*eye(s);
+
+% assign connection probabilities
+%CIJp = mx_lvl-CIJ-sz_cl;
+%CIJp = (CIJp>0).*CIJp;
+CIJp = (CIJ>=(mx_lvl-sz_cl));
+
+% determine number of remaining (non-cluster) connections and their
+% possible positions
+%CIJc = (CIJp==0);
+CIJc = (CIJp==1);
+remK = K-nnz(CIJc);
+if (remK<0) 
+    disp('Warning: K is too small, output matrix contains clusters only');
+end;
+[a,b] = find(~(CIJc+eye(N)));
+
+% assign 'remK' randomly distributed connections
+rp = randperm(length(a));
+a = a(rp(1:remK));
+b = b(rp(1:remK));
+for i=1:remK
+   CIJc(a(i),b(i)) = 1;
+end;
+
+% prepare for output
+CIJ = CIJc;
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makefractalCIJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makefractalCIJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makefractalCIJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makefractalCIJ.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,49 @@
+function  [CIJ,K] = makefractalCIJ(mx_lvl,E,sz_cl)
+%MAKEFRACTALCIJ     Synthetic hierarchical modular network
+%
+%   [CIJ,K] = makefractalCIJ(mx_lvl,E,sz_cl);
+%
+%   This function generates a directed network with a hierarchical modular
+%   organization. All modules are fully connected and connection density 
+%   decays as 1/(E^n), with n = index of hierarchical level.
+%
+%   Inputs:     mx_lvl,     number of hierarchical levels, N = 2^mx_lvl
+%               E,          connection density fall-off per level
+%               sz_cl,      size of clusters (power of 2)
+%
+%   Outputs:    CIJ,        connection matrix
+%               K,          number of connections present in the output CIJ
+%
+%
+% Olaf Sporns, Indiana University, 2005/2007
+
+% make a little template
+t = ones(2).*2;
+
+% compute N and cluster size
+N = 2^mx_lvl;
+sz_cl = sz_cl-1;
+
+% n = [0 0 0:mx_lvl-3];
+
+for lvl=1:mx_lvl-1
+   CIJ = ones(2^(lvl+1),2^(lvl+1));
+   group1 = 1:size(CIJ,1)/2;
+   group2 = size(CIJ,1)/2+1:size(CIJ,1);
+   CIJ(group1,group1) = t;
+   CIJ(group2,group2) = t;
+   CIJ = CIJ+ones(size(CIJ,1),size(CIJ,1));
+   t = CIJ;
+end;
+s = size(CIJ,1);
+CIJ = CIJ-ones(s,s)-mx_lvl.*eye(s);
+
+% assign connection probablities
+ee = mx_lvl-CIJ-sz_cl;
+ee = (ee>0).*ee;
+prob = (1./(E.^ee)).*(ones(s,s)-eye(s));
+CIJ = (prob>rand(N));
+
+% count connections
+K = sum(sum(CIJ));
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makelatticeCIJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makelatticeCIJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makelatticeCIJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makelatticeCIJ.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,45 @@
+function [CIJ] = makelatticeCIJ(N,K)
+%MAKELATTICECIJ     Synthetic lattice network
+%
+%   CIJ = makelatticeCIJ(N,K);
+%
+%   This function generates a directed lattice network without toroidal 
+%   boundary counditions (i.e. no ring-like "wrapping around").
+%
+%   Inputs:     N,      number of vertices
+%               K,      number of edges
+%
+%   Outputs:    CIJ,    connection matrix
+%
+%   Note: The lattice is made by placing connections as close as possible 
+%   to the main diagonal, without wrapping around. No connections are made 
+%   on the main diagonal. In/Outdegree is kept approx. constant at K/N.
+%
+%
+%   Olaf Sporns, Indiana University, 2005/2007
+
+% initialize
+CIJ = zeros(N);
+CIJ1 = ones(N);
+KK = 0;
+cnt = 0;
+seq = 1:N-1;
+
+% fill in
+while (KK<K)
+    cnt = cnt + 1;
+    dCIJ = triu(CIJ1,seq(cnt))-triu(CIJ1,seq(cnt)+1);
+    dCIJ = dCIJ+dCIJ';
+    CIJ = CIJ + dCIJ;
+    KK = sum(sum(CIJ));
+end;
+
+% remove excess connections
+overby = KK-K;
+if(overby>0)
+    [i,j] = find(dCIJ);
+    rp = randperm(length(i));
+    for ii=1:overby
+        CIJ(i(rp(ii)),j(rp(ii))) = 0;
+    end;
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_dir.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,24 @@
+function [CIJ] = makerandCIJ_dir(N,K)
+%MAKERANDCIJ_DIR        Synthetic directed random network
+%
+%   CIJ = makerandCIJ_dir(N,K);
+%
+%   This function generates a directed random network
+%
+%   Inputs:     N,      number of vertices
+%               K,      number of edges
+%
+%   Output:     CIJ,    directed random connection matrix
+%
+%   Note: no connections are placed on the main diagonal.
+%
+%
+% Olaf Sporns, Indiana University, 2007/2008
+
+ind = ~eye(N);
+i = find(ind);
+rp = randperm(length(i));
+irp = i(rp);
+
+CIJ = zeros(N);
+CIJ(irp(1:K)) = 1;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJ_und.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,25 @@
+function [CIJ] = makerandCIJ_und(N,K)
+%MAKERANDCIJ_UND        Synthetic directed random network
+%
+%   CIJ = makerandCIJ_und(N,K);
+%
+%   This function generates an undirected random network
+%
+%   Inputs:     N,      number of vertices
+%               K,      number of edges
+%
+%   Output:     CIJ,    undirected random connection matrix
+%
+%   Note: no connections are placed on the main diagonal.
+%
+%
+% Olaf Sporns, Indiana University, 2007/2008
+
+ind = triu(~eye(N));
+i = find(ind);
+rp = randperm(length(i));
+irp = i(rp);
+
+CIJ = zeros(N);
+CIJ(irp(1:K)) = 1;
+CIJ = CIJ+CIJ';         % symmetrize
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJdegreesfixed.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJdegreesfixed.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJdegreesfixed.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makerandCIJdegreesfixed.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,81 @@
+function [cij,flag] = makerandCIJdegreesfixed(in,out)
+%MAKERANDCIJDEGREESFIXED        Synthetic directed random network
+%
+%   CIJ = makerandCIJdegreesfixed(N,K);
+%
+%   This function generates a directed random network with a specified 
+%   in-degree and out-degree sequence. The function returns a flag, 
+%   denoting whether the algorithm succeeded or failed.
+%
+%   Inputs:     in,     indegree vector
+%               out,    outdegree vector
+%
+%   Output:     CIJ,    binary directed connectivity matrix
+%               flag,   flag=1 if the algorithm succeeded; flag=0 otherwise
+%
+%
+%   Notes:  Necessary conditions include:
+%               length(in) = length(out) = n
+%               sum(in) = sum(out) = k
+%               in(i), out(i) < n-1
+%               in(i) + out(j) < n+2
+%               in(i) + out(i) < n
+%
+%           No connections are placed on the main diagonal
+%
+%
+% Aviad Rubinstein, Indiana University 2005/2007
+
+% intialize
+n = length(in);
+k = sum(in);
+inInv = zeros(k,1);
+outInv = inInv;
+iIn = 1; iOut = 1;
+
+for i = 1:n
+    inInv(iIn:iIn+in(i) - 1) = i;
+    outInv(iOut:iOut+out(i) - 1) = i;
+    iIn = iIn+in(i);
+    iOut = iOut+out(i);
+end
+
+cij = eye(n);
+edges = [outInv(1:k)'; inInv(randperm(k))'];
+
+% create cij, and check for double edges and self-connections
+for i = 1:k
+    if cij(edges(1,i),edges(2,i))
+        warningCounter = 1;
+        while (1)
+            switchTo = ceil(k*rand);
+            if ~(cij(edges(1,i),edges(2,switchTo)) || cij(edges(1,switchTo),edges(2,i)))
+                cij(edges(1,i),edges(2,switchTo)) = 1;
+                if switchTo < i
+                    cij(edges(1,switchTo),edges(2,switchTo)) = 0;
+                    cij(edges(1,switchTo),edges(2,i)) = 1;
+                end
+                temp = edges(2,i);
+                edges(2,i) = edges(2,switchTo);
+                edges(2,switchTo) = temp;
+                break
+            end
+            warningCounter = warningCounter+1;
+            % If there is a legitimate subtitution, it has a probability of 1/k of being done.
+            % Thus it is highly unlikely that it will not be done after 2*k^2 attempts.
+            % This is an indication that the given indegree / outdegree
+            % vectors may not be possible.
+            if warningCounter == 2*k^2
+                flag = 0;  % no valid solution found
+                return;
+            end
+        end
+    else
+        cij(edges(1,i),edges(2,i)) = 1;
+    end
+end
+
+cij = cij - eye(n);
+
+% a valid solution was found
+flag = 1;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makeringlatticeCIJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makeringlatticeCIJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/makeringlatticeCIJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/makeringlatticeCIJ.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,47 @@
+function [CIJ] = makeringlatticeCIJ(N,K)
+%MAKERINGLATTICECIJ     Synthetic lattice network
+%
+%   CIJ = makeringlatticeCIJ(N,K);
+%
+%   This function generates a directed lattice network with toroidal 
+%   boundary counditions (i.e. with ring-like "wrapping around").
+%
+%   Inputs:     N,      number of vertices
+%               K,      number of edges
+%
+%   Outputs:    CIJ,    connection matrix
+%
+%   Note: The lattice is made by placing connections as close as possible 
+%   to the main diagonal, with wrapping around. No connections are made 
+%   on the main diagonal. In/Outdegree is kept approx. constant at K/N.
+%
+%
+%   Olaf Sporns, Indiana University, 2005/2007
+
+% initialize
+CIJ = zeros(N);
+CIJ1 = ones(N);
+KK = 0;
+cnt = 0;
+seq = 1:N-1;
+seq2 = N-1:-1:1;
+
+% fill in
+while (KK<K)
+    cnt = cnt + 1;
+    dCIJ = triu(CIJ1,seq(cnt))-triu(CIJ1,seq(cnt)+1);
+    dCIJ2 = triu(CIJ1,seq2(cnt))-triu(CIJ1,seq2(cnt)+1);
+    dCIJ = dCIJ+dCIJ'+dCIJ2+dCIJ2';
+    CIJ = CIJ + dCIJ;
+    KK = sum(sum(CIJ));
+end;
+
+% remove excess connections
+overby = KK-K;
+if(overby>0)
+    [i,j] = find(dCIJ);
+    rp = randperm(length(i));
+    for ii=1:overby
+        CIJ(i(rp(ii)),j(rp(ii))) = 0;
+    end;
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/maketoeplitzCIJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/maketoeplitzCIJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/maketoeplitzCIJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/maketoeplitzCIJ.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,29 @@
+function  [CIJ] = maketoeplitzCIJ(N,K,s)
+%MAKETOEPLITZCIJ    A synthetic directed network with Gaussian drop-off of
+%                   connectivity with distance
+%
+%   CIJ = maketoeprandCIJ(N,K,s)
+%
+%   This function generates a directed network with a Gaussian drop-off in
+%   edge density with increasing distance from the main diagonal. There are
+%   toroidal boundary counditions (i.e. no ring-like "wrapping around").
+%
+%   Inputs:     N,      number of vertices
+%               K,      number of edges
+%               s,      standard deviation of toeplitz
+%
+%   Output:     CIJ,    connection matrix
+%
+%   Note: no connections are placed on the main diagonal.
+%
+%
+% Olaf Sporns, Indiana University, 2005/2007
+
+profile = normpdf(1:N-1,0.5,s);
+template = toeplitz([0 profile],[0 profile]);
+template = template.*(K./sum(sum(template)));
+CIJ = zeros(N);
+
+while ((sum(sum(CIJ)) ~= K))
+   CIJ = (rand(N)<template);
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,80 @@
+function [Min,Mout,Mall] = matching_ind(CIJ)
+%MATCHING_IND       Matching index
+%
+%   [Min,Mout,Mall] = matching_ind(CIJ);
+%
+%   For any two nodes u and v, the matching index computes the amount of
+%   overlap in the connection patterns of u and v. Self-connections and
+%   u-v connections are ignored. The matching index is a symmetric 
+%   quantity, similar to a correlation or a dot product.
+%
+%   Input:      CIJ,    connection/adjacency matrix
+%
+%   Output:     Min,    matching index for incoming connections
+%               Mout,   matching index for outgoing connections
+%               Mall,   matching index for all connections
+%
+%   Notes:
+%       Does not use self- or cross connections for comparison.
+%       Does not use connections that are not present in BOTH u and v.
+%       All output matrices are calculated for upper triangular only.
+%
+%
+% Olaf Sporns, Indiana University, 2002/2007/2008
+
+N = size(CIJ,1);
+
+% compare incoming connections only
+Min = zeros(N,N);
+for i=1:N-1
+    for j=i+1:N
+        c1 = CIJ(:,i);
+        c2 = CIJ(:,j);
+        use = ~(~c1&~c2);
+        use(i) = 0;
+        use(j) = 0;
+        ncon = sum(c1(use))+sum(c2(use));
+        if (ncon==0)
+            Min(i,j) = 0;
+        else
+            Min(i,j) = 2*(sum(c1(use)&c2(use))/ncon);
+        end;
+    end;
+end;
+
+% compare outgoing connections only
+Mout = zeros(N,N);
+for i=1:N-1
+    for j=i+1:N
+        c1 = CIJ(i,:);
+        c2 = CIJ(j,:);
+        use = ~(~c1&~c2);
+        use(i) = 0;
+        use(j) = 0;
+        ncon = sum(c1(use))+sum(c2(use));
+        if (ncon==0)
+            Mout(i,j) = 0;
+        else
+            Mout(i,j) = 2*(sum(c1(use)&c2(use))/ncon);
+        end;
+    end;
+end;
+
+% compare all (incoming+outgoing) connections
+Mall = zeros(N,N);
+for i=1:N-1
+    for j=i+1:N
+        c1 = [CIJ(:,i)' CIJ(i,:)];
+        c2 = [CIJ(:,j)' CIJ(j,:)];
+        use = ~(~c1&~c2);
+        use(i) = 0;  use(i+N) = 0;
+        use(j) = 0;  use(j+N) = 0;
+        ncon = sum(c1(use))+sum(c2(use));
+        if (ncon==0)
+            Mall(i,j) = 0;
+        else
+            Mall(i,j) = 2*(sum(c1(use)&c2(use))/ncon);
+        end;
+    end;
+end;
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/matching_ind_und.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,39 @@
+function M0 = matching_ind_und(CIJ)
+%MATCHING_IND_UND       matching index
+%
+%   M0 = MATCHING_IND_UND(CIJ) computes matching index for undirected
+%   graph specified by adjacency matrix CIJ. Matching index is a measure of
+%   similarity between two nodes' connectivity profiles (excluding their
+%   mutual connection, should it exist).
+%
+%   Inputs:     CIJ,    undirected adjacency matrix
+%
+%   Outputs:    M0,     matching index matrix.
+%
+%   Richard Betzel, Indiana University, 2013
+%
+CIJ0 = CIJ;
+K = sum(CIJ0);
+R = K ~= 0;
+N = sum(R);
+CIJ = CIJ0(R,R);
+I = ~eye(N);
+M = zeros(N,N);
+for i = 1:N
+    
+    c1 = CIJ(i,:);
+    use = bsxfun(@or,c1,CIJ);
+    use(:,i) = 0;
+    use = use.*I;
+    
+    ncon1 = bsxfun(@times,use,c1);
+    ncon2 = bsxfun(@times,use,CIJ);
+    ncon = sum(ncon1 + ncon2,2);
+    
+    M(:,i) = 2*sum(ncon1 & ncon2,2)./ncon;
+    
+end
+M = M.*I;
+M(isnan(M)) = 0;
+M0 = zeros(size(CIJ0));
+M0(R,R) = M;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/mean_first_passage_time.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/mean_first_passage_time.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/mean_first_passage_time.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/mean_first_passage_time.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,45 @@
+function MFPT = mean_first_passage_time(adj)
+% MEAN_FIRST_PASSAGE_TIME           Mean first passage time
+%
+%   MFPT = mean_first_passage_time(adj)
+%
+%   The first passage time (MFPT) from i to j is the expected number of
+%   steps it takes a random walker starting at node i to arrive for the
+%   first time at node j. The mean first passage time is not a
+%   symmetric measure: mfpt(i,j) may be different from mfpt(j,i).  
+% 
+%   Input:
+%       adj,    Weighted/Unweighted, directed/undirected adjacency matrix 
+%
+%   Output:
+%       MFPT, 	Pairwise mean first passage time matrix.   
+%
+%
+%   References: Goi J, et al (2013) PLoS ONE
+%               
+%   Joaquin Goi, IU Bloomington, 2012
+
+
+P = diag(sum(adj,2))\adj; % matrix of transition probabilities
+
+tol=10^(-3); %tolerance to find value of 1 at the eigenvector
+
+n = length(P); %number of nodes
+[V,D_eigen] = eig(P');  %diagonal matrix D_eigen of eigenvalues. Full matrix V whose columns are the corresponding eigenvectors so that X*V = V*D. In our case X=P';
+
+aux = abs(diag(D_eigen)-1);
+index = find(aux==min(aux));
+if aux(index)>tol
+    error('cannot find eigenvalue of 1. Minimum eigenvalue value is %0.6f. Tolerance was set at %0.6f',aux(index)+1,tol);
+end
+
+w = V(:,index)'; %left-eigen vector associated to eigenvalue of 1.
+w = w/sum(w); %rescale of left-eigen vector to the sum of it (hence is now in probabilites form. The inverse of this vector is the return-times vector
+
+W = repmat(w,n,1); %convert column-vector w to a full matrix W by making copies of w.
+I = eye(n,n); %Identity matrix I is computed
+
+Z = inv(I-P+W); %Fundamental matrix Z is computed
+
+MFPT = (repmat(diag(Z)',n,1)-Z)./W;  % this performs MFPT(i,j)=(Z(j,j)-Z(i,j))/w(j) in a matricial way. Corresponds to theorem 11.16 pag. 459
+% r = 1./w; %as demostrated in theorem 11.15 pag. 455. Each entry r_i is the 'mean-recurrence' or 'return-time' of state i (node i when states represent nodes of a graph)
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/mleme_constraint_model.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/mleme_constraint_model.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/mleme_constraint_model.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/mleme_constraint_model.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,284 @@
+function [W0, E0, P0, Delt0] = mleme_constraint_model(samp, W, M, Lo, Li, Lm, opts)
+%MLEME_CONSTRAINT_MODEL     Unbiased sampling of networks with soft constraints
+%
+%   W0 = mleme_constraint_model(samp, W);
+%   W0 = mleme_constraint_model(samp, W, M);
+%   W0 = mleme_constraint_model(samp, W, M, Lo, Li, Lm);
+%   [W0, E0, P0, Delt0] = mleme_constraint_model(samp, W, M, Lo, Li, Lm, opts);
+%
+%   This function returns an ensemble of unbiasedly sampled networks with
+%   weighted node-strength and module-weight constraints. These constraints
+%   are soft in that they are satisfied on average for the full network
+%   ensemble but not, in general, for each individual network.
+%
+%   Inputs (for a network with n nodes, m modules and c constraints):
+%
+%       samp,   Number of networks to sample.
+%
+%       W,      (length n) square directed and weighted connectivity
+%               matrix. All weights must be nonnegative integers. Note that
+%               real-valued weights may be converted to integers with
+%               arbitrary precision through rescaling and rounding, e.g.
+%               W_int = round(10^precision * W_real).
+%
+%       M,      (length n) module affiliation vector. This vector is often
+%               obtained as the output of a community detection algorithm.
+%               The vector must contain nonnegative integers, with zeros
+%               specifying nodes which are not part of any community. This
+%               input may be left empty if there are no module constraints.
+%
+%       Lo,     (length n) out-strength constraint logical vector. This
+%               vector specifies out-strength constraints for each node.
+%               Alternatively, it is possible to specify 1 to constrain all
+%               out-strengths or 0 for no constraints. Empty or no input
+%               results in default behavour (no constraints).
+%
+%       Lo,     (length n) in-strength constraint logical vector. This
+%               vector specifies in-strength constraints for each node.
+%               Alternatively, it is possible to specify 1 to constrain all
+%               in-strengths or 0 for no constraints. Empty or no input
+%               results in default behavour (no constraints).
+%
+%       Lm,     (length m) module-weight constraint logical matrix. This
+%               matrix specifies module-weight constraints for all pairs of
+%               modules. Alternatively, it is possible to specify
+%               2 to constrain all inter-module and intra-module weights,
+%               1 to constrain all intra-module weights, or 0  for no
+%               constraints. Empty or no input results in default behavour
+%               (no constraints).
+%
+%       opts,   optional argument: pass optimization and display options with optimset.
+%               Default: optimset('MaxFunEvals', 1e6*c, 'MaxIter', 1e6, 'Display', 'iter');
+%
+%
+%   Outputs:
+%       W0,     an ensemble of sampled networks with constraints.
+%
+%       E0,     expected weights matrix.
+%
+%       P0,     probability matrix.
+%
+%       Delt0,  algorithm convergence error.
+%
+%
+%   Algorithm:
+%               Maximum-likelihood estimation of network probability
+%               distribution by numerical solution of systems of nonlinear
+%               equations, and sampling of individual networks directly
+%               from this distribution.
+%
+%
+%   Notes:
+%               Empirical connection weights are
+%               not preserved. Constraint errors are guaranteed to vanish
+%               in the limit of the full network ensemble.
+%
+%
+%   Examples:
+%               % get community structure of a weighted network W
+%               M = community_louvain(W, 2);
+%
+%               % specify node and module constraints
+%               n = length(W);              % number of nodes
+%               m = max(M);                 % number of modules
+%               Lo = true(n, 1);            % out-strength constraints
+%               Li = true(n, 1);            % in-strength constraints
+%               Lm = eye(m);                % module-weight constraints
+%
+%               % sample networks with the above constraints
+%               [W0, E0, P0, Delt0] = mleme_constraint_model(samp, W, M, Lo, Li, Lm);
+%
+%               % equivalent formulation
+%               [W0, E0, P0, Delt0] = mleme_constraint_model(samp, W, M, 1, 1, 1);
+%
+%               % alternative: sample networks with average weight constraints only
+%               [W0, E0, P0, Delt0] = mleme_constraint_model(samp, W);
+%
+%
+%   References: Squartini and Garlaschelli (2011) New J Phys 13:083001
+%               Rubinov (2016) Nat Commun 7:13812
+%
+%
+%   2016, Mika Rubinov, Janelia HHMI
+
+%   Modification History
+%   Dec 2016: Original.
+
+n = length(W);                  % number of nodes
+
+if ~exist('M', 'var') || isempty(M)
+    if exist('Lm', 'var') && any(Lm)
+        error('Need module affiliation vector for module constraints')
+    else
+        M = zeros(n, 1);
+    end
+end
+
+m = max(M);                     % number of modules
+
+if ~isequal(W, int64(W)) || min(W(:))<0
+    error('W must only contain nonnegative integers.')
+end
+if ~isequal(M, int64(M)) || min(M(:))<0
+    error('M must only contain nonnegative integers.')
+end
+
+% process node constraints
+if ~exist('Lo','var') || isempty(Lo) || isequal(Lo,0)
+    Lo = false(n, 1);
+elseif isequal(Lo, 1)
+    Lo = true(n, 1);
+end
+if ~exist('Li','var')
+    Li = Lo;
+elseif isempty(Li) || isequal(Li, 0)
+    Li = false(n, 1);
+elseif isequal(Li, 1)
+    Li = true(n, 1);
+end
+
+% process module constraints
+if ~exist('Lm','var') || isempty(Lm) || isequal(Lm,0)
+    Lm = false(m);
+elseif isequal(Lm, 2)
+    Lm = true(m);
+elseif isequal(Lm, 1)
+    Lm = diag(true(m, 1));
+end
+if any(~M)
+    m = m + 1;
+    M(~M) = m;
+    Lm(m, m) = 0;     % add a new row and column for nodes without modules
+end
+
+Lo = logical(Lo(:));
+Li = logical(Li(:));
+Lm = logical(Lm(:));
+ao = numel(Lo);
+ai = numel(Li);
+am = numel(Lm);
+uo = nnz(Lo);
+ui = nnz(Li);
+um = nnz(Lm);
+Mij = bsxfun(@plus, M, (M.'-1)*m);
+
+f_ex = @(V) system_equations(V, Mij, Lo, Li, Lm, ao, ai, am, uo, ui, um);
+f_cx = @(W) system_constraints(W, M, Lo, Li, Lm, uo, ui, um);
+
+C = f_cx(W);
+c = 1 + uo + ui + um;
+if ~exist('V','var')
+    V = mean2(W)/(1+mean2(W))*ones(c,1);
+end
+
+assert(c == numel(C));
+assert(c == numel(V));
+
+if ~exist('opts', 'var') || isempty(opts)
+    opts = optimset('MaxFunEvals', 1e6*c, 'MaxIter', 1e6, 'Display', 'iter');
+end
+
+V0 = fsolve(@(V) C - f_cx(f_ex(V)), V, opts);
+
+[E0, P0] = f_ex(V0);
+Delt0 = C - f_cx(f_ex(V0));
+
+W0 = sample_networks(P0, samp);
+
+end
+
+
+function CellW0 = sample_networks(P0, samp)
+
+if ~exist('samp', 'var')
+    samp = 1;
+end
+
+n = length(P0);
+
+CellW0 = cell(samp, 1);
+for i = 1:samp
+    W0 = zeros(n);
+    L0 = ~eye(n);
+    l0 = nnz(L0);
+    while l0
+        L0(L0) = P0(L0) > rand(l0,1);
+        W0(L0) = W0(L0) + 1;
+        l0 = nnz(L0);
+    end
+    CellW0{i} = W0;
+end
+
+end
+
+
+function [W, P] = system_equations(V, Mij, Lo, Li, Lm, ao, ai, am, uo, ui, um)
+
+X = ones(ao, 1);
+Y = ones(ai, 1);
+Z = ones(am, 1);
+
+if uo
+    offset = 1;
+    X(Lo) = V(offset + (1:uo));
+end
+if ui
+    offset = 1 + uo;
+    Y(Li) = V(offset + (1:ui));
+end
+if um
+    offset = 1 + uo + ui;
+    Z(Lm) = V(offset + (1:um));
+end
+
+P = V(1) .* (X * Y.') .* Z(Mij);            % V(1) is the total weight
+P(P>1) = 1 - eps;
+
+W = P ./ (1 - P);
+W(1:length(W)+1:end) = 0;
+
+end
+
+
+function C = system_constraints(W, M, Lo, Li, Lm, uo, ui, um)
+
+if nargin == 0
+    C = @block_density;
+    return;
+end
+
+if uo
+    So = sum(W(Lo,:), 2);
+else
+    So = [];
+end
+if ui
+    Si = sum(W(:,Li), 1).';
+else
+    Si = [];
+end
+if um
+    Wm = block_density(W, M, Lm);
+else
+    Wm = [];
+end
+
+C = [sum(sum(W)); So; Si; Wm];
+
+end
+
+
+function Wm = block_density(W, M, Lwm)
+
+m = max(M);
+
+Wm = zeros(m*m, 1);
+for u = 1:m
+    for v = 1:m
+        Wm(u + (v-1)*m) = sum(sum(W(M==u, M==v)));
+    end
+end
+
+Wm = Wm(Lwm);
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_dir.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,124 @@
+function [Ci,Q]=modularity_dir(A,gamma)
+%MODULARITY_DIR     Optimal community structure and modularity
+%
+%   Ci = modularity_dir(W);
+%   [Ci Q] = modularity_dir(W);
+%
+%   The optimal community structure is a subdivision of the network into
+%   nonoverlapping groups of nodes in a way that maximizes the number of
+%   within-group edges, and minimizes the number of between-group edges. 
+%   The modularity is a statistic that quantifies the degree to which the
+%   network may be subdivided into such clearly delineated groups. 
+%
+%   Inputs:
+%       W,
+%           directed weighted/binary connection matrix
+%       gamma,
+%           resolution parameter (optional)
+%               gamma>1,        detects smaller modules
+%               0<=gamma<1,     detects larger modules
+%               gamma=1,        classic modularity (default)
+%
+%   Outputs:    
+%       Ci,     optimal community structure
+%       Q,      maximized modularity
+%
+%   Note:
+%       This algorithm is essentially deterministic. The only potential
+%       source of stochasticity occurs at the iterative finetuning step, in
+%       the presence of non-unique optimal swaps. However, the present
+%       implementation always makes the first available optimal swap and
+%       is therefore deterministic.
+%
+%   References:
+%       Leicht and Newman (2008) Phys Rev Lett 100:118703.
+%       Reichardt and Bornholdt (2006) Phys Rev E 74:016110.
+%
+%   2008-2016
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Dani Bassett, UCSB
+%   Xindi Wang, Beijing Normal University
+%   Roan LaPlante, Martinos Center for Biomedical Imaging
+
+%   Modification History:
+%   Jul 2008: Original (Mika Rubinov)
+%   Oct 2008: Positive eigenvalues made insufficient for division (Jonathan Power)
+%   Dec 2008: Fine-tuning made consistent with Newman's description (Jonathan Power)
+%   Dec 2008: Fine-tuning vectorized (Mika Rubinov)
+%   Sep 2010: Node identities permuted (Dani Bassett)
+%   Dec 2013: Gamma resolution parameter included (Mika Rubinov)
+%   Dec 2013: Detection of maximum real part of eigenvalues enforced (Mika Rubinov)
+%               Thanks to Mason Porter and Jack Setford, University of Oxford
+%   Dec 2015: Single moves during fine-tuning enforced (Xindi Wang)
+%   Jan 2017: Removed node permutation and updated documentation (Roan LaPlante)
+
+if ~exist('gamma','var')
+    gamma = 1;
+end
+
+N=length(A);                            %number of vertices
+% n_perm = randperm(N);                   %DB: randomly permute order of nodes
+% A = A(n_perm,n_perm);                   %DB: use permuted matrix for subsequent analysis
+Ki=sum(A,1);                            %in-degree
+Ko=sum(A,2);                            %out-degree
+m=sum(Ki);                           	%number of edges
+b=A-gamma*(Ko*Ki).'/m;
+B=b+b.';                            	%directed modularity matrix
+Ci=ones(N,1);                           %community indices
+cn=1;                                   %number of communities
+U=[1 0];                                %array of unexamined communites
+
+ind=1:N;
+Bg=B;
+Ng=N;
+
+while U(1)                              %examine community U(1)
+    [V,D]=eig(Bg);
+    [~,i1]=max(real(diag(D)));         %maximal positive (real part of) eigenvalue of Bg
+    v1=V(:,i1);                         %corresponding eigenvector
+
+    S=ones(Ng,1);
+    S(v1<0)=-1;
+    q=S.'*Bg*S;                         %contribution to modularity
+
+    if q>1e-10                       	%contribution positive: U(1) is divisible
+        qmax=q;                         %maximal contribution to modularity
+        Bg(logical(eye(Ng)))=0;      	%Bg is modified, to enable fine-tuning
+        indg=ones(Ng,1);                %array of unmoved indices
+        Sit=S;
+        while any(indg)                 %iterative fine-tuning
+            Qit=qmax-4*Sit.*(Bg*Sit); 	%this line is equivalent to:
+            [qmax,imax]=max(Qit.*indg); %for i=1:Ng
+            Sit(imax)=-Sit(imax);       %	Sit(i)=-Sit(i);
+            indg(imax)=nan;             %	Qit(i)=Sit.'*Bg*Sit;
+            if qmax>q                   %	Sit(i)=-Sit(i);
+                q=qmax;                 %end
+                S=Sit;
+            end
+        end
+
+        if abs(sum(S))==Ng              %unsuccessful splitting of U(1)
+            U(1)=[];
+        else
+            cn=cn+1;
+            Ci(ind(S==1))=U(1);         %split old U(1) into new U(1) and into cn
+            Ci(ind(S==-1))=cn;
+            U=[cn U];                   %#ok<AGROW>
+        end
+    else                                %contribution nonpositive: U(1) is indivisible
+        U(1)=[];
+    end
+
+    ind=find(Ci==U(1));                 %indices of unexamined community U(1)
+    bg=B(ind,ind);
+    Bg=bg-diag(sum(bg));                %modularity matrix for U(1)
+    Ng=length(ind);                     %number of vertices in U(1)
+end
+
+s=Ci(:,ones(1,N));                      %compute modularity
+Q=~(s-s.').*B/(2*m);
+Q=sum(Q(:));
+% Ci_corrected = zeros(N,1);              % DB: initialize Ci_corrected
+% Ci_corrected(n_perm) = Ci;              % DB: return order of nodes to the order used at the input stage.
+% Ci = Ci_corrected;                      % DB: output corrected community assignments
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/modularity_und.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,122 @@
+function [Ci,Q]=modularity_und(A,gamma)
+%MODULARITY_UND     Optimal community structure and modularity
+%
+%   Ci = modularity_und(W);
+%   [Ci Q] = modularity_und(W,gamma);
+%
+%   The optimal community structure is a subdivision of the network into
+%   nonoverlapping groups of nodes in a way that maximizes the number of
+%   within-group edges, and minimizes the number of between-group edges.
+%   The modularity is a statistic that quantifies the degree to which the
+%   network may be subdivided into such clearly delineated groups.
+%
+%   Inputs:
+%       W,
+%           undirected weighted/binary connection matrix
+%       gamma,
+%           resolution parameter (optional)
+%               gamma>1,        detects smaller modules
+%               0<=gamma<1,     detects larger modules
+%               gamma=1,        classic modularity (default)
+%
+%   Outputs:    
+%       Ci,     optimal community structure
+%       Q,      maximized modularity
+%
+%   Note:
+%       This algorithm is essentially deterministic. The only potential
+%       source of stochasticity occurs at the iterative finetuning step, in
+%       the presence of non-unique optimal swaps. However, the present
+%       implementation always makes the first available optimal swap and
+%       is therefore deterministic.
+%
+%   References: 
+%       Newman (2006) -- Phys Rev E 74:036104, PNAS 23:8577-8582.
+%       Reichardt and Bornholdt (2006) Phys Rev E 74:016110.
+%
+%   2008-2016
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Dani Bassett, UCSB
+%   Xindi Wang, Beijing Normal University
+%   Roan LaPlante, Martinos Center for Biomedical Imaging
+
+%   Modification History:
+%   Jul 2008: Original (Mika Rubinov)
+%   Oct 2008: Positive eigenvalues made insufficient for division (Jonathan Power)
+%   Dec 2008: Fine-tuning made consistent with Newman's description (Jonathan Power)
+%   Dec 2008: Fine-tuning vectorized (Mika Rubinov)
+%   Sep 2010: Node identities permuted (Dani Bassett)
+%   Dec 2013: Gamma resolution parameter included (Mika Rubinov)
+%   Dec 2013: Detection of maximum real part of eigenvalues enforced (Mika Rubinov)
+%               Thanks to Mason Porter and Jack Setford, University of Oxford
+%   Dec 2015: Single moves during fine-tuning enforced (Xindi Wang)
+%   Jan 2017: Removed node permutation and updated documentation (Roan LaPlante)
+
+if ~exist('gamma','var')
+    gamma = 1;
+end
+
+N=length(A);                            %number of vertices
+% n_perm = randperm(N);                   %DB: randomly permute order of nodes
+% A = A(n_perm,n_perm);                   %DB: use permuted matrix for subsequent analysis
+K=sum(A);                               %degree
+m=sum(K);                               %number of edges (each undirected edge is counted twice)
+B=A-gamma*(K.'*K)/m;                    %modularity matrix
+Ci=ones(N,1);                           %community indices
+cn=1;                                   %number of communities
+U=[1 0];                                %array of unexamined communites
+
+ind=1:N;
+Bg=B;
+Ng=N;
+
+while U(1)                              %examine community U(1)
+    [V,D]=eig(Bg);
+    [~,i1]=max(real(diag(D)));          %maximal positive (real part of) eigenvalue of Bg
+    v1=V(:,i1);                         %corresponding eigenvector
+
+    S=ones(Ng,1);
+    S(v1<0)=-1;
+    q=S.'*Bg*S;                         %contribution to modularity
+
+    if q>1e-10                       	%contribution positive: U(1) is divisible
+        qmax=q;                         %maximal contribution to modularity
+        Bg(logical(eye(Ng)))=0;      	%Bg is modified, to enable fine-tuning
+        indg=ones(Ng,1);                %array of unmoved indices
+        Sit=S;
+        while any(indg)                 %iterative fine-tuning
+            Qit=qmax-4*Sit.*(Bg*Sit); 	%this line is equivalent to:
+            [qmax,imax]=max(Qit.*indg); %for i=1:Ng
+            Sit(imax)=-Sit(imax);       %	Sit(i)=-Sit(i);
+            indg(imax)=nan;             %	Qit(i)=Sit.'*Bg*Sit;
+            if qmax>q                   %	Sit(i)=-Sit(i);
+                q=qmax;                 %end
+                S=Sit;
+            end
+        end
+
+        if abs(sum(S))==Ng              %unsuccessful splitting of U(1)
+            U(1)=[];
+        else
+            cn=cn+1;
+            Ci(ind(S==1))=U(1);         %split old U(1) into new U(1) and into cn
+            Ci(ind(S==-1))=cn;
+            U=[cn U];                   %#ok<AGROW>
+        end
+    else                                %contribution nonpositive: U(1) is indivisible
+        U(1)=[];
+    end
+
+    ind=find(Ci==U(1));                 %indices of unexamined community U(1)
+    bg=B(ind,ind);
+    Bg=bg-diag(sum(bg));                %modularity matrix for U(1)
+    Ng=length(ind);                     %number of vertices in U(1)
+end
+
+s=Ci(:,ones(1,N));                      %compute modularity
+Q=~(s-s.').*B/m;
+Q=sum(Q(:));
+% Ci_corrected = zeros(N,1);              % DB: initialize Ci_corrected
+% Ci_corrected(n_perm) = Ci;              % DB: return order of nodes to the order used at the input stage.
+% Ci = Ci_corrected;                      % DB: output corrected community assignments
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/module_degree_zscore.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/module_degree_zscore.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/module_degree_zscore.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/module_degree_zscore.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,41 @@
+function Z=module_degree_zscore(W,Ci,flag)
+%MODULE_DEGREE_ZSCORE       Within-module degree z-score
+%
+%   Z=module_degree_zscore(W,Ci,flag);
+%
+%   The within-module degree z-score is a within-module version of degree
+%   centrality.
+%
+%   Inputs:     W,      binary/weighted, directed/undirected connection matrix
+%               Ci,     community affiliation vector
+%               flag,   0, undirected graph (default)
+%                       1, directed graph: out-degree
+%                       2, directed graph: in-degree
+%                       3, directed graph: out-degree and in-degree
+%
+%   Output:     Z,      within-module degree z-score.
+%
+%   Reference: Guimera R, Amaral L. Nature (2005) 433:895-900.
+%
+%
+%   Mika Rubinov, UNSW, 2008-2010
+
+if ~exist('flag','var')
+    flag=0;
+end
+
+switch flag
+    case 0  % no action required
+    case 1  % no action required
+    case 2; W=W.';
+    case 3; W=W+W.';
+end
+
+n=length(W);                        %number of vertices
+Z=zeros(n,1);
+for i=1:max(Ci)
+    Koi=sum(W(Ci==i,Ci==i),2);
+    Z(Ci==i)=(Koi-mean(Koi))./std(Koi);
+end
+
+Z(isnan(Z))=0;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_bin.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,78 @@
+function [f,F]=motif3funct_bin(A)
+%MOTIF3FUNCT_BIN       Frequency of functional class-3 motifs
+%
+%   [f,F] = motif3funct_bin(A);
+%
+%   *Structural motifs* are patterns of local connectivity in complex
+%   networks. In contrast, *functional motifs* are all possible subsets of
+%   patterns of local connectivity embedded within structural motifs. Such
+%   patterns are particularly diverse in directed networks. The motif
+%   frequency of occurrence around an individual node is known as the motif
+%   fingerprint of that node. The total motif frequency of occurrence in
+%   the whole network is correspondingly known as the motif fingerprint of
+%   the network.
+%
+%   Input:      A,      binary directed connection matrix
+%
+%   Output:     F,      node motif frequency fingerprint
+%               f,      network motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. There is a source of possible confusion in motif terminology.
+%          Motifs ("structural" and "functional") are most frequently
+%          considered only in the context of anatomical brain networks
+%          (Sporns and Ktter, 2004). On the other hand, motifs are not
+%          commonly studied in undirected networks, due to the paucity of
+%          local undirected connectivity patterns.
+%
+%   References: Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M3 ID3 N3
+if isempty(N3)
+    load motif34lib M3 ID3 N3            	%load motif data
+end
+
+n=length(A);                                %number of vertices in A
+f=zeros(13,1);                              %motif count for whole graph
+F=zeros(13,n);                          	%frequency
+
+A=1*(A~=0);                                 %adjacency matrix
+As=A|A.';                                   %symmetrized adjacency
+
+for u=1:n-2                               	%loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];         	%v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];       %v2: all neibs of v1 (>u)
+        V2(V1)=0;                           %not already in V1
+        V2=([false(1,v1) As(u,v1+1:n)])|V2; %and all neibs of u (>v1)
+        for v2=find(V2)
+            a=[A(v1,u);A(v2,u);A(u,v1);A(v2,v1);A(u,v2);A(v1,v2)];
+            ind=(M3*a)==N3;                 %find all contained isomorphs
+            id=ID3(ind);
+
+            [idu,j]=unique(id);             %unique motif occurences
+            j=[0;j];                        %#ok<AGROW>
+            mu=length(idu);                 %number of unique motifs
+            f2=zeros(mu,1);
+
+            for h=1:mu                      %for each unique motif
+                f2(h)=j(h+1)-j(h);              %and frequencies
+            end
+
+            %then add to cumulative count
+            f(idu)=f(idu)+f2;
+            if nargout==2
+                F(idu,[u v1 v2])=F(idu,[u v1 v2])+[f2 f2 f2];
+            end
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3funct_wei.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,101 @@
+function [I,Q,F]=motif3funct_wei(W)
+%MOTIF3FUNCT_WEI       Intensity and coherence of functional class-3 motifs
+%
+%   [I,Q,F] = motif3funct_wei(W);
+%
+%   *Structural motifs* are patterns of local connectivity in complex
+%   networks. In contrast, *functional motifs* are all possible subsets of
+%   patterns of local connectivity embedded within structural motifs. Such
+%   patterns are particularly diverse in directed networks. The motif
+%   frequency of occurrence around an individual node is known as the motif
+%   fingerprint of that node. The motif intensity and coherence are
+%   weighted generalizations of the motif frequency. The motif
+%   intensity is equivalent to the geometric mean of weights of links
+%   comprising each motif. The motif coherence is equivalent to the ratio
+%   of geometric and arithmetic means of weights of links comprising each
+%   motif.  
+%
+%   Input:      W,      weighted directed connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     I,      node motif intensity fingerprint
+%               Q,      node motif coherence fingerprint
+%               F,      node motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. Average intensity and coherence are given by I./F and Q./F
+%       3. All weights must be between 0 and 1. This may be achieved using
+%          the weight_conversion.m function, as follows: 
+%          W_nrm = weight_conversion(W, 'normalize');
+%   	4. There is a source of possible confusion in motif terminology.
+%          Motifs ("structural" and "functional") are most frequently
+%          considered only in the context of anatomical brain networks
+%          (Sporns and Ktter, 2004). On the other hand, motifs are not
+%          commonly studied in undirected networks, due to the paucity of
+%          local undirected connectivity patterns.
+%
+%   References: Onnela et al. (2005), Phys Rev E 71:065103
+%               Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M3 ID3 N3
+if isempty(N3)
+    load motif34lib M3 ID3 N3             	%load motif data
+end
+
+n=length(W);                                %number of vertices in W
+I=zeros(13,n);                              %intensity
+Q=zeros(13,n);                              %coherence
+F=zeros(13,n);                          	%frequency
+
+A=1*(W~=0);                                 %adjacency matrix
+As=A|A.';                                   %symmetrized adjacency
+
+for u=1:n-2                               	%loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];         	%v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];       %v2: all neibs of v1 (>u)
+        V2(V1)=0;                           %not already in V1
+        V2=([false(1,v1) As(u,v1+1:n)])|V2; %and all neibs of u (>v1)
+        for v2=find(V2)
+            w=[W(v1,u) W(v2,u) W(u,v1) W(v2,v1) W(u,v2) W(v1,v2)];
+            a=[A(v1,u);A(v2,u);A(u,v1);A(v2,v1);A(u,v2);A(v1,v2)];
+            ind=(M3*a)==N3;                 %find all contained isomorphs
+            m=sum(ind);                     %number of isomorphs
+
+            M=M3(ind,:).*repmat(w,m,1);
+            id=ID3(ind);
+            l=N3(ind);
+
+            x=sum(M,2)./l;                  %arithmetic mean
+            M(M==0)=1;                      %enable geometric mean
+            i=prod(M,2).^(1./l);            %intensity
+            q=i./x;                         %coherence
+
+            [idu,j]=unique(id);             %unique motif occurences
+            j=[0;j];                        %#ok<AGROW>
+            mu=length(idu);                 %number of unique motifs
+            i2=zeros(mu,1);
+            q2=i2; f2=i2;
+
+            for h=1:mu                      %for each unique motif
+                i2(h)=sum(i(j(h)+1:j(h+1)));    %sum all intensities,
+                q2(h)=sum(q(j(h)+1:j(h+1)));    %coherences
+                f2(h)=j(h+1)-j(h);              %and frequencies
+            end
+
+            %then add to cumulative count
+            I(idu,[u v1 v2])=I(idu,[u v1 v2])+[i2 i2 i2];
+            Q(idu,[u v1 v2])=Q(idu,[u v1 v2])+[q2 q2 q2];
+            F(idu,[u v1 v2])=F(idu,[u v1 v2])+[f2 f2 f2];
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_bin.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,56 @@
+function [f,F]=motif3struct_bin(A)
+%MOTIF3STRUCT_BIN       Frequency of structural class-3 motifs
+%
+%   [f,F] = motif3struct_bin(A);
+%
+%   Structural motifs are patterns of local connectivity in complex
+%   networks. Such patterns are particularly diverse in directed networks.
+%   The motif frequency of occurrence around an individual node is known as
+%   the motif fingerprint of that node. The total motif frequency of
+%   occurrence in the whole network is correspondingly known as the
+%   motif fingerprint of the network. 
+%
+%   Input:      A,      binary directed connection matrix
+%
+%   Output:     F,      node motif frequency fingerprint
+%               f,      network motif frequency fingerprint
+%
+%   Note: The function find_motif34.m outputs the motif legend.
+%
+%   References: Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M3n ID3
+if isempty(ID3)
+    load motif34lib M3n ID3              	%load motif data
+end
+
+n=length(A);                                %number of vertices in A
+F=zeros(13,n);                              %motif count of each vertex
+f=zeros(13,1);                              %motif count for whole graph
+As=A|A.';                                   %symmetrized adjacency matrix
+
+
+for u=1:n-2                               	%loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];         	%v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];       %v2: all neibs of v1 (>u)
+        V2(V1)=0;                           %not already in V1
+        V2=([false(1,v1) As(u,v1+1:n)])|V2; %and all neibs of u (>v1)
+        for v2=find(V2)
+
+            s=uint32(sum(10.^(5:-1:0).*[A(v1,u) A(v2,u) A(u,v1)...
+                A(v2,v1) A(u,v2) A(v1,v2)]));
+            ind=ID3(s==M3n);
+            if nargout==2; F(ind,[u v1 v2])=F(ind,[u v1 v2])+1; end
+            f(ind)=f(ind)+1;
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif3struct_wei.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,80 @@
+function [I,Q,F]=motif3struct_wei(W)
+%MOTIF3STRUCT_WEI       Intensity and coherence of structural class-3 motifs
+%
+%   [I,Q,F] = motif3struct_wei(W);
+%
+%   Structural motifs are patterns of local connectivity in complex
+%   networks. Such patterns are particularly diverse in directed networks.
+%   The motif frequency of occurrence around an individual node is known as
+%   the motif fingerprint of that node. The motif intensity and coherence
+%   are weighted generalizations of the motif frequency. The motif
+%   intensity is equivalent to the geometric mean of weights of links
+%   comprising each motif. The motif coherence is equivalent to the ratio
+%   of geometric and arithmetic means of weights of links comprising each
+%   motif.
+%
+%   Input:      W,      weighted directed connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     I,      node motif intensity fingerprint
+%               Q,      node motif coherence fingerprint
+%               F,      node motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. Average intensity and coherence are given by I./F and Q./F
+%       3. All weights must be between 0 and 1. This may be achieved using
+%          the weight_conversion.m function, as follows:
+%          W_nrm = weight_conversion(W, 'normalize');
+%
+%   References: Onnela et al. (2005), Phys Rev E 71:065103
+%               Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369%
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M3 M3n ID3 N3
+if isempty(N3)
+    load motif34lib M3 M3n ID3 N3         	%load motif data
+end
+
+n=length(W);                                %number of vertices in W
+I=zeros(13,n);                              %intensity
+Q=zeros(13,n);                              %coherence
+F=zeros(13,n);                          	%frequency
+
+A=1*(W~=0);                                 %adjacency matrix
+As=A|A.';                                   %symmetrized adjacency
+
+for u=1:n-2                               	%loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];         	%v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];       %v2: all neibs of v1 (>u)
+        V2(V1)=0;                           %not already in V1
+        V2=([false(1,v1) As(u,v1+1:n)])|V2; %and all neibs of u (>v1)
+        for v2=find(V2)
+            w=[W(v1,u) W(v2,u) W(u,v1) W(v2,v1) W(u,v2) W(v1,v2)];
+            s=uint32(sum(10.^(5:-1:0).*[A(v1,u) A(v2,u) A(u,v1)...
+                A(v2,v1) A(u,v2) A(v1,v2)]));
+            ind=(s==M3n);
+
+            M=w.*M3(ind,:);
+            id=ID3(ind);
+            l=N3(ind);
+            x=sum(M,2)/l;                	%arithmetic mean
+            M(M==0)=1;                      %enable geometric mean
+            i=prod(M,2)^(1/l);              %intensity
+            q=i/x;                          %coherence
+
+            %then add to cumulative count
+            I(id,[u v1 v2])=I(id,[u v1 v2])+[i i i];
+            Q(id,[u v1 v2])=Q(id,[u v1 v2])+[q q q];
+            F(id,[u v1 v2])=F(id,[u v1 v2])+[1 1 1];
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_bin.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,88 @@
+function [f,F]=motif4funct_bin(A)
+%MOTIF4FUNCT_BIN       Frequency of functional class-4 motifs
+%
+%   [f,F] = motif4funct_bin(A);
+%
+%   *Structural motifs* are patterns of local connectivity in complex
+%   networks. In contrast, *functional motifs* are all possible subsets of
+%   patterns of local connectivity embedded within structural motifs. Such
+%   patterns are particularly diverse in directed networks. The motif
+%   frequency of occurrence around an individual node is known as the motif
+%   fingerprint of that node. The total motif frequency of occurrence in
+%   the whole network is correspondingly known as the motif fingerprint of
+%   the network.
+%
+%   Input:      A,      binary directed connection matrix
+%
+%   Output:     F,      node motif frequency fingerprint
+%               f,      network motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. There is a source of possible confusion in motif terminology.
+%          Motifs ("structural" and "functional") are most frequently
+%          considered only in the context of anatomical brain networks
+%          (Sporns and Ktter, 2004). On the other hand, motifs are not
+%          commonly studied in undirected networks, due to the paucity of
+%          local undirected connectivity patterns.
+%
+%   References: Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M4 ID4 N4
+if isempty(N4)
+    load motif34lib M4 ID4 N4                 	%load motif data
+end
+
+n=length(A);                                    %number of vertices in A
+f=zeros(199,1);
+F=zeros(199,n);                                 %frequency
+
+A=1*(A~=0);                                     %adjacency matrix
+As=A|A.';                                       %symmetrized adjacency
+
+for u=1:n-3                                     %loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];                %v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];           %v2: all neibs of v1 (>u)
+        V2(V1)=0;                               %not already in V1
+        V2=V2|([false(1,v1) As(u,v1+1:n)]);     %and all neibs of u (>v1)
+        for v2=find(V2)
+            vz=max(v1,v2);                      %vz: largest rank node
+            V3=([false(1,u) As(v2,u+1:n)]);     %v3: all neibs of v2 (>u)
+            V3(V2)=0;                           %not already in V1&V2
+            V3=V3|([false(1,v2) As(v1,v2+1:n)]);%and all neibs of v1 (>v2)
+            V3(V1)=0;                           %not already in V1
+            V3=V3|([false(1,vz) As(u,vz+1:n)]); %and all neibs of u (>vz)
+            for v3=find(V3)
+
+                a=[A(v1,u);A(v2,u);A(v3,u);A(u,v1);A(v2,v1);A(v3,v1);...
+                    A(u,v2);A(v1,v2);A(v3,v2);A(u,v3);A(v1,v3);A(v2,v3)];
+                ind=(M4*a)==N4;                 %find all contained isomorphs
+                id=ID4(ind);
+
+                [idu,j]=unique(id);             %unique motif occurences
+                j=[0;j];                        %#ok<AGROW>
+                mu=length(idu);                 %number of unique motifs
+                f2=zeros(mu,1);
+
+                for h=1:mu                      %for each unique motif
+                    f2(h)=j(h+1)-j(h);              %and frequencies
+                end
+
+                %then add to cumulative count
+                f(idu)=f(idu)+f2;
+                if nargout==2
+                    F(idu,[u v1 v2 v3])=F(idu,[u v1 v2 v3])+[f2 f2 f2 f2];
+                end
+            end
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4funct_wei.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,111 @@
+function [I,Q,F]=motif4funct_wei(W)
+%MOTIF4FUNCT_WEI       Intensity and coherence of functional class-4 motifs
+%
+%   [I,Q,F] = motif4funct_wei(W);
+%
+%   *Structural motifs* are patterns of local connectivity in complex
+%   networks. In contrast, *functional motifs* are all possible subsets of
+%   patterns of local connectivity embedded within structural motifs. Such
+%   patterns are particularly diverse in directed networks. The motif
+%   frequency of occurrence around an individual node is known as the motif
+%   fingerprint of that node. The motif intensity and coherence are
+%   weighted generalizations of the motif frequency. The motif
+%   intensity is equivalent to the geometric mean of weights of links
+%   comprising each motif. The motif coherence is equivalent to the ratio
+%   of geometric and arithmetic means of weights of links comprising each
+%   motif.
+%
+%   Input:      W,      weighted directed connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     I,      node motif intensity fingerprint
+%               Q,      node motif coherence fingerprint
+%               F,      node motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. Average intensity and coherence are given by I./F and Q./F
+%       3. All weights must be between 0 and 1. This may be achieved using
+%          the weight_conversion.m function, as follows: 
+%          W_nrm = weight_conversion(W, 'normalize');
+%       4. There is a source of possible confusion in motif terminology.
+%          Motifs ("structural" and "functional") are most frequently
+%          considered only in the context of anatomical brain networks
+%          (Sporns and Ktter, 2004). On the other hand, motifs are not
+%          commonly studied in undirected networks, due to the paucity of
+%          local undirected connectivity patterns.
+%
+%   References: Onnela et al. (2005), Phys Rev E 71:065103
+%               Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369%
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M4 ID4 N4
+if isempty(N4)
+    load motif34lib M4 ID4 N4               	%load motif data
+end
+
+n=length(W);                                    %number of vertices in W
+I=zeros(199,n);                                 %intensity
+Q=zeros(199,n);                                 %coherence
+F=zeros(199,n);                                 %frequency
+
+A=1*(W~=0);                                     %adjacency matrix
+As=A|A.';                                       %symmetrized adjacency
+
+for u=1:n-3                                     %loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];                %v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];           %v2: all neibs of v1 (>u)
+        V2(V1)=0;                               %not already in V1
+        V2=V2|([false(1,v1) As(u,v1+1:n)]);     %and all neibs of u (>v1)
+        for v2=find(V2)
+            vz=max(v1,v2);                      %vz: largest rank node
+            V3=([false(1,u) As(v2,u+1:n)]);     %v3: all neibs of v2 (>u)
+            V3(V2)=0;                           %not already in V1&V2
+            V3=V3|([false(1,v2) As(v1,v2+1:n)]);%and all neibs of v1 (>v2)
+            V3(V1)=0;                           %not already in V1
+            V3=V3|([false(1,vz) As(u,vz+1:n)]); %and all neibs of u (>vz)
+            for v3=find(V3)
+
+                w=[W(v1,u) W(v2,u) W(v3,u) W(u,v1) W(v2,v1) W(v3,v1)...
+                    W(u,v2) W(v1,v2) W(v3,v2) W(u,v3) W(v1,v3) W(v2,v3)];
+                a=[A(v1,u);A(v2,u);A(v3,u);A(u,v1);A(v2,v1);A(v3,v1);...
+                    A(u,v2);A(v1,v2);A(v3,v2);A(u,v3);A(v1,v3);A(v2,v3)];
+                ind=(M4*a)==N4;                 %find all contained isomorphs
+                m=sum(ind);                     %number of isomorphs
+
+                M=M4(ind,:).*repmat(w,m,1);
+                id=ID4(ind);
+                l=N4(ind);
+                x=sum(M,2)./l;                  %arithmetic mean
+                M(M==0)=1;                      %enable geometric mean
+                i=prod(M,2).^(1./l);            %intensity
+                q=i./x;                         %coherence
+
+                [idu,j]=unique(id);             %unique motif occurences
+                j=[0;j];                        %#ok<AGROW>
+                mu=length(idu);                 %number of unique motifs
+                i2=zeros(mu,1);
+                q2=i2; f2=i2;
+
+                for h=1:mu                      %for each unique motif
+                    i2(h)=sum(i(j(h)+1:j(h+1)));    %sum all intensities,
+                    q2(h)=sum(q(j(h)+1:j(h+1)));    %coherences
+                    f2(h)=j(h+1)-j(h);              %and frequencies
+                end
+
+                %then add to cumulative count
+                I(idu,[u v1 v2 v3])=I(idu,[u v1 v2 v3])+[i2 i2 i2 i2];
+                Q(idu,[u v1 v2 v3])=Q(idu,[u v1 v2 v3])+[q2 q2 q2 q2];
+                F(idu,[u v1 v2 v3])=F(idu,[u v1 v2 v3])+[f2 f2 f2 f2];
+            end
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_bin.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,65 @@
+function [f,F]=motif4struct_bin(A)
+%MOTIF4STRUCT_BIN       Frequency of structural class-4 motifs
+%
+%   [f,F] = motif4struct_bin(A);
+%
+%   Structural motifs are patterns of local connectivity in complex
+%   networks. Such patterns are particularly diverse in directed networks.
+%   The motif frequency of occurrence around an individual node is known as
+%   the motif fingerprint of that node. The total motif frequency of
+%   occurrence in the whole network is correspondingly known as the
+%   motif fingerprint of the network. 
+%
+%   Input:      A,      binary directed connection matrix
+%
+%   Output:     F,      node motif frequency fingerprint
+%               f,      network motif frequency fingerprint
+%
+%   Note: The function find_motif34.m outputs the motif legend.
+%
+%   References: Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+
+persistent M4n ID4
+if isempty(ID4)
+    load motif34lib M4n ID4                     %load motif data
+end
+
+n=length(A);                                    %number of vertices in A
+F=zeros(199,n);                                 %motif count of each vertex
+f=zeros(199,1);                                 %motif count for whole graph
+As=A|A.';                                       %symmetric adjacency matrix
+
+for u=1:n-3                                     %loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];                %v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];           %v2: all neibs of v1 (>u)
+        V2(V1)=0;                               %not already in V1
+        V2=V2|([false(1,v1) As(u,v1+1:n)]);     %and all neibs of u (>v1)
+        for v2=find(V2)
+            vz=max(v1,v2);                      %vz: largest rank node
+            V3=([false(1,u) As(v2,u+1:n)]);     %v3: all neibs of v2 (>u)
+            V3(V2)=0;                           %not already in V1&V2
+            V3=V3|([false(1,v2) As(v1,v2+1:n)]);%and all neibs of v1 (>v2)
+            V3(V1)=0;                           %not already in V1
+            V3=V3|([false(1,vz) As(u,vz+1:n)]); %and all neibs of u (>vz)
+            for v3=find(V3)
+
+                s=uint64(sum(10.^(11:-1:0).*[A(v1,u) A(v2,u) A(v3,u)...
+                    A(u,v1) A(v2,v1) A(v3,v1) A(u,v2) A(v1,v2)...
+                    A(v3,v2) A(u,v3) A(v1,v3) A(v2,v3)]));
+                ind=ID4(s==M4n);
+                if nargout==2; F(ind,[u v1 v2 v3])=F(ind,[u v1 v2 v3])+1; end
+                f(ind)=f(ind)+1;
+            end
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_wei.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_wei.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_wei.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/motif4struct_wei.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,91 @@
+function [I,Q,F]=motif4struct_wei(W)
+%MOTIF4STRUCT_WEI       Intensity and coherence of structural class-4 motifs
+%
+%   [I,Q,F] = motif4struct_wei(W);
+%
+%   Structural motifs are patterns of local connectivity in complex
+%   networks. Such patterns are particularly diverse in directed networks.
+%   The motif frequency of occurrence around an individual node is known as
+%   the motif fingerprint of that node. The motif intensity and coherence
+%   are weighted generalizations of the motif frequency. The motif
+%   intensity is equivalent to the geometric mean of weights of links
+%   comprising each motif. The motif coherence is equivalent to the ratio
+%   of geometric and arithmetic means of weights of links comprising each
+%   motif.
+%
+%   Input:      W,      weighted directed connection matrix
+%                       (all weights must be between 0 and 1)
+%
+%   Output:     I,      node motif intensity fingerprint
+%               Q,      node motif coherence fingerprint
+%               F,      node motif frequency fingerprint
+%
+%   Notes: 
+%       1. The function find_motif34.m outputs the motif legend.
+%       2. Average intensity and coherence are given by I./F and Q./F
+%       3. All weights must be between 0 and 1. This may be achieved using
+%          the weight_conversion.m function, as follows: 
+%          W_nrm = weight_conversion(W, 'normalize');
+%
+%   References: Onnela et al. (2005), Phys Rev E 71:065103
+%               Milo et al. (2002) Science 298:824-827
+%               Sporns O, Ktter R (2004) PLoS Biol 2: e369%
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2007-2015
+
+%   Modification History:
+%   2007: Original
+%   2015: Improved documentation
+
+persistent M4 M4n ID4 N4
+if isempty(N4)
+    load motif34lib M4 M4n ID4 N4           	%load motif data
+end
+
+n=length(W);                                    %number of vertices in W
+I=zeros(199,n);                                 %intensity
+Q=zeros(199,n);                                 %coherence
+F=zeros(199,n);                                 %frequency
+
+A=1*(W~=0);                                     %adjacency matrix
+As=A|A.';                                       %symmetrized adjacency
+
+for u=1:n-3                                     %loop u 1:n-2
+    V1=[false(1,u) As(u,u+1:n)];                %v1: neibs of u (>u)
+    for v1=find(V1)
+        V2=[false(1,u) As(v1,u+1:n)];           %v2: all neibs of v1 (>u)
+        V2(V1)=0;                               %not already in V1
+        V2=V2|([false(1,v1) As(u,v1+1:n)]);     %and all neibs of u (>v1)
+        for v2=find(V2)
+            vz=max(v1,v2);                      %vz: largest rank node
+            V3=([false(1,u) As(v2,u+1:n)]);     %v3: all neibs of v2 (>u)
+            V3(V2)=0;                           %not already in V1&V2
+            V3=V3|([false(1,v2) As(v1,v2+1:n)]);%and all neibs of v1 (>v2)
+            V3(V1)=0;                           %not already in V1
+            V3=V3|([false(1,vz) As(u,vz+1:n)]); %and all neibs of u (>vz)
+            for v3=find(V3)
+
+                w=[W(v1,u) W(v2,u) W(v3,u) W(u,v1) W(v2,v1) W(v3,v1)...
+                    W(u,v2) W(v1,v2) W(v3,v2) W(u,v3) W(v1,v3) W(v2,v3)];
+                s=uint64(sum(10.^(11:-1:0).*[A(v1,u) A(v2,u) A(v3,u)...
+                    A(u,v1) A(v2,v1) A(v3,v1) A(u,v2) A(v1,v2)...
+                    A(v3,v2) A(u,v3) A(v1,v3) A(v2,v3)]));
+                ind=(s==M4n);
+
+                M=w.*M4(ind,:);
+                id=ID4(ind);
+                l=N4(ind);
+                x=sum(M,2)/l;                   %arithmetic mean
+                M(M==0)=1;                      %enable geometric mean
+                i=prod(M,2)^(1/l);              %intensity
+                q=i/x;                          %coherence
+
+                %then add to cumulative count
+                I(id,[u v1 v2 v3])=I(id,[u v1 v2 v3])+[i i i i];
+                Q(id,[u v1 v2 v3])=Q(id,[u v1 v2 v3])+[q q q q];
+                F(id,[u v1 v2 v3])=F(id,[u v1 v2 v3])+[1 1 1 1];
+            end
+        end
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/navigation_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/navigation_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/navigation_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/navigation_wu.m	2019-03-03 20:31:46.000000000 +0100
@@ -0,0 +1,112 @@
+function [sr, PL_bin, PL_wei, PL_dis, paths] = navigation_wu(L, D, max_hops)
+
+% Navigation of connectivity length matrix L guided by nodal distance D
+%
+% % Navigation
+% [sr, PL_bin, PL_wei] = navigation_wu(L,D);
+% % Binary shortest path length
+% sp_PL_bin = distance_bin(L);
+% % Weighted shortest path length
+% sp_PL_wei = distance_wei_floyd(L);
+% % Binary efficiency ratio
+% er_bin = mean(mean(sp_PL_bin./PL_bin));
+% % Weighted efficiency ratio
+% er_wei = mean(mean(sp_PL_wei./PL_wei));
+%
+% *** Inputs:
+%
+% L - Weighted/unweighted directed/undirected NxN SC matrix of connection *lengths*
+% L(i,j) is the strength-to-length remapping of the connection weight
+% between i and j. L(i,j) = 0 denotes the lack of a connection between i
+% and j.
+%
+% D - Symmetric NxN nodal distance matrix (e.g., Euclidean distance between node centroids)
+%
+% max_hops (optional) - Limits the maximum number of hops of navigation
+% paths
+%
+% *** Outputs:
+%
+% sr - The success ratio (scalar) is the proportion of node pairs
+% successfully reached by navigation.
+% 
+% PL_bin - NxN matrix of binary navigation path length (i.e., number of hops in
+% navigation paths). Infinte values indicate failed navigation paths.
+%
+% PL_wei - NxN matrix of weighted navigation path length (i.e., sum of connection 
+% weights--as defined by C--along navigaiton path). Infinte values indicate failed navigation paths.
+%
+% PL_dis - NxN matrix of distance-based navigation path length (i.e., sum of connection 
+% distances--as defined by D--along navigaiton path). Infinte values indicate failed navigation paths.
+%
+% paths - NxN cell of nodes comprising navigation paths.
+%
+% *** Reference: Seguin et al. (2018) PNAS.
+% 
+% Caio Seguin, University of Melbourne, 2017
+
+    if nargin == 2
+        max_hops = length(L);
+    end
+
+    N = size(L, 1);
+    paths = cell(N);
+    PL_bin = zeros(N);
+    PL_wei = zeros(N);
+    PL_dis = zeros(N);
+    
+    for i = 1:N
+        for j = 1:N
+            if (i ~= j)
+
+                curr_node = i;
+                last_node = curr_node;
+                target = j;
+                paths{i,j} = curr_node;
+                
+                pl_bin = 0;
+                pl_wei = 0;
+                pl_dis = 0;
+                
+                while (curr_node ~= target)
+                    
+                    neighbors = find(L(curr_node,:) ~= 0);
+                    
+                    [~, min_index] = min(D(target, neighbors));
+                    
+                    next_node = neighbors(min_index);
+                    
+                    if isempty(next_node) || next_node == last_node || pl_bin > max_hops
+
+                        pl_bin = Inf;
+                        pl_wei = Inf;
+                        pl_dis = Inf;
+                        break;
+                    
+                    end
+                    
+                    paths{i,j} = [paths{i,j} next_node];
+                    pl_bin = pl_bin + 1;
+                    pl_wei = L(curr_node, next_node) + pl_wei;
+                    pl_dis = D(curr_node, next_node) + pl_dis;
+                    
+                    last_node = curr_node;
+                    curr_node = next_node;
+                
+                end
+
+                PL_bin(i,j) = pl_bin;
+                PL_wei(i,j) = pl_wei;
+                PL_dis(i,j) = pl_dis;
+                
+            end
+        end
+    end
+    
+    PL_bin(1:N+1:end) = Inf;
+    PL_wei(1:N+1:end) = Inf;
+    PL_dis(1:N+1:end) = Inf;
+    
+    sr = 1 - (length(find(PL_bin == Inf)) - N)/(N*N - N);
+
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_dir_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_dir_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_dir_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_dir_sign.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,188 @@
+function [W0, R] = null_model_dir_sign(W,bin_swaps,wei_freq)
+%NULL_MODEL_DIR_SIGN     Directed random graphs with preserved weight,
+%                        degree and strength distributions
+%
+%   W0 = null_model_dir_sign(W);
+%   W0 = null_model_dir_sign(W,bin_swaps);
+%   W0 = null_model_dir_sign(W,bin_swaps,wei_freq);
+%   [W0 R] = null_model_dir_sign(W,bin_swaps,wei_freq);
+%
+%   This function randomizes an directed network with positive and
+%   negative weights, while preserving the degree and strength
+%   distributions. This function calls randmio_dir_signed.m
+%
+%   Inputs: W,          Directed weighted connection matrix
+%           bin_swaps,  Average number of swaps of each edge in binary randomization.
+%                           bin_swap=5 is the default (each edge rewired 5 times)
+%                           bin_swap=0 implies no binary randomization 
+%           wei_freq,   Frequency of weight sorting in weighted randomization
+%                           wei_freq must be in the range of: 0 < wei_freq <= 1
+%                           wei_freq=1 implies that weights are sorted at each step
+%                               (default in older [<2011] versions of MATLAB)
+%                           wei_freq=0.1 implies that weights are sorted at each 10th step
+%                               (faster, default in newer versions of MATLAB)
+%
+%   Output:     W0,     Randomized weighted connection matrix
+%               R,      Correlation coefficients between strength sequences
+%                           of input and output connection matrices
+%
+%   Notes:
+%       The value of bin_swaps is ignored when binary topology is fully
+%   connected (e.g. when the network has no negative weights).
+%       Randomization may be better (and execution time will be slower) for
+%   higher values of bin_swaps and wei_freq. Higher values of bin_swaps may
+%   enable a more random binary organization, and higher values of wei_freq
+%   may enable a more accurate conservation of strength sequences.
+%       R are the correlation coefficients between positive and negative
+%   in-strength and out-strength sequences of input and output connection
+%   matrices and are used to evaluate the accuracy with which strengths
+%   were preserved. Note that correlation coefficients may be a rough
+%   measure of strength-sequence accuracy and one could implement more
+%   formal tests (such as the Kolmogorov-Smirnov test) if desired.
+%
+%   Example usage:
+%
+%   %Create random directed weights matrix
+%
+%   W=randn(100);
+%
+%   %Compute one instance of null model (slow execution time):
+%   %bin_swaps=5,   rewire each binary edge 5 times on average
+%   %wei_freq=1,    sort all edges at every step
+%
+%   tic; [W0_slow R_slow]=null_model_dir_sign(W,5,1); R_slow, toc
+%
+%   R_slow =
+%       0.9795    0.9724    0.9772    0.9773
+%   Elapsed time is 3.485388 seconds.
+%
+%   %Compute another instance of of null model (fast execution time):
+%   %bin_swaps=5,   rewire each binary edge 5 times on average
+%   %wei_freq=0.1,  sort all edges at every 10th step (10=1/0.1)
+%
+%   tic; [W0_fast R_fast]=null_model_dir_sign(W,5,0.1); R_fast, toc
+%
+%   R_fast =
+%       0.9655    0.9652    0.9717    0.9804
+%   Elapsed time is 0.763831 seconds.
+%
+%
+%   Reference: Rubinov and Sporns (2011) Neuroimage 56:2068-79
+%
+%
+%   2011-2015, Mika Rubinov, U Cambridge
+
+%   Modification History
+%   Mar 2011: Original.
+%   Sep 2012: Edge-sorting acceleration.
+%   Dec 2015: Enforce preservation of negative degrees in sparse
+%             networks with negative weights (thanks to Andrew Zalesky).
+
+%#ok<*ASGLU>
+
+if ~exist('bin_swaps','var')
+    bin_swaps=5;
+end
+if ~exist('wei_freq','var')
+    if nargin('randperm')==1
+        wei_freq=1;
+    else
+        wei_freq=0.1;
+    end
+end
+
+if wei_freq<=0 || wei_freq>1
+    error('wei_freq must be in the range of: 0 < wei_freq <= 1.')
+end
+if wei_freq && wei_freq<1 && nargin('randperm')==1
+    warning('wei_freq may only equal 1 in older (<2011) versions of MATLAB.')
+    wei_freq=1;
+end
+
+n=size(W,1);                                            %number of nodes
+W(1:n+1:end)=0;                                         %clear diagonal
+Ap = W>0;                                               %positive adjacency matrix
+An = W<0;                                               %negative adjacency matrix
+
+if nnz(Ap)<(n*(n-1))                                    %if Ap is not full
+    W_r  = randmio_dir_signed(W,bin_swaps);
+    Ap_r = W_r>0;
+    An_r = W_r<0;
+else
+    Ap_r = Ap;
+    An_r = An;
+end
+
+W0=zeros(n);                                            %null model network
+for s=[1 -1]
+    switch s                                            %switch sign (positive/negative)
+        case 1
+            Si=sum(W.*Ap,1).';                          %positive in-strength
+            So=sum(W.*Ap,2);                            %positive out-strength
+            Wv=sort(W(Ap));                             %sorted weights vector
+            [I, J]=find(Ap_r);                          %weights indices
+            Lij=n*(J-1)+I;                              %linear weights indices
+        case -1
+            Si=sum(-W.*An,1).';                         %negative in-strength
+            So=sum(-W.*An,2);                           %negative out-strength
+            Wv=sort(-W(An));                            %sorted weights vector
+            [I, J]=find(An_r);                          %weights indices
+            Lij=n*(J-1)+I;                              %linear weights indices
+    end
+    
+    P=(So*Si.');                                        %expected weights matrix
+    
+    if wei_freq==1
+        for m=numel(Wv):-1:1                            %iteratively explore all weights
+            [dum, Oind]=sort(P(Lij));                   %get indices of Lij that sort P
+            r=ceil(rand*m);
+            o=Oind(r);                                  %choose random index of sorted expected weight
+            W0(Lij(o)) = s*Wv(r);                       %assign corresponding sorted weight at this index
+            
+            f = 1 - Wv(r)/So(I(o));                     %readjust expected weight probabilities for node I(o)
+            P(I(o),:) = P(I(o),:)*f;                    %[1 - Wv(r)/S(I(o)) = (S(I(o)) - Wv(r))/S(I(o))]
+            f = 1 - Wv(r)/Si(J(o));                     %readjust expected weight probabilities for node J(o)
+            P(:,J(o)) = P(:,J(o))*f;                    %[1 - Wv(r)/S(J(o)) = (S(J(o)) - Wv(r))/S(J(o))]
+            
+            So(I(o)) = So(I(o)) - Wv(r);                %readjust in-strength of node I(o)
+            Si(J(o)) = Si(J(o)) - Wv(r);                %readjust out-strength of node J(o)
+            Lij(o)=[];                                  %remove current index from further consideration
+            I(o)=[];
+            J(o)=[];
+            Wv(r)=[];                                   %remove current weight from further consideration
+        end
+    else
+        wei_period = round(1/wei_freq);                 %convert frequency to period
+        for m=numel(Wv):-wei_period:1                   %iteratively explore at the given period
+            [dum, Oind]=sort(P(Lij));                   %get indices of Lij that sort P
+            R=randperm(m,min(m,wei_period)).';
+
+            O=Oind(R);                                  %choose random index of sorted expected weight
+            W0(Lij(O)) = s*Wv(R);                       %assign corresponding sorted weight at this index
+
+            WAi = accumarray(I(O),Wv(R),[n,1]);
+            Iu = any(WAi,2);
+            F = 1 - WAi(Iu)./So(Iu);                    %readjust expected weight probabilities for node I(o)
+            P(Iu,:) = P(Iu,:).*F(:,ones(1,n));          %[1 - Wv(r)/S(I(o)) = (S(I(o)) - Wv(r))/S(I(o))]
+            So(Iu) = So(Iu) - WAi(Iu);                  %readjust in-strength of node I(o)
+
+            WAj = accumarray(J(O),Wv(R),[n,1]);
+            Ju = any(WAj,2);
+            F = 1 - WAj(Ju)./Si(Ju);                    %readjust expected weight probabilities for node J(o)
+            P(:,Ju) = P(:,Ju).*F(:,ones(1,n)).';        %[1 - Wv(r)/S(J(o)) = (S(J(o)) - Wv(r))/S(J(o))]
+            Si(Ju) = Si(Ju) - WAj(Ju);                  %readjust out-strength of node J(o)
+            
+            O=Oind(R);
+            Lij(O)=[];                                  %remove current index from further consideration
+            I(O)=[];
+            J(O)=[];
+            Wv(R)=[];                                   %remove current weight from further consideration
+        end
+    end
+end
+
+rpos_in=corrcoef(sum( W.*(W>0),1), sum( W0.*(W0>0),1) );
+rpos_ou=corrcoef(sum( W.*(W>0),2), sum( W0.*(W0>0),2) );
+rneg_in=corrcoef(sum(-W.*(W<0),1), sum(-W0.*(W0<0),1) );
+rneg_ou=corrcoef(sum(-W.*(W<0),2), sum(-W0.*(W0<0),2) );
+R=[rpos_in(2) rpos_ou(2) rneg_in(2) rneg_ou(2)];
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_und_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_und_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_und_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/null_model_und_sign.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,181 @@
+function [W0,R] = null_model_und_sign(W,bin_swaps,wei_freq)
+%NULL_MODEL_UND_SIGN     Random graphs with preserved weight, degree and
+%                        strength distributions
+%
+%   W0 = null_model_und_sign(W);
+%   W0 = null_model_und_sign(W,bin_swaps);
+%   W0 = null_model_und_sign(W,bin_swaps,wei_freq);
+%   [W0 R] = null_model_und_sign(W,bin_swaps,wei_freq);
+%
+%   This function randomizes an undirected network with positive and
+%   negative weights, while preserving the degree and strength
+%   distributions. This function calls randmio_und_signed.m
+%
+%   Inputs: W,          Undirected weighted connection matrix
+%           bin_swaps,  Average number of swaps of each edge in binary randomization.
+%                           bin_swap=5 is the default (each edge rewired 5 times)
+%                           bin_swap=0 implies no binary randomization
+%           wei_freq,   Frequency of weight sorting in weighted randomization
+%                           wei_freq must be in the range of: 0 < wei_freq <= 1
+%                           wei_freq=1 implies that weights are resorted at each step
+%                                   (default in older [<2011] versions of MATLAB)
+%                           wei_freq=0.1 implies that weights are sorted at each 10th step
+%                                   (faster, default in newer versions of Matlab)
+%
+%   Output:     W0,     Randomized weighted connection matrix
+%               R,      Correlation coefficient between strength sequences
+%                           of input and output connection matrices
+%
+%   Notes:
+%       The value of bin_swaps is ignored when binary topology is fully
+%   connected (e.g. when the network has no negative weights).
+%       Randomization may be better (and execution time will be slower) for
+%   higher values of bin_swaps and wei_freq. Higher values of bin_swaps may
+%   enable a more random binary organization, and higher values of wei_freq
+%   may enable a more accurate conservation of strength sequences.
+%       R are the correlation coefficients between positive and negative
+%   strength sequences of input and output connection matrices and are
+%   used to evaluate the accuracy with which strengths were preserved. Note
+%   that correlation coefficients may be a rough measure of
+%   strength-sequence accuracy and one could implement more formal tests
+%   (such as the Kolmogorov-Smirnov test) if desired.
+%
+%   Example usage:
+%
+%   %Create random weights matrix
+%
+%   W=tril(randn(100),-1); W=W+W.';
+%
+%   %Compute one instance of null model (slow execution time):
+%   %bin_swaps=5,   rewire each binary edge 5 times on average
+%   %wei_freq=1,    sort all edges at every step
+%
+%   tic; [W0_slow R_slow]=null_model_und_sign(W,5,1); R_slow, toc
+%
+%   R_slow =
+%       0.9720    0.9746
+%   Elapsed time is 2.112715 seconds.
+%
+%   %Compute another instance of of null model (fast execution time):
+%   %bin_swaps=5,   rewire each binary edge 5 times on average
+%   %wei_freq=0.1,  sort all edges at every 10th step (10=1/0.1)
+%
+%   tic; [W0_fast R_fast]=null_model_und_sign(W,5,0.1); R_fast, toc
+%
+%   R_fast =
+%       0.9623    0.9789
+%   Elapsed time is 0.584797 seconds.
+%
+%
+%   Reference: Rubinov and Sporns (2011) Neuroimage 56:2068-79
+%
+%
+%   2011-2015, Mika Rubinov, U Cambridge
+
+%   Modification History
+%   Mar 2011: Original
+%   Sep 2012: Edge-sorting acceleration
+%   Dec 2015: Enforce preservation of negative degrees in sparse
+%             networks with negative weights (thanks to Andrew Zalesky).
+
+%#ok<*ASGLU>
+
+if ~exist('bin_swaps','var')
+    bin_swaps=5;
+end
+if ~exist('wei_freq','var')
+    if nargin('randperm')==1
+        wei_freq=1;
+    else
+        wei_freq=0.1;
+    end
+end
+
+if wei_freq<=0 || wei_freq>1
+    error('wei_freq must be in the range of: 0 < wei_freq <= 1.')
+end
+if wei_freq && wei_freq<1 && nargin('randperm')==1
+    warning('wei_freq may only equal 1 in older (<2011) versions of MATLAB.')
+    wei_freq=1;
+end
+
+n=size(W,1);                                                %number of nodes
+W(1:n+1:end)=0;                                             %clear diagonal
+Ap = W>0;                                                   %positive adjacency matrix
+An = W<0;                                                   %negative adjacency matrix
+
+if nnz(Ap)<(n*(n-1))                                        %if Ap is not full
+    W_r  = randmio_und_signed(W,bin_swaps);
+    Ap_r = W_r>0;
+    An_r = W_r<0;
+else
+    Ap_r = Ap;
+    An_r = An;
+end
+
+W0=zeros(n);                                                %null model network
+for s=[1 -1]
+    switch s                                                %switch sign (positive/negative)
+        case 1
+            S=sum(W.*Ap,2);                                 %positive strength
+            Wv=sort(W(triu(Ap)));                           %sorted weights vector
+            [I,J]=find(triu(Ap_r));                         %weights indices
+            Lij=n*(J-1)+I;                                  %linear weights indices
+        case -1
+            S=sum(-W.*An,2);                                %negative strength
+            Wv=sort(-W(triu(An)));                          %sorted weights vector
+            [I,J]=find(triu(An_r));                         %weights indices
+            Lij=n*(J-1)+I;                                  %linear weights indices
+    end
+    
+    P=(S*S.');                                              %expected weights matrix
+    
+    if wei_freq==1
+        for m=numel(Wv):-1:1                                %iteratively explore all weights
+            [dum,Oind]=sort(P(Lij));                        %get indices of Lij that sort P
+            r=ceil(rand*m);
+            o=Oind(r);                                      %choose random index of sorted expected weight
+            W0(Lij(o)) = s*Wv(r);                           %assign corresponding sorted weight at this index
+            
+            f = 1 - Wv(r)/S(I(o));                          %readjust expected weight probabilities for node I(o)
+            P(I(o),:) = P(I(o),:)*f;                        %[1 - Wv(r)/S(I(o)) = (S(I(o)) - Wv(r))/S(I(o))]
+            P(:,I(o)) = P(:,I(o))*f;
+            f = 1 - Wv(r)/S(J(o));                          %readjust expected weight probabilities for node J(o)
+            P(J(o),:) = P(J(o),:)*f;                        %[1 - Wv(r)/S(J(o)) = (S(J(o)) - Wv(r))/S(J(o))]
+            P(:,J(o)) = P(:,J(o))*f;
+            
+            S([I(o) J(o)]) = S([I(o) J(o)])-Wv(r);          %readjust strengths of nodes I(o) and J(o)
+            Lij(o)=[];                                      %remove current index from further consideration
+            I(o)=[];
+            J(o)=[];
+            Wv(r)=[];                                       %remove current weight from further consideration
+        end
+    else
+        wei_period = round(1/wei_freq);                     %convert frequency to period
+        for m=numel(Wv):-wei_period:1                       %iteratively explore at the given period
+            [dum,Oind]=sort(P(Lij));                        %get indices of Lij that sort P
+            R=randperm(m,min(m,wei_period)).';
+            O = Oind(R);
+            W0(Lij(O)) = s*Wv(R);                           %assign corresponding sorted weight at this index
+            
+            WA = accumarray([I(O);J(O)],Wv([R;R]),[n,1]);   %cumulative weight
+            IJu = any(WA,2);
+            F = 1-WA(IJu)./S(IJu);
+            F = F(:,ones(1,n));                             %readjust expected weight probabilities for node I(o)
+            P(IJu,:) = P(IJu,:).*F;                         %[1 - Wv(r)/S(I(o)) = (S(I(o)) - Wv(r))/S(I(o))]
+            P(:,IJu) = P(:,IJu).*F.';
+            S(IJu) = S(IJu)-WA(IJu);                      	%re-adjust strengths of nodes I(o) and J(o)
+            
+            O=Oind(R);
+            Lij(O)=[];                                      %remove current index from further consideration
+            I(O)=[];
+            J(O)=[];
+            Wv(R)=[];                                       %remove current weight from further consideration
+        end
+    end
+end
+W0=W0+W0.';
+
+rpos=corrcoef(sum( W.*(W>0)),sum( W0.*(W0>0)));
+rneg=corrcoef(sum(-W.*(W<0)),sum(-W0.*(W0<0)));
+R=[rpos(2) rneg(2)];
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/pagerank_centrality.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/pagerank_centrality.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/pagerank_centrality.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/pagerank_centrality.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,57 @@
+function r = pagerank_centrality(A, d, falff)
+%PAGERANK_CENTRALITY       PageRank centrality
+%
+%   r = pagerank_centrality(A, d, falff)
+%
+%   The PageRank centrality is a variant of eigenvector centrality. This
+%   function computes the PageRank centrality of each vertex in a graph.
+%
+%   Formally, PageRank is defined as the stationary distribution achieved
+%   by instantiating a Markov chain on a graph. The PageRank centrality of
+%   a given vertex, then, is proportional to the number of steps (or amount
+%   of time) spent at that vertex as a result of such a process. 
+%
+%   The PageRank index gets modified by the addition of a damping factor,
+%   d. In terms of a Markov chain, the damping factor specifies the
+%   fraction of the time that a random walker will transition to one of its
+%   current state's neighbors. The remaining fraction of the time the
+%   walker is restarted at a random vertex. A common value for the damping
+%   factor is d = 0.85.
+%
+%   Inputs:     A,      adjacency matrix
+%               d,      damping factor
+%           falff,      initial page rank probability (non-negative)
+%
+%   Outputs:    r,      vectors of page rankings
+%
+%   Note: The algorithm will work well for smaller matrices (number of
+%   nodes around 1000 or less) 
+%
+%   References:
+%
+%   [1]. GeneRank: Using search engine technology for the analysis of
+%   microarray experiments, by Julie L. Morrison, Rainer Breitling, Desmond
+%   J. Higham and David R. Gilbert, BMC Bioinformatics, 6:233, 2005.
+% 	[2]. Boldi P, Santini M, Vigna S (2009) PageRank: Functional
+% 	dependencies. ACM Trans Inf Syst 27, 1-23.
+%
+%   Xi-Nian Zuo, Institute of Psychology, Chinese Academy of Sciences, 2011
+%   Rick Betzel, Indiana University, 2012
+
+N = size(A,1);
+if nargin < 3
+    norm_falff = ones(N,1)/N;
+else
+    falff = abs(falff);
+    norm_falff = falff/sum(falff);
+end
+
+deg = sum(A);
+ind = (deg == 0);
+deg(ind) = 1;
+D1 = zeros(N);
+D1(1:(N+1):end) = 1./deg;
+B = eye(N) - d*(A*D1);
+b = (1-d)*norm_falff;
+r = B\b;
+r = r/sum(r);
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,49 @@
+function P=participation_coef(W,Ci,flag)
+%PARTICIPATION_COEF     Participation coefficient
+%
+%   P = participation_coef(W,Ci);
+%
+%   Participation coefficient is a measure of diversity of intermodular
+%   connections of individual nodes.
+%
+%   Inputs:     W,      binary/weighted, directed/undirected connection matrix
+%               Ci,     community affiliation vector
+%               flag,   0, undirected graph (default)
+%                       1, directed graph: out-degree
+%                       2, directed graph: in-degree
+%
+%   Output:     P,      participation coefficient
+%
+%   Reference: Guimera R, Amaral L. Nature (2005) 433:895-900.
+%
+%
+%   2008-2015
+%   Mika Rubinov, UNSW/U Cambridge
+%   Alex Fornito, University of Melbourne
+
+%   Modification History:
+%   Jul 2008: Original (Mika Rubinov)
+%   Mar 2011: Weighted-network bug fixes (Alex Fornito)
+%   Jan 2015: Generalized for in- and out-degree (Mika Rubinov)
+
+if ~exist('flag','var')
+    flag=0;
+end
+
+switch flag
+    case 0 % no action required
+    case 1 % no action required
+    case 2; W=W.';
+end
+
+n=length(W);                        %number of vertices
+Ko=sum(W,2);                        %degree
+Gc=(W~=0)*diag(Ci);                 %neighbor community affiliation
+Kc2=zeros(n,1);                     %community-specific neighbors
+
+for i=1:max(Ci)
+   Kc2=Kc2+(sum(W.*(Gc==i),2).^2);
+end
+
+P=ones(n,1)-Kc2./(Ko.^2);
+P(~Ko)=0;                           %P=0 if for nodes with no (out)neighbors
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/participation_coef_sign.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,47 @@
+function [Ppos,Pneg]=participation_coef_sign(W,Ci)
+%PARTICIPATION_COEF_SIGN     Participation coefficient
+%
+%   [Ppos Pneg] = participation_coef_sign(W,Ci);
+%
+%   Participation coefficient is a measure of diversity of intermodular
+%   connections of individual nodes.
+%
+%   Inputs:     W,      undirected connection matrix with positive and
+%                       negative weights
+%
+%               Ci,     community affiliation vector
+%
+%   Output:     Ppos,   participation coefficient from positive weights
+%
+%               Pneg,   participation coefficient from negative weights
+%
+%   Reference: Guimera R, Amaral L. Nature (2005) 433:895-900.
+%
+%
+%   2011, Mika Rubinov, UNSW
+
+%   Modification History:
+%   Mar 2011: Original
+%   Sep 2012: Fixed treatment of nodes with no negative strength
+%             (thanks to Alex Fornito and Martin Monti)
+
+
+n=length(W);                                %number of vertices
+
+Ppos = pcoef( W.*(W>0));
+Pneg = pcoef(-W.*(W<0));
+
+    function P=pcoef(W_)
+        S   = sum(W_,2);                    %strength
+        Gc  = (W_~=0)*diag(Ci);             %neighbor community affiliation
+        Sc2 = zeros(n,1);                   %community-specific neighbors
+        
+        for i = 1:max(Ci)
+            Sc2 = Sc2 + (sum(W_.*(Gc==i),2).^2);
+        end
+        
+        P = ones(n,1) - Sc2./(S.^2);
+        P(isnan(P)) = 0;
+        P(~P) = 0;                            %p_ind=0 if no (out)neighbors
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/partition_distance.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/partition_distance.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/partition_distance.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/partition_distance.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,97 @@
+function [VIn, MIn] = partition_distance(Cx, Cy)
+%PARTITION_DISTANCE     Distance or similarity between community partitions
+%
+%   This function quantifies information-theoretic distance (normalized
+%   variation of information) or similarity (normalized mutual information)
+%   between community partitions.
+%
+%   VIn        = partition_distance(Cx);
+%   VIn        = partition_distance(Cx, Cy);
+%   [VIn, MIn] = partition_distance(Cx, Cy);
+%
+%   Inputs:
+%       Cx,
+%           Community partition vector or matrix of n rows and p columns,
+%           n is the number of network nodes, and p is the number of input
+%           community partitions (in the case of vector input p=1).
+%
+%       Cy (optional argument),
+%           Community partition vector or matrix of n rows and q columns. n
+%           is the number of nodes (must be equal to the number of nodes in
+%           Cq) and q is the number of input community partitions (may be
+%           different to the number of nodes in Cq). This argument may be
+%           omitted, in which case, the partition distance is computed
+%           between all pairwise partitions of Cx.
+%
+%   Outputs:
+%       VIn,
+%           Normalized variation of information ([p, q] matrix)
+%
+%       MIn,
+%           Normalized mutual information ([p, q] matrix)
+%
+%   Notes:
+%       Mathematical definitions.
+%
+%           VIn = [H(X) + H(Y) - 2MI(X, Y)]/log(n)
+%           MIn = 2MI(X, Y) / [H(X) + H(Y)]
+%
+%           where H is the entropy and MI is the mutual information
+%
+%
+%   Reference: Meila M (2007) J Multivar Anal 98, 873-895.
+%
+%
+%   2011-2017, Mika Rubinov, UNSW, Janelia HHMI
+
+%   Modification History:
+%   Mar 2011: Original
+%   Jan 2017: Added computation between input matrices.
+
+s = (nargin==1);
+if s
+    Cy = Cx;
+    d = 10.^ceil(log10(double(1 + max( Cx(:)) )));
+else
+    d = 10.^ceil(log10(double(1 + max([Cx(:);Cy(:)]) )));
+end
+
+if ~isequal([Cx(:);Cy(:)], int64([Cx(:);Cy(:)])) || min([Cx(:);Cy(:)])<=0
+    error('Input partitions must contain only positive integers.')
+end
+
+[n, p] = size(Cx);
+HX = zeros(p, 1);
+for i = 1:p
+    Px = nonzeros(accumarray(Cx(:, i), 1)) / n;                     % P(x)
+    HX(i) = - sum(Px .* log(Px));                                   % H(x)
+end
+
+if s
+    q = p;
+    HY = HX;
+else
+    [n_, q] = size(Cy);
+    assert(n == n_);
+    HY = zeros(q, 1);
+    for j = 1:q
+        Py = nonzeros(accumarray(Cy(:, j), 1)) / n;                 % P(y)
+        HY(j) = - sum(Py .* log(Py));                               % H(y)
+    end
+end
+
+VIn = zeros(p, q);
+MIn = zeros(p, q);
+for i = 1:p
+    j_idx = (s * (i - 1) + 1):q;
+    for j = j_idx
+        Pxy = nonzeros(accumarray(d*Cx(:, i) + Cy(:, j), 1)) / n; 	% P(x,y)
+        Hxy = -sum(Pxy .* log(Pxy));                                % H(x,y)
+        VIn(i, j) = (2 * Hxy - HX(i) - HY(j)) / log(n);             % VIn
+        MIn(i, j) = 2 * (HX(i) + HY(j) - Hxy) / (HX(i) + HY(j));    % MIn
+    end
+    if s
+        VIn(j_idx, i) = VIn(i, j_idx);
+        MIn(j_idx, i) = MIn(i, j_idx);
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/path_transitivity.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/path_transitivity.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/path_transitivity.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/path_transitivity.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,84 @@
+function T=path_transitivity(W,transform)
+% PATH_TRANSITIVITY             Transitivity based on shortest paths
+%
+%   T = path_transitivity(W,transform)
+%
+%   This function computes the density of local detours (triangles) that
+%   are available along the shortest-paths between all pairs of nodes.
+%
+%   Inputs:
+%
+%       W,
+%           unweighted/weighted undirected connection *weight* OR *length*
+%           matrix.
+%
+%
+%       transform,
+%           If the input matrix is a connection *weight* matrix, specify a
+%           transform that map input connection weights to connection
+%           lengths. Two transforms are available.
+%               'log' -> l_ij = -log(w_ij)
+%               'inv' -> l_ij =    1/w_ij
+%
+%           If the input matrix is a connection *length* matrix, do not
+%           specify a transform (or specify an empty transform argument).
+%
+%
+%   Output:
+%
+%       T,
+%           matrix of pairwise path transitivity.
+%
+%
+%   Olaf Sporns, Andrea Avena-Koenigsberger and Joaquin Goi, IU Bloomington, 2014
+%
+%   References: Goi et al (2014) PNAS doi: 10.1073/pnas.131552911
+%
+
+if ~exist('transform','var')
+    transform = [];
+end
+
+n=length(W);
+m=zeros(n,n);
+T=zeros(n,n);
+
+for i=1:n-1
+    for j=i+1:n
+        x=0;
+        y=0;
+        z=0;
+        for k=1:n
+            if W(i,k)~=0 && W(j,k)~=0 && k~=i && k~=j
+                x=x+W(i,k)+W(j,k);
+            end
+            if k~=j
+                y=y+W(i,k);
+            end
+            if k~=i
+                z=z+W(j,k);
+            end
+        end
+        m(i,j)=x/(y+z);
+    end
+end
+m=m+m';
+
+[~,hops,Pmat] = distance_wei_floyd(W,transform);
+
+% --- path transitivity ---%%
+for i=1:n-1
+    for j=i+1:n
+        x=0;
+        path = retrieve_shortest_path(i,j,hops,Pmat);
+        K=length(path);
+        
+        for t=1:K-1
+            for l=t+1:K
+                x=x+m(path(t),path(l));
+            end
+        end
+        T(i,j)=2*x/(K*(K-1));
+    end
+end
+T=T+T';
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/predict_fc.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/predict_fc.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/predict_fc.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/predict_fc.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,228 @@
+function [FCpre,FCcorr,beta,pred_data,R] = predict_fc(SC,FC,ED,pred_var,model)
+% PREDICT_FC    Prediction of functional connectivity from structural connectivity
+%
+%   [FCpre,FCcorr,beta,pred_data,R] = predict_fc(SC,FC,ED,pred_var,model)
+%   [FCpre,FCcorr,beta] = predict_fc(SC,FC,[],{'SPLwei_log','SIwei_log'},'quadratic')
+%
+%   This function computes regression coefficients to predict FC from
+%   structural-based measures that are used as predictor variables.
+%
+%   Inputs:
+%
+%       SC,
+%           Weighted/unweighted undirected NxN Structural Connectivity matrix.
+%
+%       FC,
+%           Functional connections. FC can be a NxN symmetric matrix or an
+%           ((N*(N-1))/2) x 1 vector containing the upper triangular
+%           elements of the square FC matrix (excluding diagonal elements).
+%
+%       ED,
+%           Euclidean distance matrix or upper triangular vector of the
+%           matrix (optional)
+%
+%       pred_var,
+%           Set of M predictors. These can be given as an KxM array where
+%           K = ((N*(N-1))/2) and M is the number of predictors.
+%           Alternatively, pred_var can be a cell with the names of network
+%           measures to be used as predictors. Accepted network measure
+%           names are:
+%               SPLbin        - Shortest-path length (binary)
+%               SPLwei_inv    - Shortest-path length computed with an inv transform
+%             	SPLwei_log    - Shortest-path length computed with a log transform
+%              	SPLdist       - Shortest-path length computed with no transform
+%              	SIbin         - Search Information of binary shortest-paths
+%              	SIwei_inv     - Search Information of shortest-paths computed with an inv transform
+%              	SIwei_log     - Search Information of shortest-paths computed with a log transform
+%              	SIdist        - Search Information of shortest-paths computed with no transform
+%              	T             - Path Transitivity
+%              	deltaMFPT     - Column-wise z-scored mean first passage time
+%               neighOverlap  - Neighborhood Overlap
+%              	MI            - Matching Index
+%
+%       	If no predictors are specified, the defaults are {'SPLwei_log', 'SIwei_log'}.
+%
+%   	model,
+%       	Specifies the order of the regression model used within
+%       	matlab's function regstats.m. 'model' can be any option
+%       	accepted by matlab's regstats.m function (e.g. 'linear',
+%       	'interaction', 'quadratic', etc.) If no model is specified,
+%       	'linear' is the default.
+%
+%   Output:
+%
+%       FCpre,
+%           Predicted NxN Functional Connectivity matrix
+%
+%      	FCcorr,
+%           Pearson Correlation between PCpred and FC
+%
+%      	beta,
+%           Regression Coefficients
+%
+%      	pred_data,
+%           KxM array of predictors.
+%
+%      	R,
+%           Output from regstats.m (e.g. 'beta', 'yhat', 'rsquare',
+%           'adjrsquare', 'tstat', 'r', 'mse', 'standres').
+%
+%
+%   References: Goi et al (2014) PNAS,  833838, doi: 10.1073/pnas.1315529111
+%
+%
+%   Andrea Avena-Koenigsberger and Joaquin Goi, IU Bloomington, 2014
+
+%   Modification history
+%   2012: Original
+%   2016: Added more predictors and possibility of accepting predictor
+%         names as input.
+
+pred_names = {'SPLbin','SPLwei_inv','SPLwei_log','SPLdist','SIbin',...
+    'SIwei_inv','SIwei_log','SIdist','T','deltaMFPT','neighOverlap','MI'};
+
+N = size(SC,1);
+indx = find(triu(ones(N),1));
+
+% select model
+if ~exist('model','var')
+    model = 'linear';
+end
+
+if ~exist('pred_var','var') && ~isempty(ED)
+    pred_var = {'ED','SPLwei_log','SI','T'};
+    flag_var_names = true;
+    flag_ED = true;
+elseif ~exist('pred_var','var') && isempty(ED)
+    pred_var = {'SPLwei_log','SI','T'};
+    flag_var_names = true;
+elseif exist('pred_var','var') && ~isnumeric(pred_var) && ~isempty(ED)
+    flag_var_names = true;
+    flag_ED = true;
+elseif exist('pred_var','var') && ~isnumeric(pred_var) && isempty(ED)
+    flag_var_names = true;
+    flag_ED = false;
+elseif exist('pred_var','var') && isnumeric(pred_var) && ~isempty(ED)
+    flag_var_names = false;
+    flag_ED = true;
+elseif exist('pred_var','var') && isnumeric(pred_var) && isempty(ED)
+    flag_var_names = false;
+    flag_ED = false;
+else
+    err_str = '"pred_var" must be an KxM array of M predictors, or any of the following graph-measure names:';
+    s1 = sprintf('SPLbin - Shortest-path length (binary) \n');
+    s2 = sprintf('SPLwei_inv - Shortest-path length computed with an inv transform \n');
+    s3 = sprintf('SPLwei_log - Shortest-path length computed with a log transform \n');
+    s4 = sprintf('SPLdist - Shortest-path length computed with no transform \n');
+    s5 = sprintf('SIbin - Search Information of binary shortest-paths \n');
+    s6 = sprintf('SIwei_inv - Search Information of shortest-paths computed with an inv transform \n');
+    s7 = sprintf('SIwei_log - Search Information of shortest-paths computed with a log transform \n');
+    s8 = sprintf('SIdist - Search Information of shortest-paths computed with no transform \n');
+    s9 = sprintf('T - Path Transitivity \n');
+    s10 = sprintf('deltaMFPT - Column-wise z-scored mean first passage time \n');
+    s11 = sprintf('neighOverlap - Neighborhood Overlap \n');
+    s12 = sprintf('MI - Matching Index \n');
+    error('%s \n %s %s %s %s %s %s %s %s %s %s %s %s',err_str,s1,s2,s3,s4,s5,s6,s7,s8,s9,s10,s11,s12);
+end
+
+if flag_ED
+    [n1,n2] = size(ED);
+    if n1 == n2 && n1 == N
+        % square ED matrix
+        pred_data = ED(indx);
+    elseif n1 == length(indx) && n2 == 1
+        % ED is already an upper-triangle vector
+        pred_data = ED;
+    else
+        error('ED must be square matrix or a vector containing the upper triangle of the square ED matrix \n')
+    end
+else
+    pred_data = [];
+end
+
+
+if flag_var_names
+    fprintf('\n----------------------');
+    fprintf('\n Selected predictors: \n');
+    ind2start = size(pred_data,2);
+    pred_data = [pred_data,zeros(length(indx),length(pred_var))];
+    
+    for v = 1:length(pred_var)
+        var_ind = find(strcmp(pred_var{v},pred_names));
+        switch var_ind
+            
+            case 1   %SPLbin
+                fprintf('Shortest-path length (binary) \n\n');
+                data = distance_wei_floyd(double(SC>0));
+            case 2   %SPLwei_inv
+                fprintf('Shortest-path length computed with an inv transform \n');
+                data = distance_wei_floyd(SC,'inv');
+            case 3   %SPLwei_log
+                fprintf('Shortest-path length computed with a log transform \n');
+                data = distance_wei_floyd(SC,'log');
+            case 4   %SPLdist
+                fprintf('Shortest-path length computed with no transform \n');
+                data = distance_wei_floyd(SC);
+            case 5   %SIbin
+                fprintf('Search Information of binary shortest-paths \n');
+                data = search_information(double(SC>0));
+                data = data + data';
+            case 6   %SIwei_inv
+                fprintf('Search Information of shortest-paths computed with an inv transform \n');
+                data = search_information(SC,'inv');
+                data = data + data';
+            case 7   %SIwei_log
+                fprintf('Search Information of shortest-paths computed with a log transform \n');
+                data = search_information(SC,'log');
+                data = data + data';
+            case 8   %SIdist
+                fprintf('Search Information of shortest-paths computed with no transform \n');
+                data = search_information(SC);
+                data = data + data';
+            case 9   %T
+                fprintf('Path Transitivity \n');
+                data = path_transitivity(double(SC>0));
+            case 10  %deltaMFPT
+                fprintf('Column-wise z-scored mean first passage time \n');
+                mfpt = mean_first_passage_time(SC);
+                deltamfpt = zscore(mfpt,[],1);
+                data = deltamfpt+deltamfpt';
+            case 11  %neighOverlap
+                fprintf('Neighborhood Overlap \n');
+                data = double(SC>0) * double(SC>0)';
+            case 12  %MI
+                fprintf('Matching Index \n');
+                data = matching_ind(SC);
+            otherwise
+                error('This is not an accepted predictor. See list of available predictors \n')
+        end
+        pred_data(:,ind2start+v) = data(indx);
+    end
+else
+    if size(pred_var,1) == length(indx)
+        pred_data = [pred_data,pred_var];
+    else
+        error('Custom predictors must be provided as KxM array of M predictors \n');
+    end
+end
+
+
+[n1,n2] = size(FC);
+if n1 == n2 && n1 == N
+    % square FC matrix
+    responses = FC(indx);
+elseif n1 == length(indx) && n2 == 1
+    % FC is already an upper-triangle vector
+    responses = FC;
+else
+    error('FC must be square matrix or a vector containing the upper triangle (no diagonal elements) of the square FC matrix \n')
+end
+
+% run multilinear model
+R =  regstats(responses,pred_data,model,{'beta','yhat','rsquare','adjrsquare','tstat','r','mse','standres'});
+beta = R.beta;
+FCpre = zeros(size(SC));
+FCpre(indx) = R.yhat;
+FCpre = FCpre+FCpre';
+FCcorr = corr(responses,FCpre(indx));
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/quasi_idempotence.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/quasi_idempotence.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/quasi_idempotence.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/quasi_idempotence.m	2019-03-03 20:23:48.000000000 +0100
@@ -0,0 +1,110 @@
+function [XN,IOTA,EPS,U]=quasi_idempotence(X,K)
+%QUASI_IDEMPOTENCE     Connection matrix quasi-idempotence
+%
+%   [XN,IOTA,EPS,U]=quasi_idempotence(X)
+%   [XN,IOTA,EPS,U]=quasi_idempotence(X,K)
+%
+%   The degree of quasi-idempotence of a matrix represents how close it 
+%   is to being idempotent, i.e. invariant to squaring. In turn, this 
+%   reflects how closely related the edges in the graph it corresponds to 
+%   are to the sums of all triangles between the corresponding nodes, 
+%   spanning the entirety of the network. This probes a form of 
+%   self-similarity, intended not as between nodes and modules, but 
+%   between as edges and triangles (or more generally paths).
+%   Networks wherein the edge strengths are weakly related to the 
+%   nodal strengths have low quasi-idempotence, and networks wherein the 
+%   edge strengths are significantly influenced by the strengths of the
+%   corresponding nodes have high quasi-idempotence. In other words the
+%   degree of quasi-idempotence may be viewed as a measure of
+%   "collectivity", meaning how strongly individual nodes participate
+%   in collective dynamics.
+%   
+%   Inputs:
+%       X,
+%           undirected weighted/binary connection matrix with 
+%           non-negative weights,
+%       K,
+%           number of iterations (optional),
+%               K<inf,        iterate a predetermined number of times,
+%               K=inf,        iterate until convergence is attained (default).
+%
+%       Note: see Minati et al. (2017) for a discussion of the issues
+%       associated with applying this measure to binary graphs.
+%
+%   Outputs:
+%       XN,
+%           the final matrix, from the last iteration,
+%       IOTA,
+%           the vector of correlation coefficients, one per iteration,
+%       EPS,
+%           the vector of errors, one per iteration,
+%       U,
+%           the number of iterations performed.
+%
+%       Note: see Minati et al. (2017) for a discussion of the 
+%       significance of IOTA(1) and IOTA(end).
+%
+%   Example:
+%       % Create a random undirected weighted connection matrix
+%       N=100;
+%       X=rand(N);
+%       X=X+X';
+%       [XN,IOTA,EPS,U]=quasi_idempotence(X);
+%       % observe that IOTA~0
+%       disp(IOTA);
+%       % 
+%       % make the connection matrix idempotent by squaring it
+%       X=X^2;
+%       [XN,IOTA,EPS,U]=quasi_idempotence(X);
+%       % observe that IOTA~1
+%       disp(IOTA);
+%
+%   Reference:
+%       Minati et al. (2017) CHAOS, 043115
+%
+%   Ludovico Minati, University of Trento, Trento, Italy
+%   and Institute of Nuclear Physics, Polish Academy of Sciences, Krakow,
+%   Poland, 2017-2018
+%   lminati@ieee.org
+%
+%   Modification history
+%   2018: Original
+
+X=cast(X,'double'); % enforce double-precision format
+if any(any(isnan(X))) % bail out if any nan found
+    error('found nan, giving up!');
+end
+if any(any(X<0)) % bail out if any negative elements found
+    error('found a negative element, giving up!');
+end
+if ~exist('K','var')||isempty(K)
+    K=inf;
+end
+N=length(X); % get matrix size
+X(eye(N)>0)=0; % null the diagonal in the initial matrix
+X=X/norm(X); % normalize to unit norm
+XN=X;
+mask=triu(ones(N),1)>0; % create mask for superdiagonal elements
+U=0; % initialize iterations counter
+IOTA=[]; % this vector will contain the correlation coefficients
+EPS=inf; % and this will contain the errors
+if isinf(K)
+    while EPS(end)>eps('double') % iterate until error below precision
+        U=U+1; % increase iteration counter
+        XN_hat=XN; % save the initial matrix
+        XN=XN^2; % square the matrix
+        XN=XN/norm(XN); % normalize it again (for numerical reasons)
+        IOTA(end+1)=corr(X(mask),XN(mask)); % calculate correlation coefficient
+        EPS(end+1)=norm(XN_hat-XN); % calculate error
+    end
+else
+    while U<K % iterate a prescribed number of times
+        U=U+1; % increase iteration counter
+        XN_hat=XN; % save the initial matrix
+        XN=XN^2; % square the matrix
+        XN=XN/norm(XN); % normalize it again (for numerical reasons)
+        IOTA(end+1)=corr(X(mask),XN(mask)); % calculate correlation coefficient
+        EPS(end+1)=norm(XN_hat-XN); % calculate error
+    end
+end
+EPS(1)=[];
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,70 @@
+function [R,eff]=randmio_dir(R, ITER)
+%RANDMIO_DIR    Random graph with preserved in/out degree distribution
+%
+%   R = randmio_dir(W, ITER);
+%   [R eff] = randmio_dir(W, ITER);
+%
+%   This function randomizes a directed network, while preserving the in-
+%   and out-degree distributions. In weighted networks, the function
+%   preserves the out-strength but not the in-strength distributions.
+%
+%   Input:      W,      directed (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Mar 2012: Limit number of rewiring attempts, count number of successful
+%             rewirings (Olaf Sporns)
+
+
+n=size(R,1);
+[i,j]=find(R);
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+            
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            R(a,d)=R(a,b); R(a,b)=0;
+            R(c,b)=R(c,d); R(c,d)=0;
+            
+            j(e1) = d;          %reassign edge indices
+            j(e2) = b;
+            eff = eff+1;
+            break;
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_connected.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_connected.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_connected.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_connected.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,98 @@
+function [R,eff] = randmio_dir_connected(R, ITER)
+%RANDMIO_DIR_CONNECTED    Random graph with preserved in/out degree distribution
+%
+%   R = randmio_dir_connected(W, ITER);
+%   [R eff] = randmio_dir_connected(W, ITER);
+%
+%   This function randomizes a directed network, while preserving the in-
+%   and out-degree distributions. In weighted networks, the function
+%   preserves the out-strength but not the in-strength distributions. The
+%   function also ensures that the randomized network maintains
+%   connectedness, the ability for every node to reach every other node in
+%   the network. The input network for this function must be connected.
+%
+%   Input:      W,      directed (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Mar 2012: Limit number of rewiring attempts, count number of successful
+%             rewirings (Olaf Sporns)
+
+
+n=size(R,1);
+[i,j]=find(R);
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        rewire=1;
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+            
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %connectedness condition
+            if ~(any([R(a,c) R(d,b) R(d,c)]) && any([R(c,a) R(b,d) R(b,a)]))
+                P=R([a c],:);
+                P(1,b)=0; P(1,d)=1;
+                P(2,d)=0; P(2,b)=1;
+                PN=P;
+                PN(1,a)=1; PN(2,c)=1;
+                
+                while 1
+                    P(1,:)=any(R(P(1,:)~=0,:),1);
+                    P(2,:)=any(R(P(2,:)~=0,:),1);
+                    P=P.*(~PN);
+                    PN=PN+P;
+                    if ~all(any(P,2))
+                        rewire=0;
+                        break
+                    elseif any(PN(1,[b c])) && any(PN(2,[d a]))
+                        break
+                    end
+                end
+            end %connectedness testing
+            
+            if rewire               %reassign edges
+                R(a,d)=R(a,b); R(a,b)=0;
+                R(c,b)=R(c,d); R(c,d)=0;
+                
+                j(e1) = d;          %reassign edge indices
+                j(e2) = b;
+                eff = eff+1;
+                break;
+            end %edge reassignment
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_signed.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_signed.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_signed.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_dir_signed.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,80 @@
+function [R,eff] = randmio_dir_signed(W, ITER)
+% RANDMIO_DIR_SIGNED        Random directed graph with preserved signed
+%                           in/out degree distribution
+%
+%   R       = randmio_dir(W, ITER);
+%   [R,eff] = randmio_dir(W, ITER);
+%
+%   This function randomizes a directed weighted network with positively and
+%   negatively signed connections, while preserving the positively and
+%   negatively signed in- and out-degree distributions. In weighted
+%   networks, the function preserves the out-strength but not the
+%   in-strength distributions.
+%
+%   Input:      W,      directed (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   Reference:  Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2011-2015
+%   Dani Bassett, UCSB
+%   Olaf Sporns,  Indiana U
+%   Mika Rubinov, U Cambridge
+
+%   Modification History:
+%   Mar 2011: Original (Dani Bassett, based on randmio_und.m)
+%   Mar 2012: Limit number of rewiring attempts,
+%             count number of successful rewirings (Olaf Sporns)
+%   Dec 2015: Rewritten the core of the rewiring algorithm to allow
+%             unbiased exploration of all network configurations. The new
+%             algorithm allows positive-positive/negative-negative
+%             rewirings, in addition to the previous positive-positive/0-0
+%             and negative-negative/0-0 rewirings (Mika Rubinov). 
+
+if nargin('randperm')==1
+    warning('This function requires a recent (>2011) version of MATLAB.')
+end
+
+R = double(W);              % sign function requires double input
+n = size(R,1);
+ITER=ITER*n*(n-1);
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts=n;
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        %select four distinct vertices
+        nodes = randperm(n,4);
+        a = nodes(1);
+        b = nodes(2);
+        c = nodes(3);
+        d = nodes(4);
+        
+        r0_ab = R(a,b);
+        r0_cd = R(c,d);
+        r0_ad = R(a,d);
+        r0_cb = R(c,b);
+        
+        %rewiring condition
+        if      (sign(r0_ab)==sign(r0_cd)) && ...
+                (sign(r0_ad)==sign(r0_cb)) && ...
+                (sign(r0_ab)~=sign(r0_ad))
+            
+            R(a,d)=r0_ab; R(a,b)=r0_ad;
+            R(c,b)=r0_cd; R(c,d)=r0_cb;
+            
+            eff = eff+1;
+            break;
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,80 @@
+function [R,eff]=randmio_und(R, ITER)
+%RANDMIO_UND     Random graph with preserved degree distribution
+%
+%   R = randmio_und(W,ITER);
+%   [R eff]=randmio_und(W, ITER);
+%
+%   This function randomizes an undirected network, while preserving the
+%   degree distribution. The function does not preserve the strength
+%   distribution in weighted networks.
+%
+%   Input:      W,      undirected (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Apr 2008: Edge c-d is flipped with 50% probability, allowing to explore
+%             all potential rewirings (Jonathan Power)
+%   Mar 2012: Limit number of rewiring attempts, count number of successful
+%             rewirings (Olaf Sporns)
+
+
+n=size(R,1);
+[i,j]=find(tril(R));
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+            
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+        
+        if rand>0.5
+            i(e2)=d; j(e2)=c; 	%flip edge c-d with 50% probability
+            c=i(e2); d=j(e2); 	%to explore all potential rewirings
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            R(a,d)=R(a,b); R(a,b)=0;
+            R(d,a)=R(b,a); R(b,a)=0;
+            R(c,b)=R(c,d); R(c,d)=0;
+            R(b,c)=R(d,c); R(d,c)=0;
+            
+            j(e1) = d;          %reassign edge indices
+            j(e2) = b;
+            eff = eff+1;
+            break;
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_connected.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_connected.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_connected.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_connected.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,107 @@
+function [R,eff] = randmio_und_connected(R, ITER)
+%RANDMIO_UND_CONNECTED     Random graph with preserved degree distribution
+%
+%   R = randmio_und_connected(W,ITER);
+%   [R eff] = randmio_und_connected(W, ITER);
+%
+%   This function randomizes an undirected network, while preserving the
+%   degree distribution. The function does not preserve the strength
+%   distribution in weighted networks. The function also ensures that the
+%   randomized network maintains connectedness, the ability for every node
+%   to reach every other node in the network. The input network for this
+%   function must be connected.
+%
+%   Input:      W,      undirected (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2007-2012
+%   Mika Rubinov, UNSW
+%   Jonathan Power, WUSTL
+%   Olaf Sporns, IU
+
+%   Modification History:
+%   Jun 2007: Original (Mika Rubinov)
+%   Apr 2008: Edge c-d is flipped with 50% probability, allowing to explore
+%             all potential rewirings (Jonathan Power)
+%   Mar 2012: Limit number of rewiring attempts, count number of successful
+%             rewirings (Olaf Sporns)
+
+
+n=size(R,1);
+[i,j]=find(tril(R));
+K=length(i);
+ITER=K*ITER;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts= round(n*K/(n*(n-1)));
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)                                     %while not rewired
+        rewire=1;
+        while 1
+            e1=ceil(K*rand);
+            e2=ceil(K*rand);
+            while (e2==e1)
+                e2=ceil(K*rand);
+            end
+            a=i(e1); b=j(e1);
+            c=i(e2); d=j(e2);
+            
+            if all(a~=[c d]) && all(b~=[c d])
+                break           %all four vertices must be different
+            end
+        end
+        
+        if rand>0.5
+            i(e2)=d; j(e2)=c; 	%flip edge c-d with 50% probability
+            c=i(e2); d=j(e2); 	%to explore all potential rewirings
+        end
+        
+        %rewiring condition
+        if ~(R(a,d) || R(c,b))
+            %connectedness condition
+            if ~(R(a,c) || R(b,d))
+                P=R([a d],:);
+                P(1,b)=0; P(2,c)=0;
+                PN=P;
+                PN(:,d)=1; PN(:,a)=1;
+                
+                while 1
+                    P(1,:)=any(R(P(1,:)~=0,:),1);
+                    P(2,:)=any(R(P(2,:)~=0,:),1);
+                    P=P.*(~PN);
+                    if ~all(any(P,2))
+                        rewire=0;
+                        break
+                    elseif any(any(P(:,[b c])))
+                        break
+                    end
+                    PN=PN+P;
+                end
+            end %connectedness testing
+            
+            if rewire               %reassign edges
+                R(a,d)=R(a,b); R(a,b)=0;
+                R(d,a)=R(b,a); R(b,a)=0;
+                R(c,b)=R(c,d); R(c,d)=0;
+                R(b,c)=R(d,c); R(d,c)=0;
+                
+                j(e1) = d;          %reassign edge indices
+                j(e2) = b;
+                eff = eff+1;
+                break;
+            end %edge reassignment
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_signed.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_signed.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_signed.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randmio_und_signed.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,80 @@
+function [R,eff] = randmio_und_signed(W, ITER)
+% RANDMIO_UND_SIGNED	Random graph with preserved signed degree distribution
+%
+%   R       = randmio_und_signed(W,ITER);
+%   [R,eff] = randmio_und_signed(W,ITER);
+%
+%   This function randomizes an undirected network with positively and
+%   negatively signed connections, while preserving the positively and
+%   negatively signed degree distribution. The function does not preserve
+%   the strength distribution in weighted networks.
+%
+%   Input:      W,      undirected (binary/weighted) connection matrix
+%               ITER,   rewiring parameter
+%                       (each edge is rewired approximately ITER times)
+%
+%   Output:     R,      randomized network
+%               eff,    number of actual rewirings carried out
+%
+%   Reference:  Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   2011-2015
+%   Dani Bassett, UCSB
+%   Olaf Sporns,  Indiana U
+%   Mika Rubinov, U Cambridge
+
+%   Modification History:
+%   Mar 2011: Original (Dani Bassett, based on randmio_und.m)
+%   Mar 2012: Limit number of rewiring attempts,
+%             count number of successful rewirings (Olaf Sporns)
+%   Dec 2015: Rewritten the core of the rewiring algorithm to allow
+%             unbiased exploration of all network configurations. The new
+%             algorithm allows positive-positive/negative-negative
+%             rewirings, in addition to the previous positive-positive/0-0
+%             and negative-negative/0-0 rewirings (Mika Rubinov). 
+
+if nargin('randperm')==1
+    warning('This function requires a recent (>2011) version of MATLAB.')
+end
+
+R     = double(W);              % sign function requires double input
+n     = size(R,1);
+ITER  = ITER*n*(n-1)/2;
+
+% maximal number of rewiring attempts per 'iter'
+maxAttempts = round(n/2);
+% actual number of successful rewirings
+eff = 0;
+
+for iter=1:ITER
+    att=0;
+    while (att<=maxAttempts)    %while not rewired
+        %select four distinct vertices
+        nodes = randperm(n,4);
+        a = nodes(1);
+        b = nodes(2);
+        c = nodes(3);
+        d = nodes(4);
+        
+        r0_ab = R(a,b);
+        r0_cd = R(c,d);
+        r0_ad = R(a,d);
+        r0_cb = R(c,b);
+        
+        %rewiring condition
+        if      (sign(r0_ab)==sign(r0_cd)) && ...
+                (sign(r0_ad)==sign(r0_cb)) && ...
+                (sign(r0_ab)~=sign(r0_ad))
+            
+            R(a,d)=r0_ab; R(a,b)=r0_ad;
+            R(d,a)=r0_ab; R(b,a)=r0_ad;
+            R(c,b)=r0_cd; R(c,d)=r0_cb;
+            R(b,c)=r0_cd; R(d,c)=r0_cb;
+            
+            eff = eff+1;
+            break;
+        end %rewiring condition
+        att=att+1;
+    end %while not rewired
+end %iterations
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randomize_graph_partial_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randomize_graph_partial_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randomize_graph_partial_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randomize_graph_partial_und.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,53 @@
+function A = randomize_graph_partial_und(A,B,maxswap)
+% RANDOMIZE_GRAPH_PARTIAL_UND    Swap edges with preserved degree sequence
+%
+%   A = RANDOMIZE_GRAPH_PARTIAL_UND(A,B,MAXSWAP) takes adjacency matrices A 
+%   and B and attempts to randomize matrix A by performing MAXSWAP 
+%   rewirings. The rewirings will avoid any spots where matrix B is 
+%   nonzero.
+%
+%   Inputs:       A,      undirected adjacency matrix
+%                 B,      edges to avoid
+%           MAXSWAP,      number of rewirings
+%
+%   Outputs:      A,      randomized matrix
+%
+%   Richard Betzel, Indiana University, 2013
+%
+%   Notes:
+%   1. Based on the script randmio_und.m.
+%   2. Graph may become disconnected as a result of rewiring. Always
+%      important to check.
+%   3. A can be weighted, though the weighted degree sequence will not be
+%      preserved.
+%
+
+[i,j] = find(triu(A,1));
+m = length(i);
+nswap = 0;
+while nswap < maxswap
+    while 1
+        e1 = randi(m); e2 = randi(m);
+        while e2 == e1
+            e2 = randi(m);
+        end
+        a = i(e1); b = j(e1);
+        c = i(e2); d = j(e2);
+        if all(a~=[c,d]) && all(b~=[c,d])
+            break
+        end
+    end
+    if rand > 0.5
+        i(e2) = d; j(e2) = c;
+        c = i(e2); d = j(e2);
+    end
+    if ~(A(a,d) || A(c,b) || B(a,d) || B(c,b))
+        A(a,d) = A(a,b); A(a,b) = 0;
+        A(d,a) = A(b,a); A(b,a) = 0;
+        A(c,b) = A(c,d); A(c,d) = 0;
+        A(b,c) = A(d,c); A(d,c) = 0;
+        j(e1) = d;
+        j(e2) = b;
+        nswap = nswap + 1;
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randomizer_bin_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randomizer_bin_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/randomizer_bin_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/randomizer_bin_und.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,140 @@
+function [R] = randomizer_bin_und(R,alpha)
+%RANDOMIZER_BIN_UND     Random graph with preserved in/out degree distribution
+%
+%   R = randomizer_bin_und(A,alpha);
+%
+%   This function randomizes a binary undirected network, while preserving 
+%   the degree distribution. The function directly searches for rewirable 
+%   edge pairs (rather than trying to rewire edge pairs at random), and 
+%   hence avoids long loops and works especially well in dense matrices.
+%
+%   Inputs:     A,          binary undirected connection matrix
+%               alpha,      fraction of edges to rewire
+%
+%   Outputs:    R,          randomized network
+%
+%   References: Maslov and Sneppen (2002) Science 296:910
+%
+%
+%   Jonathan Power, WUSTL. 3/1/10.
+
+%#ok<*ASGLU>
+
+% make binary
+R=ceil(R);
+
+% ensure that matrix is binary
+if (max(R(:))~=1) || (min(R(:))~=0)
+    error('Matrix should be binary');
+end
+
+% ensure that matrix is undirected
+if ~isequal(R,R.')
+    error('Matrix should be undirected');
+end
+
+% find how many edges are possible in the network
+[a,b]=size(R);
+numpossibleedges=((a*a)-a)/2;
+
+% excise the diagonal and replace it with 9999
+savediag=R.*(eye(size(R,1)));
+R=R.*(~eye(size(R,1)));
+R=R+(eye(size(R,1)).*9999);
+
+% if there are more edges than non-edges we invert the matrix to reduce
+% computation time, then revert at the end of the script
+inverted=0;
+[i,j]=find(triu(R,1)); 
+K=size(i,1);
+if K>(numpossibleedges/2)
+    inverted=1;
+    R=double(~R);
+    R=R.*(~eye(size(R,1)));
+    R=R+(eye(size(R,1)).*9999);
+end
+
+% find edges
+[i,j]=find(triu(R,1));
+% K=size(i,1);
+
+% exclude fully connected nodes. will replace later
+fullnode=find((sum(triu(R,1),1)+(sum(triu(R,1),2))')==(a-1));
+if ~isempty(fullnode)
+    R(fullnode,:)=0; R(:,fullnode)=0;
+    R=R.*(~eye(size(R,1)));
+    R=R+(eye(size(R,1)).*9999);
+end
+
+% find the edges
+[i,j]=find(triu(R,1));
+K=size(i,1);
+
+if (isempty(K) || K==(numpossibleedges) || (K==numpossibleedges-1))
+    fprintf('No possible randomization.\n')
+else
+    for iter=1:K % for every edge
+        if rand<=alpha % rewire ~alpha% of edges
+
+            % this is the chosen edge
+            a=i(iter);
+            b=j(iter);
+            
+            % for selected edge, see where each end can connect to
+            alliholes=find(R(:,i(iter))==0);
+            alljholes=find(R(:,j(iter))==0);
+            
+            % we can only use edges with connection to neither node
+            iintersect=intersect(alliholes,alljholes);            
+            
+            % find which of these nodes are connected
+            [ii,jj]=find(R(iintersect,iintersect)==1);         
+
+            % if there an edge to switch
+            if ~isempty(ii)
+
+                % choose one randomly
+                nummates=size(ii,1);
+                mate=ceil(rand*nummates);
+                
+                % randomly orient the second edge
+                if rand<0.5
+                    c=iintersect(ii(mate));
+                    d=iintersect(jj(mate));
+                else
+                    d=iintersect(ii(mate));
+                    c=iintersect(jj(mate));
+                end
+
+                % make the changes in the matrix
+                R(a,b)=0; R(c,d)=0;
+                R(b,a)=0; R(d,c)=0;
+                R(a,c)=1; R(b,d)=1;
+                R(c,a)=1; R(d,b)=1;
+                
+                % update the edge index
+                for m=1:K
+                    if ((i(m)==d) && (j(m)==c))
+                        j(iter)=c; j(m)=b;
+                    elseif ((i(m)==c) && (j(m)==d))
+                        j(iter)=c; i(m)=b;
+                    end
+                end
+            end % rewiring
+        end % if rand<alpha
+    end % for every edge
+end % if enough edges to flip
+
+% restore full columns
+if ~isempty(fullnode)
+    R(fullnode,:)=1; R(:,fullnode)=1;
+end
+
+% if we did non-edges switch it back to edges
+if inverted==1
+    R=double(~R);
+end
+
+% clear and restore the diagonal
+R=R.*(~eye(size(R,1)));
+R=R+savediag;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reachdist.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reachdist.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reachdist.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reachdist.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,68 @@
+function  [R,D] = reachdist(CIJ)
+%REACHDIST      Reachability and distance matrices
+%
+%   [R,D] = reachdist(CIJ);
+%
+%   The binary reachability matrix describes reachability between all pairs
+%   of nodes. An entry (u,v)=1 means that there exists a path from node u
+%   to node v; alternatively (u,v)=0.
+%
+%   The distance matrix contains lengths of shortest paths between all
+%   pairs of nodes. An entry (u,v) represents the length of shortest path 
+%   from node u to  node v. The average shortest path length is the 
+%   characteristic path length of the network.
+%
+%   Input:      CIJ,     binary (directed/undirected) connection matrix
+%
+%   Outputs:    R,       reachability matrix
+%               D,       distance matrix
+%
+%   Note: faster but more memory intensive than "breadthdist.m".
+%
+%   Algorithm: algebraic path count.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+% initialize
+R = CIJ;
+D = CIJ;
+powr = 2;
+N = size(CIJ,1);
+CIJpwr = CIJ;
+
+% Check for vertices that have no incoming or outgoing connections.
+% These are "ignored" by 'reachdist'.
+id = sum(CIJ,1);       % indegree = column sum of CIJ
+od = sum(CIJ,2)';      % outdegree = row sum of CIJ
+id_0 = find(id==0);    % nothing goes in, so column(R) will be 0
+od_0 = find(od==0);    % nothing comes out, so row(R) will be 0
+% Use these columns and rows to check for reachability:
+col = setxor(1:N,id_0);
+row = setxor(1:N,od_0);
+
+[R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row);
+
+% "invert" CIJdist to get distances
+D = powr - D+1;
+
+% Put 'Inf' if no path found
+D(D==(N+2)) = Inf;
+D(:,id_0) = Inf;
+D(od_0,:) = Inf;
+
+
+%----------------------------------------------------------------------------
+
+function  [R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row)
+
+% Olaf Sporns, Indiana University, 2002/2008
+
+CIJpwr = CIJpwr*CIJ;
+R = double(R | ((CIJpwr)~=0));
+D = D+R;
+
+if ((powr<=N)&&(~isempty(nonzeros(R(row,col)==0)))) 
+   powr = powr+1;
+   [R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row); 
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_2d.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_2d.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_2d.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_2d.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,197 @@
+function [N,E] = rentian_scaling_2d(A,XY,n,tol)
+% RENTIAN_SCALING_2D    Rentian scaling for networks embedded in two dimensions.
+%
+% [N,E] = rentian_scaling_2d(A,XY,n,tol)
+%
+% Physical Rentian scaling (or more simply Rentian scaling) is a property
+% of systems that are cost-efficiently embedded into physical space. It is
+% what is called a "topo-physical" property because it combines information
+% regarding the topological organization of the graph with information
+% about the physical placement of connections. Rentian scaling is present
+% in very large scale integrated circuits, the C. elegans neuronal network,
+% and morphometric and diffusion-based graphs of human anatomical networks.
+% Rentian scaling is determined by partitioning the system into cubes,
+% counting the number of nodes inside of each cube (N), and the number of
+% edges traversing the boundary of each cube (E). If the system displays
+% Rentian scaling, these two variables N and E will scale with one another
+% in loglog space. The Rent's exponent is given by the slope of log10(E)
+% vs. log10(N), and can be reported alone or can be compared to the
+% theoretical minimum Rent's exponent to determine how cost efficiently the
+% network has been embedded into physical space. Note: if a system displays
+% Rentian scaling, it does not automatically mean that the system is
+% cost-efficiently embedded (although it does suggest that). Validation
+% occurs when comparing to the theoretical minimum Rent's exponent for that
+% system.
+%
+% INPUTS:
+%
+% A:              MxM adjacency matrix.
+%                 Must be unweighted, binary, and symmetric.
+% XY:             Matrix of node placement coordinates.
+%                 Must be in the form of an Mx2 matrix [x y], where M is the
+%                 number of nodes and x and y are column vectors of node
+%                 coordinates.
+% n:              Number of partitions to compute. Each partition is a data
+%                 point. You want a large enough number to adequately
+%                 estimate the Rent's exponent.
+% tol:            This should be a small value (for example 1e-6).
+%                 In order to mitigate the effects of boundary conditions due
+%                 to the finite size of the network, we only allow partitions
+%                 that are contained within the boundary of the network. This
+%                 is achieved by first computing the volume of the convex
+%                 hull of the node coordinates (V). We then ensure that the
+%                 volume of the convex hull computed on the original node
+%                 coordinates plus the coordinates of the randomly generated
+%                 partition (Vnew) is within a given tolerance of the
+%                 original (i.e. check abs(V - Vnew) < tol). Thus tol, should
+%                 be a small value in order to make sure the partitions are
+%                 contained largely within the boundary of the network, and
+%                 thus the number of nodes and edges within the box are not
+%                 skewed by finite size effects.
+%
+% OUTPUTS:
+%
+% N:              nx1 vector of the number of nodes in each of the n partitions.
+% E:              nx1 vector of the number of edges crossing the boundary of
+%                 each partition.
+%
+% Subsequent Analysis:
+%
+%     Rentian scaling plots are created by: figure; loglog(E,N,'*');
+%
+%     To determine the Rent's exponent, p, we need to determine
+%     the slope of E vs. N in loglog space, which is the Rent's
+%     exponent. There are many ways of doing this with more or less
+%     statistical rigor. Robustfit in MATLAB is one such option:
+%
+%         [b,stats] = robustfit(log10(N),log10(E))
+%
+%     Then the Rent's exponent is b(1,2) and the standard error of the
+%     estimation is given by stats.se(1,2).
+%
+% Note: n=5000 was used in Bassett et al. 2010 in PLoS CB.
+%
+% Reference:
+% Danielle S. Bassett, Daniel L. Greenfield, Andreas Meyer-Lindenberg,
+% Daniel R. Weinberger, Simon W. Moore, Edward T. Bullmore. Efficient
+% physical embedding of topologically complex information processing
+% networks in brains and computer circuits. PLoS Comput Biol, 2010,
+% 6(4):e1000748.
+%
+% Modification History:
+%
+%     2010:     Original (Dani Bassett)
+%     Dec 2016: Updated code so that both partition centers and partition
+%               sizes are chosen at random. Also added in a constraint on
+%               partition placement that prevents boxes from being located
+%               outside the edges of the network. This helps prevent skewed
+%               results due to boundary effects arising from the finite size
+%               of the network. (Lia Papadopoulos)
+
+% determine the number of nodes in the system
+M = numel(XY(:,1));
+
+% rescale coordinates so that they are all greater than unity
+XYn = XY - repmat(min(XY)-1,M,1);
+
+% compute the area of convex hull (i.e. are of the boundary) of the network
+[~,V] = convhull(XYn(:,1),XYn(:,2));
+
+% min and max network coordinates
+xmin = min(XYn(:,1));
+xmax = max(XYn(:,1));
+ymin = min(XYn(:,2));
+ymax = max(XYn(:,2));
+
+% initialize vectors of number of nodes in box and number of edges crossing
+% box
+
+N = zeros(n,1);
+E = zeros(n,1);
+
+% create partitions, and count the number of nodes inside the partition (N)
+% and the number of edges traversing the boundary of the partition (E)
+
+nPartitions = 0;
+
+while nPartitions<(n+1)
+    
+    % variable to check if partition center is within network boundary
+    % OK if inside == 1
+    inside = 0;
+    
+    while inside == 0
+        
+        % pick a random (x,y) coordinate to be the center of the box
+        randx = xmin+(xmax-xmin)*rand(1);
+        randy = ymin+(ymax-ymin)*rand(1);
+        
+        % make sure the point is inside the convex hull of the network
+        newCoords = [XYn; [randx randy]];
+        [~,Vnew] = convhull(newCoords(:,1),newCoords(:,2));
+        
+        % if the old convex hull area and new convex hull area are equal
+        % then the box center must be inside the network boundary.
+        
+        if isequal(V,Vnew)==0
+            inside = 0;
+        else
+            inside = 1;
+        end
+        
+    end
+    
+    % determine the approximate maximum distance the box can extend, given
+    % the center point and the  bounds of the network
+    deltaY = min(abs(ymax-randy),abs(ymin-randy));
+    deltaX = min(abs(xmax-randx),abs(xmin-randx));
+    deltaLmin = min(deltaY,deltaX);
+    
+    % variable to check if partition is within network boundary
+    % OK if inside == 1
+    inside = 0;
+    
+    while inside == 0
+        
+        % pick a random (side length)/2 that is between 0 and the
+        % max possible
+        deltaL = deltaLmin*rand(1);
+        
+        % (x,y) coordinates for corners of box
+        boxCoords = [randx - deltaL randy - deltaL; ...
+            randx - deltaL randy + deltaL; ...
+            randx + deltaL randy - deltaL; ...
+            randx + deltaL randy + deltaL];
+        
+        % check if all corners of box are inside the convex hull of the
+        % network
+        newCoords = [XYn; boxCoords];
+        [~,Vnew] = convhull(newCoords(:,1),newCoords(:,2));
+        
+        % make sure the new convex hull that includes the partition corners
+        % is within a certain tolerance of the original convex hull area.
+        
+        if abs(V-Vnew)>tol
+            inside = 0;
+        else
+            inside = 1;
+        end
+    end
+    
+    % Find nodes inside the box, edges crossing the boundary
+    
+    L = find(XYn(:,1)>(randx-deltaL) & XYn(:,1)<(randx+deltaL) ...
+        & XYn(:,2)>(randy-deltaL) & XYn(:,2)<(randy+deltaL));
+    
+    if ~isempty(L) == 1
+        nPartitions = nPartitions+1;
+        % count edges crossing the boundary of the box
+        E(nPartitions,1) = sum(sum(A(L,setdiff(1:M,L))));
+        % count nodes inside of the box
+        N(nPartitions,1) = numel(L);
+        
+    end
+    
+end
+
+return;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_3d.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_3d.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_3d.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rentian_scaling_3d.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,209 @@
+function [N,E] = rentian_scaling_3d(A,XYZ,n,tol)
+% RENTIAN_SCALING_3D 	Rentian scaling for networks embedded in three dimensions.
+%
+% [N,E] = rentian_scaling_3d(A,XYZ,n,tol)
+%
+% Physical Rentian scaling (or more simply Rentian scaling) is a property
+% of systems that are cost-efficiently embedded into physical space. It is
+% what is called a "topo-physical" property because it combines information
+% regarding the topological organization of the graph with information
+% about the physical placement of connections. Rentian scaling is present
+% in very large scale integrated circuits, the C. elegans neuronal network,
+% and morphometric and diffusion-based graphs of human anatomical networks.
+% Rentian scaling is determined by partitioning the system into cubes,
+% counting the number of nodes inside of each cube (N), and the number of
+% edges traversing the boundary of each cube (E). If the system displays
+% Rentian scaling, these two variables N and E will scale with one another
+% in loglog space. The Rent's exponent is given by the slope of log10(E)
+% vs. log10(N), and can be reported alone or can be compared to the
+% theoretical minimum Rent's exponent to determine how cost efficiently the
+% network has been embedded into physical space. Note: if a system displays
+% Rentian scaling, it does not automatically mean that the system is
+% cost-efficiently embedded (although it does suggest that). Validation
+% occurs when comparing to the theoretical minimum Rent's exponent for that
+% system.
+%
+% INPUTS:
+%
+% A:              MxM adjacency matrix.
+%                 Must be unweighted, binary, and symmetric.
+% XYZ:            Matrix of node placement coordinates.
+%                 Must be in the form of an Mx3 matrix [x y z], where M is
+%                 the number of nodes and x, y, z are column vectors of node
+%                 coordinates.
+% n:              Number of partitions to compute. Each partition is a data
+%                 point. You want a large enough number to adequately
+%                 estimate the Rent's exponent.
+% tol:            This should be a small value (for example 1e-6).
+%                 In order to mitigate the effects of boundary conditions due
+%                 to the finite size of the network, we only allow partitions
+%                 that are contained within the boundary of the network. This
+%                 is achieved by first computing the volume of the convex
+%                 hull of the node coordinates (V). We then ensure that the
+%                 volume of the convex hull computed on the original node
+%                 coordinates plus the coordinates of the randomly generated
+%                 partition (Vnew) is within a given tolerance of the
+%                 original (i.e. check abs(V - Vnew) < tol). Thus tol, should
+%                 be a small value in order to make sure the partitions are
+%                 contained largely within the boundary of the network, and
+%                 thus the number of nodes and edges within the box are not
+%                 skewed by finite size effects.
+%
+% OUTPUTS:
+%
+% N:              nx1 vector of the number of nodes in each of the n partitions.
+% E:              nx1 vector of the number of edges crossing the boundary of
+%                 each partition.
+%
+% Subsequent Analysis:
+%
+%     Rentian scaling plots are created by: figure; loglog(E,N,'*');
+%
+%     To determine the Rent's exponent, p, we need to determine
+%     the slope of E vs. N in loglog space, which is the Rent's
+%     exponent. There are many ways of doing this with more or less
+%     statistical rigor. Robustfit in MATLAB is one such option:
+%
+%         [b,stats] = robustfit(log10(N),log10(E))
+%
+%     Then the Rent's exponent is b(1,2) and the standard error of the
+%     estimation is given by stats.se(1,2).
+%
+% Note: n=5000 was used in Bassett et al. 2010 in PLoS CB.
+%
+% Reference:
+% Danielle S. Bassett, Daniel L. Greenfield, Andreas Meyer-Lindenberg,
+% Daniel R. Weinberger, Simon W. Moore, Edward T. Bullmore. Efficient
+% physical embedding of topologically complex information processing
+% networks in brains and computer circuits. PLoS Comput Biol, 2010,
+% 6(4):e1000748.
+%
+% Modification History:
+%
+%     2010:     Original (Dani Bassett)
+%     Dec 2016: Updated code so that both partition centers and partition
+%               sizes are chosen at random. Also added in a constraint on
+%               partition placement that prevents boxes from being located
+%               outside the edges of the network. This helps prevent skewed
+%               results due to boundary effects arising from the finite size
+%               of the network. (Lia Papadopoulos)
+%
+
+
+% determine the number of nodes in the system
+M = numel(XYZ(:,1));
+
+% rescale coordinates so that they are all greater than unity
+XYZn = XYZ - repmat(min(XYZ)-1,M,1);
+
+% compute the area of convex hull (i.e. are of the boundary) of the network
+[~,V] = convhull(XYZn(:,1),XYZn(:,2),XYZn(:,3));
+
+% min and max network coordinates
+xmin = min(XYZn(:,1));
+xmax = max(XYZn(:,1));
+ymin = min(XYZn(:,2));
+ymax = max(XYZn(:,2));
+zmin = min(XYZn(:,3));
+zmax = max(XYZn(:,3));
+
+% initialize vectors of number of nodes in box and number of edges crossing
+% box
+N = zeros(n,1);
+E = zeros(n,1);
+
+% create partitions, and count the number of nodes inside the partition (N)
+% and the number of edges traversing the boundary of the partition (E)
+nPartitions = 0;
+
+% create partitions, and count the number of nodes inside the partition (N)
+% and the number of edges traversing the boundary of the partition (E)
+
+while nPartitions<(n+1)
+    
+    % variable to check if partition center is within network boundary
+    % OK if inside == 1
+    inside = 0;
+    
+    while inside == 0
+        
+        % pick a random (x,y,z) coordinate to be the center of the box
+        randx = xmin+(xmax-xmin)*rand(1);
+        randy = ymin+(ymax-ymin)*rand(1);
+        randz = zmin+(zmax-zmin)*rand(1);
+        
+        % make sure the point is inside the convex hull of the network
+        newCoords = [XYZn; [randx randy randz]];
+        [~,Vnew] = convhull(newCoords(:,1),newCoords(:,2),newCoords(:,3));
+        
+        % if the old convex hull area and new convex hull area are equal
+        % then the box center must be inside the network boundary.
+        
+        if isequal(V,Vnew)==0
+            inside = 0;
+        else
+            inside = 1;
+        end
+        
+    end
+    
+    % determine the approximate maximum distance the box can extend, given
+    % the center point and the  bounds of the network
+    deltaX = min(abs(xmax-randx),abs(xmin-randx));
+    deltaY = min(abs(ymax-randy),abs(ymin-randy));
+    deltaZ = min(abs(zmax-randz),abs(zmin-randz));
+    deltaLmin = min([deltaX deltaY deltaZ]);
+    
+    % variable to check if partition is within network boundary
+    % OK if inside == 1
+    inside = 0;
+    
+    while inside == 0
+        
+        % pick a random (side length)/2 that is between 0 and the
+        % max possible
+        deltaL = deltaLmin*rand(1);
+        
+        % (x,y,z) coordinates for corners of box
+        boxCoords = [randx - deltaL randy - deltaL randz - deltaL; ...
+            randx - deltaL randy - deltaL randz + deltaL; ...
+            randx - deltaL randy + deltaL randz - deltaL; ...
+            randx - deltaL randy + deltaL randz + deltaL; ...
+            randx + deltaL randy - deltaL randz - deltaL;...
+            randx + deltaL randy - deltaL randz + deltaL; ...
+            randx + deltaL randy + deltaL randz - deltaL; ...
+            randx + deltaL randy + deltaL randz + deltaL];
+        
+        % check if all corners of box are inside the convex hull of the
+        % network
+        newCoords = [XYZn; boxCoords];
+        [~,Vnew] = convhull(newCoords(:,1),newCoords(:,2),newCoords(:,3));
+        
+        % make sure the new convex hull that includes the partition corners
+        % is within a certain tolerance of the original convex hull area.
+        
+        if abs(V-Vnew)>tol
+            inside = 0;
+        else
+            inside = 1;
+        end
+    end
+    
+    % Find nodes inside the box, edges crossing the boundary
+    
+    L = find(XYZn(:,1)>(randx-deltaL) & XYZn(:,1)<(randx+deltaL) ...
+        & XYZn(:,2)>(randy-deltaL) & XYZn(:,2)<(randy+deltaL) ...
+        & XYZn(:,3)>(randz-deltaL) & XYZn(:,3)<(randz+deltaL));
+    
+    if ~isempty(L) == 1
+        nPartitions = nPartitions+1;
+        % count edges crossing the boundary of the cube
+        E(nPartitions,1) = sum(sum(A(L,setdiff(1:M,L))));
+        % count nodes inside of the cube
+        N(nPartitions,1) = numel(L);
+        
+    end
+    
+end
+
+return;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorderMAT.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorderMAT.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorderMAT.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorderMAT.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,64 @@
+function [MATreordered,MATindices,MATcost] = reorderMAT(MAT,H,cost)
+%REORDERMAT         Reorder matrix for visualization
+%
+%   [MATreordered,MATindices,MATcost] = reorderMAT(MAT,H,cost);
+%
+%   This function reorders the connectivity matrix in order to place more
+%   edges closer to the diagonal. This often helps in displaying community
+%   structure, clusters, etc.
+%
+%   Inputs:     MAT,            connection matrix
+%               H,              number of reordering attempts
+%               cost,           'line' or 'circ', for shape of lattice
+%                               (linear or ring lattice)
+%
+%               MATreordered    reordered connection matrix
+%               MATindices      reordered indices
+%               MATcost         cost of reordered matrix
+%   
+%
+%   Olaf Sporns, Indiana University
+
+
+N = length(MAT);
+diagMAT = diag(diag(MAT));
+MAT = MAT-diagMAT;
+
+% generate cost function
+if strcmp(cost,'line')
+    profil = fliplr(normpdf(1:N,0,N/2));
+end;
+if strcmp(cost,'circ')
+    profil = fliplr(normpdf(1:N,N/2,N/4));
+end;
+COST = toeplitz(profil,profil);
+
+% initialize lowCOST
+lowMATcost = sum(sum(COST.*MAT));
+
+% keep track of starting configuration
+MATstart = MAT;
+starta = 1:N;
+   
+% reorder
+for h=1:H
+    a = 1:N;
+    % choose two positions at random and flip them
+    r = randperm(N);
+    a(r(1)) = r(2);
+    a(r(2)) = r(1);
+    MATcostnew = sum(sum(MAT(a,a).*COST));
+    if (MATcostnew < lowMATcost)
+        MAT = MAT(a,a);
+        r2 = starta(r(2));
+        r1 = starta(r(1));
+        starta(r(1)) = r2;
+        starta(r(2)) = r1;
+        lowMATcost = MATcostnew;
+    end;
+end;	% h
+
+MATreordered = MATstart(starta,starta) + diagMAT(starta,starta);
+MATindices = starta;
+MATcost = lowMATcost;
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_matrix.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_matrix.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_matrix.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_matrix.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,105 @@
+function [Mreordered,Mindices,cost] = reorder_matrix(M1,cost,flag)
+% REORDER_MATRIX         Matrix reordering for visualization
+%
+%   [Mreordered,Mindices,cost] = reorder_matrix(M1,cost,flag)
+%
+%   This function rearranges the nodes in matrix M1 such that the matrix
+%   elements are squeezed along the main diagonal.  The function uses a
+%   version of simulated annealing. 
+%
+%   Inputs:     M1             = connection matrix (weighted or binary, 
+%                                directed or undirected)
+%               cost           = 'line' or 'circ', for shape of lattice
+%                                cost (linear or ring lattice)
+%
+%               Mreordered     = reordered connection matrix
+%               Mindices       = reordered indices
+%               cost           = distance between M1 and Mreordered
+%
+%   Note that in general, the outcome will depend on the initial condition
+%   (the setting of the random number seed).  Also, there is no good way to 
+%   determine optimal annealing parameters in advance - these paramters 
+%   will need to be adjusted "by hand" (particularly H, Texp, and T0).  
+%   For large and/or dense matrices, it is highly recommended to perform 
+%   exploratory runs varying the settings of 'H' and 'Texp' and then select 
+%   the best values.
+%
+%   Based on extensive testing, it appears that T0 and Hbrk can remain
+%   unchanged in most cases.  Texp may be varied from 1-1/H to 1-10/H, for
+%   example.  H is the most important parameter - set to larger values as
+%   the problem size increases.  It is advisable to run this function
+%   multiple times and select the solution(s) with the lowest 'cost'.
+%
+%   Setting 'Texp' to zero cancels annealing and uses a greedy algorithm
+%   instead.
+%
+%   Yusuke Adachi, University of Tokyo 2010
+%   Olaf Sporns, Indiana University 2010
+
+N = size(M1,1);
+
+% generate cost function
+if (strcmp(cost,'line'))
+    profil = fliplr(normpdf(1:N,0,N/2));
+end;
+if (strcmp(cost,'circ'))
+    profil = fliplr(normpdf(1:N,N/2,N/4));
+end;
+COST = (toeplitz(profil,profil).*~eye(N));
+COST = COST./sum(sum(COST));
+
+% establish maxcost, lowcost, mincost
+maxcost = sum(sort(COST(:)).*(sort(M1(:))));
+lowcost = sum(sum(M1.*COST))/maxcost;
+mincost = lowcost;
+
+% initialize
+anew = 1:N;
+amin = 1:N;
+h = 0; hcnt = 0;
+
+% set annealing parameters
+% H determines the maximal number of steps
+% Texp determines the steepness of the temperature gradient
+% T0 sets the initial temperature (and scales the energy term)
+% Hbrk sets a break point for the simulation (if no further improvement)
+H = 1e04; Texp = 1-10/H; T0 = 1e-03; Hbrk = H/10;
+%Texp = 0;
+
+while h<H
+    h = h+1; hcnt = hcnt+1;
+    % terminate if no new mincost has been found for some time
+    if (hcnt>Hbrk)
+        break; 
+    end;
+    % current temperature
+    T = T0*Texp^h;
+    % choose two positions at random and flip them
+    atmp = anew;
+    %r = randperm(N);  % slower
+    r = ceil(rand(1,2).*N);
+    atmp(r(1)) = anew(r(2));
+    atmp(r(2)) = anew(r(1));
+    costnew = sum(sum(M1(atmp,atmp).*COST))/maxcost;
+    % annealing
+    if (costnew < lowcost) || (rand < exp(-(costnew-lowcost)/T))
+        anew = atmp;
+        lowcost = costnew;
+        % is this a new absolute best?
+        if (lowcost<mincost)
+            amin = anew;
+            mincost = lowcost;
+            if (flag==1) 
+                disp(['step ',num2str(h),' ... current lowest cost = ',num2str(mincost)]);
+            end;
+            hcnt = 0;
+        end;
+    end;
+end;
+disp(['step ',num2str(h),' ... final lowest cost = ',num2str(mincost)]);
+
+% prepare output
+Mreordered = M1(amin,amin);
+Mindices = amin;
+cost = mincost;
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_mod.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_mod.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_mod.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/reorder_mod.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,107 @@
+function [On,Wr] = reorder_mod(W,M)
+%REORDER_MOD         Reorder connectivity matrix by modular structure
+%
+%   On = reorder_mod(W,M);
+%   [On Wr] = reorder_mod(W,M);
+%
+%   This function reorders the connectivity matrix by modular structure and
+%   may consequently be useful in visualization of modular structure.
+%
+%   Inputs:
+%       W,      connectivity matrix (binary/weighted undirected/directed)
+%       M,      module affiliation vector
+%
+%   Outputs:
+%       On,     new node order
+%       Wr,     reordered connectivity matrix
+%
+%
+%   Used in: Rubinov and Sporns (2011) NeuroImage; Zingg et al. (2014) Cell.
+%
+%
+%   2011, Mika Rubinov, UNSW/U Cambridge
+
+%   Modification History:
+%   Mar 2011: Original
+%	Jan 2015: Improved behavior for directed networks
+
+%#ok<*ASGLU>
+%#ok<*AGROW>
+
+W = W+eps;
+[u,dum,M] = unique(M);                                      %make consecutive;
+n = numel(M);                                               %number of nodes
+m = numel(u);                                               %number of modules
+
+Nm=zeros(1,m);                                              %number of nodes in modules
+Knm_o=zeros(n,m);                                           %node-to-module out-degree
+Knm_i=zeros(n,m);                                           %node-to-module in-degree
+for i=1:m
+    Nm(i)=nnz(M==i);
+    Knm_o(:,i)=sum(W(:,M==i),2);
+    Knm_i(:,i)=sum(W(M==i,:),1);
+end
+Knm=(Knm_o+Knm_i)/2;
+
+Wm=zeros(m);
+for u=1:m
+    for v=1:m
+        Wm(u,v)=sum(sum(W(M==u,M==v)));
+    end
+end
+Bm=(Wm+Wm.')./(2*(Nm.'*Nm));
+% Km_o=sum(Wm,2);
+% Km_i=sum(Wm,1);
+% Bm_oi=Wm-Km_o*Km_i/sum(sum(Wm));
+% Bm=(Bm_oi+Bm_oi.')/2;
+
+%1. Arrange densely connected modules together
+[I,J,bv]=find(tril(Bm,-1));                             	%symmetrized intermodular connectivity values
+[~,ord]=sort(bv,'descend');                                 %sort by greatest relative connectivity
+I=I(ord);
+J=J(ord);
+Om=[I(1) J(1)];                                             %new module order
+
+Si=true(size(I));
+Sj=true(size(J));
+Si(I==I(1) | I==J(1))=0;
+Sj(J==I(1) | J==J(1))=0;
+
+while length(Om)<m                                          %while not all modules ordered
+    for u=1:numel(I)
+        if Si(u)     && any(J(u)==Om([1 end]))
+            old=J(u);
+            new=I(u);
+            break
+        elseif Sj(u) && any(I(u)==Om([1 end]))
+            old=I(u);
+            new=J(u);
+            break
+        end
+    end
+    if old==Om(1)
+        Om=[new Om];
+    elseif old==Om(end)
+        Om=[Om new];
+    end
+    Si(I==new)=false;
+    Sj(J==new)=false;
+end
+
+
+%2. Reorder nodes within modules
+On = zeros(n,1,'uint64');                                   %node order array
+for i=1:m
+    u = Om(i);
+    ind = find(M==u);                                       %indices
+    
+    mod_imp=[Om; sign((1:m)-i); abs((1:m)-i); Bm(u,Om)].';
+    mod_imp=sortrows(mod_imp,[3 -4]);
+    mod_imp=prod(mod_imp(2:end,[1 2]),2);
+    
+    [dum,ord]=sortrows(Knm(ind,:),mod_imp);                 %sort nodes by number of links to close modules
+    On(ind(ord))=1e6*i+(1:Nm(u));                           %assign node order (assumes <1e6 nodes in a module)
+end
+
+[dum,On]=sort(On);                                          %reorder nodes
+Wr=W(On,On);                                                %reorder matrix
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/resource_efficiency_bin.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/resource_efficiency_bin.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/resource_efficiency_bin.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/resource_efficiency_bin.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,140 @@
+function [Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL,M)
+% RESOURCE_EFFICIENCY_BIN       Resource efficiency and shortest-path probability
+%
+%   [Eres,prob_SPL] = resource_efficiency_bin(adj,lambda,SPL,M)
+%
+%   The resource efficiency between nodes i and j is inversly proportional
+%   to the amount of resources (i.e. number of particles or messages)
+%   required to ensure with probability 0 < lambda < 1 that at least one of
+%   them will arrive at node j in exactly SPL steps, where SPL is the
+%   length of the shortest-path between i and j.
+%
+%   The shortest-path probability between nodes i and j is the probability
+%   that a single random walker starting at node i will arrive at node j by
+%   following (one of) the shortest path(s).
+%
+%   Inputs:
+%
+%       adj,
+%           Unweighted, undirected adjacency matrix
+%
+%       lambda,
+%           Probability (0 < lambda < 1)
+%          	set to NAN if computation of Eres is not desired
+%
+%       SPL,
+%           Shortest-path length matrix (optional)
+%
+%     	M,
+%           Transition probability matrix (optional)
+%
+%
+%   Outputs:
+%
+%       Eres,
+%           Resource efficiency matrix.
+%
+%       prob_SPL,
+%           Shortest-path probability matrix
+%
+%   Notes:
+%
+%       Global measures for both Eres and prob_SPL are well defined and can
+%       be computed as the average across the off-diagonal elements:
+%           GEres = mean(Eres(~eye(N)>0));
+%           Gprob_SPL = mean(rob_SPL(~eye(N)>0));
+%
+%
+%   Reference: Goi J, et al (2013) PLoS ONE
+%
+%
+%   Joaquin Goi, IU Bloomington, 2012
+
+
+N = size(adj,1);
+EYE = logical(eye(N,N));
+
+flagResources = ~isnan(lambda);
+
+if flagResources
+    if lambda<=0 || lambda>=1
+        error('p_req_values must be non-zero probabilities')
+    end
+    z = zeros(N,N);
+end
+if nargin<4
+    SPL = distance_wei_floyd(adj);
+end
+if nargin<5
+    M = diag(sum(adj,2))\adj;
+end
+
+Lvalues =  unique(SPL(:));
+Lvalues = Lvalues(~(Lvalues==0));
+
+prob_SPL = zeros(N,N);         % a priori zero probability of going through SPL among nodes
+
+for indexSPL=1:length(Lvalues) %for each possible value of SPL for current component
+    SPLvalue = Lvalues(indexSPL);
+    [~, hcols] = find(SPL==SPLvalue);
+    hvector = unique(hcols); clear hrows hcols
+    entries = SPL==SPLvalue;
+    
+    if flagResources  % compute Eres
+        [prob_aux,z_aux] = prob_first_particle_arrival(M,SPLvalue,hvector,lambda);
+    else              % not compute Eres
+        [prob_aux] = prob_first_particle_arrival(M,SPLvalue,hvector,[]);
+    end
+    
+    prob_aux(~entries) = 0;
+    prob_SPL = prob_SPL + prob_aux;
+    
+    if flagResources
+        z_aux(~entries) = 0;
+        z = z + z_aux;
+    end
+end
+
+prob_SPL(EYE) = 0;
+
+if flagResources
+    z(prob_SPL==1) = 1;
+    Eres = 1./z;
+    Eres(EYE) = 0;
+else
+    Eres =  nan;
+end
+
+function [prob,resources] = prob_first_particle_arrival(M,L,hvector,lambda)
+
+N = size(M,1);
+prob = zeros(N,N);
+
+if nargin<4
+    hvector=1:N;
+end
+
+flagResources = ~isnan(lambda);
+
+if flagResources
+    if lambda<=0 || lambda>=1
+        error('p_req_values must be non-zero probabilities')
+    end
+    resources = zeros(N,N);
+end
+
+for hindex=1:length(hvector)          %for each destination node h
+    h = hvector(hindex);
+    B_h = M;
+    B_h(h,:) = 0; B_h(h,h) = 1;       % h becomes absorbant state.
+    
+    B_h_L = B_h^L;
+    
+    term = 1-B_h_L(:,h);
+    
+    prob(:,h)= 1-term;
+    
+    if flagResources
+        resources(:,h) = repmat(log(1-lambda),N,1)./repmat(log(term),1);
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/retrieve_shortest_path.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/retrieve_shortest_path.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/retrieve_shortest_path.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/retrieve_shortest_path.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,37 @@
+function path = retrieve_shortest_path(s,t,hops,Pmat)
+% RETRIEVE_SHORTEST_PATH        Retrieval of shortest path
+%
+%   This function finds the sequence of nodes that comprise the shortest
+%   path between a given source and target node.
+%
+%   Inputs:
+%       s,
+%           Source node: i.e. node where the shortest path begins. 
+%    	t,
+%           Target node: i.e. node where the shortest path ends.
+%       hops,
+%           Number of edges in the path. This matrix may be obtained as the
+%           second output argument of the function "distance_wei_floyd.m".
+%       Pmat,
+%           Pmat is a matrix whose elements {k,t} indicate the next node in
+%           the shortest path between k and t. This matrix may be obtained
+%           as the third output of the function "distance_wei_floyd.m"
+%
+%   Output:
+%       path,
+%           Nodes comprising the shortest path between nodes s and t.
+%
+%
+%   Andrea Avena-Koenigsberger and Joaquin Goi, IU, 2012
+
+path_length = hops(s,t);
+if path_length ~= 0
+    path = nan(path_length+1,1);
+    path(1) = s;
+    for ind = 2:length(path)
+        s = Pmat(s,t);
+        path(ind) = s;
+    end
+else
+    path = [];
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bd.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,53 @@
+function [R,Nk,Ek] = rich_club_bd(CIJ,varargin)
+%RICH_CLUB_BD        Rich club coefficients (binary directed graph)
+%
+%   R = rich_club_bd(CIJ)
+%   [R,Nk,Ek] = rich_club_bd(CIJ,klevel)
+%
+%   The rich club coefficient, R, at level k is the fraction of edges that
+%   connect nodes of degree k or higher out of the maximum number of edges
+%   that such nodes might share.
+%
+%   Input:      CIJ,        connection matrix, binary and directed
+%            klevel,        optional input argument. klevel sets the
+%                              maximum level at which the rich club
+%                              coefficient will be calculated. If klevel is
+%                              not included the the maximum level will be
+%                              set to the maximum degree of CIJ.
+%
+%   Output:       R,        vector of rich-club coefficients for levels
+%                              1 to klevel.
+%                Nk,        number of nodes with degree>k
+%                Ek,        number of edges remaining in subgraph with
+%                              degree>k
+%
+%   Reference: Colizza et al. (2006) Nat. Phys. 2:110.
+%
+%   Martijn van den Heuvel, University Medical Center Utrecht, 2011
+
+N = size(CIJ,1);                    %#ok<NASGU>
+
+% definition of "degree" as used for RC coefficients
+% degree is taken to be the sum of incoming and outgoing connectons
+[~,~,degree] = degrees_dir(CIJ);
+
+if nargin == 1
+    klevel = max(degree);
+elseif nargin == 2
+    klevel = varargin{1};
+elseif nargin > 2
+    error('number of inputs incorrect. Should be [CIJ], or [CIJ, klevel]')
+end
+
+R = zeros(1,klevel);
+Nk = zeros(1,klevel);
+Ek = zeros(1,klevel);
+for k = 1:klevel
+    SmallNodes=find(degree<=k);       %get 'small nodes' with degree <=k
+    subCIJ=CIJ;                       %extract subnetwork of nodes >k by removing nodes <=k of CIJ
+    subCIJ(SmallNodes,:)=[];          %remove rows
+    subCIJ(:,SmallNodes)=[];          %remove columns
+    Nk(k)=size(subCIJ,2);             %number of nodes with degree >k
+    Ek(k)=sum(subCIJ(:));             %total number of connections in subgraph
+    R(k)=Ek(k)/(Nk(k)*(Nk(k)-1));     %unweighted rich-club coefficient
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_bu.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,49 @@
+function [R,Nk,Ek] = rich_club_bu(CIJ,varargin)
+%RICH_CLUB_BU        Rich club coefficients (binary undirected graph)
+%
+%   R = rich_club_bu(CIJ)
+%   [R,Nk,Ek] = rich_club_bu(CIJ,klevel)
+%
+%   The rich club coefficient, R, at level k is the fraction of edges that
+%   connect nodes of degree k or higher out of the maximum number of edges
+%   that such nodes might share.
+%
+%   Input:      CIJ,        connection matrix, binary and undirected
+%            klevel,        optional input argument. klevel sets the
+%                              maximum level at which the rich club
+%                              coefficient will be calculated. If klevel is
+%                              not included the the maximum level will be
+%                              set to the maximum degree of CIJ.
+%
+%   Output:       R,        vector of rich-club coefficients for levels
+%                              1 to klevel.
+%                Nk,        number of nodes with degree>k
+%                Ek,        number of edges remaining in subgraph with
+%                              degree>k
+%
+%   Reference: Colizza et al. (2006) Nat. Phys. 2:110.
+%
+%   Martijn van den Heuvel, University Medical Center Utrecht, 2011
+
+Degree = sum(CIJ);  %compute degree of each node
+
+if nargin == 1
+    klevel = max(Degree);
+elseif nargin == 2
+    klevel = varargin{1};
+elseif nargin > 2
+    error('number of inputs incorrect. Should be [CIJ], or [CIJ, klevel]')
+end
+
+R = zeros(1,klevel);
+Nk = zeros(1,klevel);
+Ek = zeros(1,klevel);
+for k = 1:klevel
+    SmallNodes=find(Degree<=k);       %get 'small nodes' with degree <=k
+    subCIJ=CIJ;                       %extract subnetwork of nodes >k by removing nodes <=k of CIJ
+    subCIJ(SmallNodes,:)=[];          %remove rows
+    subCIJ(:,SmallNodes)=[];          %remove columns
+    Nk(k)=size(subCIJ,2);             %number of nodes with degree >k
+    Ek(k)=sum(subCIJ(:));             %total number of connections in subgraph
+    R(k)=Ek(k)/(Nk(k)*(Nk(k)-1));     %unweighted rich-club coefficient
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,75 @@
+function   [Rw] = rich_club_wd(CIJ,varargin)
+%RICH_CLUB_WD	Rich club coefficients curve (weighted directed graph)
+%
+%   Rw = rich_club_wd(CIJ,varargin)
+%
+%   The weighted rich club coefficient, Rw, at level k is the fraction of
+%   edge weights that connect nodes of degree k or higher out of the
+%   maximum edge weights that such nodes might share.
+%
+%   Inputs:
+%       CIJ:        weighted directed connection matrix
+%
+%       k-level:    (optional) max level of RC(k).
+%                   (by default k-level quals the maximal degree of CIJ)
+%
+%   Output:
+%       Rw:         rich-club curve
+%
+%
+%   References:
+%       T Opsahl et al. Phys Rev Lett, 2008, 101(16)
+%       M van den Heuvel, O Sporns, J Neurosci 2011 31(44)
+%
+%   Martijn van den Heuvel, University Medical Center Utrecht, 2011
+
+%   Modification History:
+%   2011: Original
+%   2015: Expanded documentation (Mika Rubinov)
+
+
+NofNodes = size(CIJ,2); %#ok<NASGU>        %number of nodes
+NodeDegree = sum((CIJ~=0))+sum((CIJ'~=0)); %define degree of each node (indegree + outdegree)
+
+%define to which level rc should be computed
+if size(varargin,2)==1
+    klevel = varargin{1};
+elseif isempty(varargin)
+    klevel = max(NodeDegree);
+else
+    error('number of inputs incorrect. Should be [CIJ], or [CIJ, klevel]')
+end
+
+
+%wrank contains the ranked weights of the network, with strongest connections on top
+
+wrank = sort(CIJ(:), 'descend');
+
+%loop over all possible k-levels
+for kk = 1:klevel
+    
+    SmallNodes=find(NodeDegree<kk);
+    
+    if isempty(SmallNodes)
+        Rw(kk)=NaN;         %#ok<*AGROW>
+        continue
+    end
+    
+    %remove small nodes with NodeDegree<kk
+    CutoutCIJ=CIJ;
+    CutoutCIJ(SmallNodes,:)=[];
+    CutoutCIJ(:,SmallNodes)=[];
+    
+    %total weight of connections in subset E>r
+    Wr = sum(CutoutCIJ(:));
+    
+    %total number of connections in subset E>r
+    Er = length(find(CutoutCIJ~=0));
+    
+    %E>r number of connections with max weight in network
+    wrank_r = wrank(1:1:Er);
+    
+    %weighted rich-club coefficient
+    Rw(kk)=Wr / sum(wrank_r);
+    
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rich_club_wu.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,72 @@
+function   [Rw] = rich_club_wu(CIJ,varargin)
+%RICH_CLUB_WU 	Rich club coefficients curve (weighted undirected graph)
+%
+%   Rw = rich_club_wu(CIJ,varargin) % rich club curve for weighted graph
+%
+%   The weighted rich club coefficient, Rw, at level k is the fraction of
+%   edge weights that connect nodes of degree k or higher out of the
+%   maximum edge weights that such nodes might share.
+%
+%   Inputs:
+%       CIJ:        weighted directed connection matrix
+%
+%       k-level:    (optional) max level of RC(k).
+%                   (by default k-level quals the maximal degree of CIJ)
+%
+%   Output:
+%       Rw:         rich-club curve
+%
+%
+%   References:
+%       T Opsahl et al. Phys Rev Lett, 2008, 101(16)
+%       M van den Heuvel, O Sporns, J Neurosci 2011 31(44)
+%
+%   Martijn van den Heuvel, University Medical Center Utrecht, 2011
+
+%   Modification History:
+%   2011: Original
+%   2015: Expanded documentation (Mika Rubinov)
+
+
+NofNodes = size(CIJ,2);     %#ok<NASGU> %number of nodes
+NodeDegree = sum((CIJ~=0)); %define degree of each node
+
+%define to which level rc should be computed
+if size(varargin,2)==1
+    klevel = varargin{1};
+elseif isempty(varargin)
+    klevel = max(NodeDegree);
+else
+    error('number of inputs incorrect. Should be [CIJ], or [CIJ, klevel]')
+end
+
+%wrank contains the ranked weights of the network, with strongest connections on top
+wrank = sort(CIJ(:), 'descend');
+
+%loop over all possible k-levels
+for kk = 1:klevel
+    
+    SmallNodes=find(NodeDegree<kk);
+    
+    if isempty(SmallNodes)
+        Rw(kk)=NaN;             %#ok<*AGROW>
+        continue
+    end
+    
+    %remove small nodes with NodeDegree<kk
+    CutoutCIJ=CIJ;
+    CutoutCIJ(SmallNodes,:)=[];
+    CutoutCIJ(:,SmallNodes)=[];
+    
+    %total weight of connections in subset E>r
+    Wr = sum(CutoutCIJ(:));
+    
+    %total number of connections in subset E>r
+    Er = length(find(CutoutCIJ~=0));
+    
+    %E>r number of connections with max weight in network
+    wrank_r = wrank(1:1:Er);
+    
+    %weighted rich-club coefficient
+    Rw(kk)=Wr / sum(wrank_r);
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rout_efficiency.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rout_efficiency.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/rout_efficiency.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/rout_efficiency.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,87 @@
+function [GErout,Erout,Eloc] = rout_efficiency(D,transform)
+% ROUT_EFFICIENCY       Mean, pair-wise and local routing efficiency
+%
+%   [GErout,Erout,Eloc] = rout_efficiency(D,transform);
+%
+%   The routing efficiency is the average of inverse shortest path length.
+%
+%   The local routing efficiency of a node u is the routing efficiency
+%   computed on the subgraph formed by the neighborhood of node u
+%   (excluding node u).
+%
+%
+%   Inputs:
+%
+%       D,
+%           Weighted/unweighted directed/undirected
+%           connection *weight* OR *length* matrix.
+%
+%       transform,
+%           If the input matrix is a connection *weight* matrix, specify a
+%           transform that map input connection weights to connection
+%           lengths. Two transforms are available.
+%               'log' -> l_ij = -log(w_ij)
+%               'inv' -> l_ij =    1/w_ij
+%
+%           If the input matrix is a connection *length* matrix, do not
+%           specify a transform (or specify an empty transform argument).
+%
+%
+%   Outputs:
+%
+%       GErout,
+%           Mean global routing efficiency (scalar).
+%
+%   	Erout,
+%           Pair-wise routing efficiency (matrix).
+%
+%    	Eloc,
+%           Local efficiency (vector)
+%
+%
+%   Note:
+%
+%       The input matrix may be either a connection weight matrix, or a
+%       connection length matrix. The connection length matrix is typically
+%       obtained with a mapping from weight to length, such that higher
+%       weights are mapped to shorter lengths (see above).
+%
+%
+%   Algorithm:  FloydWarshall Algorithm
+%
+%
+%   References:
+%       Latora and Marchiori (2001) Phys Rev Lett
+%       Goi et al (2013) PLoS ONE
+%       Avena-Koenigsberger et al (2016) Brain Structure and Function
+%
+%
+%   Andrea Avena-Koenigsberger and Joaquin Goi, IU Bloomington, 2012
+%
+
+%   Modification history
+%   2012 - original
+%   2016 - included comutation of local efficiency
+%   2016 - included transform variable that maps strengths onto distances
+
+
+if ~exist('transform','var')
+    transform = [];
+end
+
+n=length(D);                                            % number of nodes
+
+Erout = distance_wei_floyd(D,transform);               	% pair-wise routing efficiency
+Erout = 1./Erout;
+Erout(eye(n)>0) = 0;
+GErout = sum(Erout(~eye(n)>0))/(n^2-n);                 % global routing efficiency
+
+if nargout == 3
+    Eloc = zeros(n,1);
+    for u = 1:n
+        Gu = find(D(u,:) | D(:,u).');                 	% u's neighbors
+        nGu = length(Gu);
+        e = distance_wei_floyd(D(Gu,Gu),transform);
+        Eloc(u) = sum(sum(1./e(~eye(nGu)>0)))/nGu;     	% efficiency of subgraph Gu
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/score_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/score_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/score_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/score_wu.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,40 @@
+function [CIJscore,sn] = score_wu(CIJ,s)
+%SCORE_WU       S-score
+%
+%   [CIJscore,sn] = score_wu(CIJ,s);
+%
+%   The s-core is the largest subnetwork comprising nodes of strength at
+%   least s. This function computes the s-core for a given weighted
+%   undirected connection matrix. Computation is analogous to the more
+%   widely used k-core, but is based on node strengths instead of node
+%   degrees. 
+%
+%   input:          CIJ,	connection/adjacency matrix (weighted, undirected)
+%                     s,    level of s-core. Note: s can take on any fractional value
+%
+%   output:    CIJscore,    connection matrix of the s-core.  This matrix 
+%                           contains only nodes with a strength of at least s.
+%                    sn,    size of s-score
+%
+%   Olaf Sporns, Indiana University, 2007/2008/2010/2012
+
+while 1
+
+    % get strengths of matrix
+    [str] = strengths_und(CIJ);
+
+    % find nodes with strength <s
+    ff = find((str<s)&(str>0));
+    
+    % if none found -> stop
+    if (isempty(ff)) break; end;            %#ok<SEPEX>
+
+    % peel found nodes
+    CIJ(ff,:) = 0;
+    CIJ(:,ff) = 0;
+
+end;
+
+CIJscore = CIJ;
+sn = sum(str>0);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/search_information.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/search_information.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/search_information.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/search_information.m	2019-03-03 21:12:00.000000000 +0100
@@ -0,0 +1,117 @@
+function SI = search_information(W, L, has_memory)
+% SEARCH_INFORMATION                    Search information
+%
+%   SI = search_information(W, L,has_memory)
+%
+%   Computes the amount of information (measured in bits) that a random
+%   walker needs to follow the shortest path between a given pair of nodes.
+%
+%   Inputs:
+%
+%       W
+%           Weighted/unweighted directed/undirected
+%           connection weight matrix.
+%
+%       L
+%           Weighted/unweighted directed/undirected
+%           connection length matrix.
+%
+%      	has_memory,
+%           This flag defines whether or not the random walker "remembers"
+%           its previous step, which has the effect of reducing the amount
+%           of information needed to find the next state. If this flag is
+%           not set, the walker has no memory by default.
+%
+%
+%   Outputs:
+%
+%       SI,
+%           pair-wise search information (matrix). Note that SI(i,j) may be
+%           different from SI(j,i), hense, SI is not a symmetric matrix
+%           even when adj is symmetric.
+%
+%
+%   References: Rosvall et al. (2005) Phys Rev Lett 94, 028701
+%               Goi et al (2014) PNAS doi: 10.1073/pnas.131552911
+%
+%
+%   Andrea Avena-Koenigsberger and Joaquin Goi, IU Bloomington, 2014
+%   Caio Seguin, University of Melbourne, 2019
+
+%   Modification history
+%   2014 - original
+%   2016 - included SPL transform option and generalized for
+%          symmetric/asymmetric networks
+%   2019 - modified to make user directly specify  weight-to-length transformations
+
+
+if ~exist('has_memory','var')
+    has_memory = false;
+end
+
+N = size(W,1);
+
+if issymmetric(W)
+    flag_triu = true;
+else
+    flag_triu = false;
+end
+
+T = diag(sum(W,2))\W;
+[~,hops,Pmat] = distance_wei_floyd(L,[]); % Compute shortest paths based on L
+
+SI = zeros(N,N);
+SI(eye(N)>0) = nan;
+
+for i = 1:N
+    for j = 1:N
+        if (j > i && flag_triu) || (~flag_triu && i ~= j)
+            path = retrieve_shortest_path(i,j,hops,Pmat);
+            lp = length(path);
+            if flag_triu
+                if ~isempty(path)
+                    pr_step_ff = nan(1,lp-1);
+                    pr_step_bk = nan(1,lp-1);
+                    if has_memory
+                        pr_step_ff(1) = T(path(1),path(2));
+                        pr_step_bk(lp-1) = T(path(lp),path(lp-1));
+                        for z=2:lp-1
+                            pr_step_ff(z) = T(path(z),path(z+1))/(1 - T(path(z-1),path(z)));
+                            pr_step_bk(lp-z) = T(path(lp-z+1),path(lp-z))/(1 - T(path(lp-z+2),path(lp-z+1)));
+                        end
+                    else
+                        for z=1:length(path)-1
+                            pr_step_ff(z) = T(path(z),path(z+1));
+                            pr_step_bk(z) = T(path(z+1),path(z));
+                        end
+                    end
+                    prob_sp_ff = prod(pr_step_ff);
+                    prob_sp_bk = prod(pr_step_bk);
+                    SI(i,j) = -log2(prob_sp_ff);
+                    SI(j,i) = -log2(prob_sp_bk);
+                else
+                    SI(i,j) = inf;
+                    SI(j,i) = inf;
+                end
+            else
+                if ~isempty(path)
+                    pr_step_ff = nan(1,lp-1);
+                    if has_memory
+                        pr_step_ff(1) = T(path(1),path(2));
+                        for z=2:lp-1
+                            pr_step_ff(z) = T(path(z),path(z+1))/(1 - T(path(z-1),path(z)));
+                        end
+                    else
+                        for z=1:length(path)-1
+                            pr_step_ff(z) = T(path(z),path(z+1));
+                        end
+                    end
+                    prob_sp_ff = prod(pr_step_ff);
+                    SI(i,j) = -log2(prob_sp_ff);
+                else
+                    SI(i,j) = inf;
+                end
+            end
+        end
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_dir.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_dir.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_dir.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_dir.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,27 @@
+function [is,os,str] = strengths_dir(CIJ)
+%STRENGTHS_DIR      In-strength and out-strength
+%
+%   [is,os,str] = strengths_dir(CIJ);
+%
+%   Node strength is the sum of weights of links connected to the node. The
+%   instrength is the sum of inward link weights and the outstrength is the
+%   sum of outward link weights.
+%
+%   Input:      CIJ,    directed weighted connection matrix
+%
+%   Output:     is,     node instrength
+%               os,     node outstrength
+%               str,    node strength (instrength + outstrength)
+%
+%   Notes:  Inputs are assumed to be on the columns of the CIJ matrix.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2006/2008
+
+
+% compute strengths
+is = sum(CIJ,1);    % instrength = column sum of CIJ
+os = sum(CIJ,2)';   % outstrength = row sum of CIJ
+str = is+os;        % strength = instrength+outstrength
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,18 @@
+function [str] = strengths_und(CIJ)
+%STRENGTHS_UND        Strength
+%
+%   str = strengths_und(CIJ);
+%
+%   Node strength is the sum of weights of links connected to the node.
+%
+%   Input:      CIJ,    undirected weighted connection matrix
+%
+%   Output:     str,    node strength
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2006/2008
+
+% compute strengths
+str = sum(CIJ);        % strength
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und_sign.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und_sign.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und_sign.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/strengths_und_sign.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,30 @@
+function [Spos,Sneg,vpos,vneg] = strengths_und_sign(W)
+%STRENGTHS_UND_SIGN        Strength and weight
+%
+%   [Spos Sneg] = strengths_und_sign(W);
+%   [Spos Sneg vpos vneg] = strengths_und_sign(W);
+%
+%   Node strength is the sum of weights of links connected to the node.
+%
+%   Inputs:     W,              undirected connection matrix with positive
+%                               and negative weights
+%
+%   Output:     Spos/Sneg,      nodal strength of positive/negative weights
+%               vpos/vneg,      total positive/negative weight
+%
+%
+%   2011, Mika Rubinov, UNSW
+
+%   Modification History:
+%   Mar 2011: Original
+
+
+n = length(W);              %number of nodes
+W(1:n+1:end) = 0;           %clear diagonal
+Spos = sum( W.*(W>0));      %positive strengths
+Sneg = sum(-W.*(W<0));      %negative strengths
+
+if nargout>2
+    vpos = sum(Spos);       %positive weight
+    vneg = sum(Sneg);       %negative weight
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/subgraph_centrality.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/subgraph_centrality.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/subgraph_centrality.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/subgraph_centrality.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,24 @@
+function Cs = subgraph_centrality(CIJ)
+% SUBGRAPH_CENTRALITY     Subgraph centrality of a network
+%
+%   Cs = subgraph_centrality(CIJ)
+%
+%   The subgraph centrality of a node is a weighted sum of closed walks of
+%   different lengths in the network starting and ending at the node. This
+%   function returns a vector of subgraph centralities for each node of the
+%   network.
+%
+%   Inputs:     CIJ,        adjacency matrix (binary)
+%
+%   Outputs:     Cs,        subgraph centrality
+%
+%   Reference: Estrada and Rodriguez-Velasquez (2005) Phys Rev E 71, 056103
+%              Estrada and Higham (2010) SIAM Rev 52, 696.
+%
+%   Xi-Nian Zuo, Chinese Academy of Sciences, 2010
+%   Rick Betzel, Indiana University, 2012
+
+[V,lambda] = eig(CIJ);                 % Compute the eigenvectors and
+lambda     = diag(lambda);             % eigenvalues.
+V2         = V.^2;                     % Matrix of squares of the eigenvectors elements.
+Cs         = real(V2 * exp(lambda));   % Compute eigenvector centrality. Lop off imaginary part remaining due to precision error.
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_absolute.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_absolute.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_absolute.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_absolute.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,19 @@
+function W = threshold_absolute(W, thr)
+% THRESHOLD_ABSOLUTE    Absolute thresholding
+% 
+%   W_thr = threshold_absolute(W, thr);
+%
+%   This function thresholds the connectivity matrix by absolute weight
+%   magnitude. All weights below the given threshold, and all weights
+%   on the main diagonal (self-self connections) are set to 0.
+%
+%   Inputs: W           weighted or binary connectivity matrix
+%           thr         weight treshold
+%
+%   Output: W_thr       thresholded connectivity matrix
+%
+%
+%   Mika Rubinov, UNSW, 2009-2010
+
+W(1:size(W,1)+1:end)=0;                 %clear diagonal
+W(W<thr)=0;                             %apply threshold
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_proportional.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_proportional.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_proportional.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/threshold_proportional.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,45 @@
+function W = threshold_proportional(W, p)
+%THRESHOLD_PROPORTIONAL     Proportional thresholding
+%
+%   W_thr = threshold_proportional(W, p);
+%
+%   This function "thresholds" the connectivity matrix by preserving a
+%   proportion p (0<p<1) of the strongest weights. All other weights, and
+%   all weights on the main diagonal (self-self connections) are set to 0.
+%
+%   Inputs: W,      weighted or binary connectivity matrix
+%           p,      proportion of weights to preserve
+%                       range:  p=1 (all weights preserved) to
+%                               p=0 (no weights preserved)
+%
+%   Output: W_thr,  thresholded connectivity matrix
+%
+%
+%   Mika Rubinov, U Cambridge,
+%   Roan LaPlante, Martinos Center, MGH
+%   Zitong Zhang, Penn Engineering
+
+%   Modification history:
+%   2010: Original (MR)
+%   2012: Bug fix for symmetric matrices (RLP)
+%   2015: Improved symmetricity test (ZZ)
+
+n=size(W,1);                                %number of nodes
+W(1:n+1:end)=0;                             %clear diagonal
+
+if max(max(abs(W-W.'))) < 1e-10             %if symmetric matrix
+    W=triu(W);                              %ensure symmetry is preserved
+    ud=2;                                   %halve number of removed links
+else
+    ud=1;
+end
+
+ind=find(W);                                %find all links
+E=sortrows([ind W(ind)], -2);               %sort by magnitude
+en=round((n^2-n)*p/ud);                     %number of links to be preserved
+
+W(E(en+1:end,1))=0;                         %apply threshold
+
+if ud==2                                    %if symmetric matrix
+    W=W+W.';                                %reconstruct symmetry
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bd.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,41 @@
+function T=transitivity_bd(A)
+%TRANSITIVITY_BD    Transitivity
+%
+%   T = transitivity_bd(A);
+%
+%   Transitivity is the ratio of 'triangles to triplets' in the network.
+%   (A classical version of the clustering coefficient).
+%
+%   Input:      A       binary directed connection matrix
+%
+%   Output:     T       transitivity scalar
+%
+%   Reference:  Rubinov M, Sporns O (2010) NeuroImage 52:1059-69
+%               based on Fagiolo (2007) Phys Rev E 76:026107.
+%
+%
+%   Contributors:
+%   Mika Rubinov, UNSW/University of Cambridge
+%   Christoph Schmidt, Friedrich Schiller University Jena
+%   Andrew Zalesky, University of Melbourne
+%   2007-2015
+
+%   Modification history:
+%   2007: original (MR)
+%   2013, 2015: removed tests for absence of nodewise 3-cycles (CS,AZ)
+
+%   Methodological note: In directed graphs, 3 nodes generate up to 8 
+%   triangles (2*2*2 edges). The number of existing triangles is the main 
+%   diagonal of S^3/2. The number of all (in or out) neighbour pairs is 
+%   K(K-1)/2. Each neighbour pair may generate two triangles. "False pairs"
+%   are i<->j edge pairs (these do not generate triangles). The number of 
+%   false pairs is the main diagonal of A^2. Thus the maximum possible 
+%   number of triangles = (2 edges)*([ALL PAIRS] - [FALSE PAIRS])
+%                       = 2 * (K(K-1)/2 - diag(A^2))
+%                       = K(K-1) - 2(diag(A^2))
+
+S    = A+A.';                           % symmetrized input graph
+K    = sum(S,2);                        % total degree (in + out)
+cyc3 = diag(S^3)/2;                     % number of 3-cycles (ie. directed triangles)
+CYC3 = K.*(K-1)-2*diag(A^2);            % number of all possible 3-cycles
+T    = sum(cyc3)./sum(CYC3);            % transitivity
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_bu.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,20 @@
+function [C_tri]=transitivity_bu(A)
+%TRANSITIVITY_BU    Transitivity
+%
+%   T = transitivity_bu(A);
+%
+%   Transitivity is the ratio of 'triangles to triplets' in the network.
+%   (A classical version of the clustering coefficient).
+%
+%   Input:      A       binary undirected connection matrix
+%
+%   Output:     T       transitivity scalar
+%
+%   Reference: e.g. Humphries et al. (2008) Plos ONE 3: e0002051
+%
+%
+%   Alexandros Goulas, Maastricht University, 2010
+
+    C_tri = trace(A^3) / (sum(sum(A^2)) - trace(A^2));
+
+return;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wd.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wd.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wd.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wd.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,47 @@
+function T=transitivity_wd(W)
+%TRANSITIVITY_WD    Transitivity
+%
+%   T = transitivity_wd(W);
+%
+%   Transitivity is the ratio of 'triangles to triplets' in the network.
+%   (A classical version of the clustering coefficient).
+%
+%   Input:      W       weighted directed connection matrix
+%
+%   Output:     T       transitivity scalar
+%
+%   Note:       All weights must be between 0 and 1.
+%               This may be achieved using the weight_conversion.m function,
+%               W_nrm = weight_conversion(W, 'normalize');
+%
+%   Reference:  Rubinov M, Sporns O (2010) NeuroImage 52:1059-69
+%               based on Fagiolo (2007) Phys Rev E 76:026107.
+%
+%
+%   Contributors:
+%   Mika Rubinov, UNSW/University of Cambridge
+%   Christoph Schmidt, Friedrich Schiller University Jena
+%   Andrew Zalesky, University of Melbourne
+%   2007-2015
+
+%   Modification history:
+%   2007: original (MR)
+%   2013, 2015: removed tests for absence of nodewise 3-cycles (CS,AZ)
+%   2015: Expanded documentation
+
+
+%   Methodological note (also see note for clustering_coef_bd)
+%   The weighted modification is as follows:
+%   - The numerator: adjacency matrix is replaced with weights matrix ^ 1/3
+%   - The denominator: no changes from the binary version
+%
+%   The above reduces to symmetric and/or binary versions of the clustering
+%   coefficient for respective graphs.
+
+A    = W~=0;                        % adjacency matrix
+S    = W.^(1/3)+(W.').^(1/3);      	% symmetrized weights matrix ^1/3
+K    = sum(A+A.',2);               	% total degree (in + out)
+cyc3 = diag(S^3)/2;                	% number of 3-cycles (ie. directed triangles)
+CYC3 = K.*(K-1)-2*diag(A^2);        % number of all possible 3-cycles
+T    = sum(cyc3)./sum(CYC3);       	% transitivity
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wu.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wu.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/transitivity_wu.m	2019-03-03 19:42:48.000000000 +0100
@@ -0,0 +1,29 @@
+function T=transitivity_wu(W)
+%TRANSITIVITY_WU    Transitivity
+%
+%   T = transitivity_wu(W);
+%
+%   Transitivity is the ratio of 'triangles to triplets' in the network.
+%   (A classical version of the clustering coefficient).
+%
+%   Input:      W       weighted undirected connection matrix
+%
+%   Output:     T       transitivity scalar
+%
+%   Note:      All weights must be between 0 and 1.
+%              This may be achieved using the weight_conversion.m function,
+%              W_nrm = weight_conversion(W, 'normalize');
+%
+%   Reference: Rubinov M, Sporns O (2010) NeuroImage 52:1059-69
+%              based on Onnela et al. (2005) Phys Rev E 71:065103
+%
+%
+%   Mika Rubinov, UNSW/U Cambridge, 2010-2015
+
+%   Modification history:
+%   2010: Original
+%   2015: Expanded documentation
+
+K    = sum(W~=0,2);            	
+cyc3 = diag((W.^(1/3))^3);           
+T    = sum(cyc3)./sum((K.*(K-1)));       %transitivity
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/weight_conversion.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/weight_conversion.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/weight_conversion.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/weight_conversion.m	2019-03-03 19:42:46.000000000 +0100
@@ -0,0 +1,88 @@
+function W = weight_conversion(W, wcm)
+% WEIGHT_CONVERSION    Conversion of weights in input matrix
+%
+%   W_bin = weight_conversion(W, 'binarize');
+%   W_nrm = weight_conversion(W, 'normalize');
+%   L = weight_conversion(W, 'lengths');
+%   W_fix = weight_conversion(W, 'autofix');
+%
+%   This function may either binarize an input weighted connection matrix,
+%   normalize an input weighted connection matrix, convert an input
+%   weighted connection matrix to a weighted connection-length matrix, or
+%   fix common connection problems in binary or weighted connection matrices.
+%
+%       Binarization converts all present connection weights to 1.
+%
+%       Normalization rescales all weight magnitudes to the range [0,1] and
+%   should be done prior to computing some weighted measures, such as the
+%   weighted clustering coefficient.
+%
+%       Conversion of connection weights to connection lengths is needed
+%   prior to computation of weighted distance-based measures, such as
+%   distance and betweenness centrality. In a weighted connection network,
+%   higher weights are naturally interpreted as shorter lengths. The
+%   connection-lengths matrix here is defined as the inverse of the
+%   connection-weights matrix.
+%
+%       Autofix removes all Inf and NaN values, remove all self connections
+%   (sets all weights on the main diagonal to 0), ensures that symmetric matrices
+%   are exactly symmetric (by correcting for round-off error), and ensures that
+%   binary matrices are exactly binary (by correcting for round-off error).
+%
+%   Inputs: W           binary or weighted connectivity matrix
+%           wcm         weight-conversion command - possible values:
+%                           'binarize'      binarize weights
+%                           'normalize'     normalize weights
+
+%                           'lengths'       convert weights to lengths
+%                           'autofix'       fixes common weights problems
+%
+%   Output: W_          output connectivity matrix
+%
+%
+%   Mika Rubinov, U Cambridge, 2012
+
+%   Modification History:
+%   Sep 2012: Original
+%   Jan 2015: Added autofix feature.
+%   Jan 2017: Corrected bug in autofix (thanks to Jeff Spielberg)
+
+switch wcm
+    case 'binarize'
+        W=double(W~=0);         % binarize
+    case 'normalize'
+        W=W./max(abs(W(:)));    % rescale by maximal weight
+    case 'lengths'
+        E=find(W);
+        W(E)=1./W(E);           % invert weights
+    case 'autofix'
+        % clear diagonal
+        n = length(W);
+        W(1:n+1:end)=0;
+        
+        % remove Infs and NaNs
+        idx = isnan(W) | isinf(W);
+        if any(any(idx))
+            W(idx)=0;
+        end
+        
+        % ensure exact binariness
+        U = unique(W);
+        if nnz(U) > 1
+            idx_0 = abs(W  ) < 1e-10;
+            idx_1 = abs(W-1) < 1e-10;
+            if all(all(idx_0 | idx_1))
+                W(idx_0)=0;
+                W(idx_1)=1;
+            end
+        end
+        
+        % ensure exact symmetry
+        if ~isequal(W,W.')
+            if max(max(abs(W-W.'))) < 1e-10
+                W=(W+W).'/2;
+            end
+        end
+    otherwise
+        error('Unknown weight-conversion command.')
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/writetoPAJ.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/writetoPAJ.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/2019_03_03_BCT/writetoPAJ.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/2019_03_03_BCT/writetoPAJ.m	2019-03-03 19:42:50.000000000 +0100
@@ -0,0 +1,41 @@
+function writetoPAJ(CIJ, fname, arcs)
+%WRITETOPAJ         Write to Pajek
+%
+%   writetoPAJ(CIJ, fname, arcs);
+%
+%   This function writes a Pajek .net file from a MATLAB matrix
+%
+%   Inputs:     CIJ,        adjacency matrix
+%               fname,      filename minus .net extension
+%               arcs,       1 for directed network
+%                           0 for an undirected network
+%
+%   Chris Honey, Indiana University, 2007
+
+
+N = size(CIJ,1);
+fid = fopen(cat(2,fname,'.net'), 'w');
+
+%%%VERTICES
+fprintf(fid, '*vertices %6i \r', N);
+for i = 1:N
+    fprintf(fid, '%6i "%6i" \r', [i i]);
+end
+
+%%%ARCS/EDGES
+if arcs
+    fprintf(fid, '*arcs \r');
+else
+    fprintf(fid, '*edges \r');
+end
+
+for i = 1:N
+    for j = 1:N
+        if CIJ(i,j) ~= 0
+            fprintf(fid, '%6i %6i %6f \r', [i j CIJ(i,j)]);
+        end
+    end
+end
+
+fclose(fid);
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/release_notes.html NeuroMiner-1-main.clara/graphkernels_Clara/BCT/release_notes.html
--- NeuroMiner-1-main/graphkernels_Clara/BCT/release_notes.html	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/release_notes.html	2019-03-03 21:13:32.000000000 +0100
@@ -0,0 +1,210 @@
+
+<!DOCTYPE html
+  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
+<html><head>
+      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
+   <!--
+This HTML was auto-generated from MATLAB code.
+To make changes, update the MATLAB code and republish this document.
+      --><title>release_notes</title><meta name="generator" content="MATLAB 9.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-03-03"><meta name="DC.source" content="release_notes.m"><style type="text/css">
+html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}
+
+html { min-height:100%; margin-bottom:1px; }
+html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
+html body td { vertical-align:top; text-align:left; }
+
+h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
+h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
+h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }
+
+a { color:#005fce; text-decoration:none; }
+a:hover { color:#005fce; text-decoration:underline; }
+a:visited { color:#004aa0; text-decoration:none; }
+
+p { padding:0px; margin:0px 0px 20px; }
+img { padding:0px; margin:0px 0px 20px; border:none; }
+p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 
+
+ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
+ul li { padding:0px; margin:0px 0px 7px 0px; }
+ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
+ul li ol li { list-style:decimal; }
+ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
+ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
+ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
+ol li ol li { list-style-type:lower-alpha; }
+ol li ul { padding-top:7px; }
+ol li ul li { list-style:square; }
+
+.content { font-size:1.2em; line-height:140%; padding: 20px; }
+
+pre, code { font-size:12px; }
+tt { font-size: 1.2em; }
+pre { margin:0px 0px 20px; }
+pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
+pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
+pre.error { color:red; }
+
+@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }
+
+span.keyword { color:#0000FF }
+span.comment { color:#228B22 }
+span.string { color:#A020F0 }
+span.untermstring { color:#B20000 }
+span.syscmd { color:#B28C00 }
+
+.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
+.footer p { margin:0px; }
+.footer a { color:#878787; }
+.footer a:hover { color:#878787; text-decoration:underline; }
+.footer a:visited { color:#878787; }
+
+table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
+table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }
+
+
+
+
+
+  </style></head><body><div class="content"><h1></h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Version 2019-03-03: Minor update</a></li><li><a href="#2">Version 2017-15-01: Major update</a></li><li><a href="#3">Version 2016-16-01: Major update</a></li><li><a href="#4">Version 2015-25-01: Major update</a></li><li><a href="#5">Version 2014-04-05: Minor update</a></li></ul></div><h2 id="1">Version 2019-03-03: Minor update</h2><p><b>New network models</b></p><div><ul><li>navigation_wu.m: Navigation of connectivity length matrix guided by   nodal distance</li><li>quasi_idempotence.m: Connection matrix quasi-idempotence</li></ul></div><p><b>Bug fixes and/or code improvements</b> * gateway_coef_sign.m: Bugfix, change in handling of weighted matrices. * search_information.m: Modified to make user directly specify                         weight-to-length transformations.</p><h2 id="2">Version 2017-15-01: Major update</h2><p><b>New network models</b></p><div><ul><li>generate_fc.m: Generation of synthetic functional connectivity matrices   based on structural network measures.</li><li>predict_fc.m: Prediction of functional connectivity matrices from   structural connectivity matrices.</li><li>mleme_constraint_model.m: Unbiased sampling of networks with soft   module and hub constraints (maximum-likelihood estimation of maximum   entropy networks).</li></ul></div><p><b>New measures and demos</b></p><div><ul><li>clique_communities.m: Overlapping community structure via the clique   percolation method.</li><li>rentian_scaling_2d.m and rentian_scaling_3d.m: Updated rentian scaling   functions to replace rentian_scaling.m.</li><li>diffusion_efficiency.m:  Global mean and pair-wise effiency based on   a diffusion process.</li><li>distance_wei_floyd.m: All pairs shortest paths via the Floyd-Warshall   algorithm.</li><li>mean_first_passage_time.m: Mean first passage time.</li><li>path_transitivity.m: Transitivity based on shortest paths.</li><li>resource_efficiency_bin.m: Resource efficiency and shortest path   probability.</li><li>rout_efficiency.m: Mean, pair-wise and local routing efficiency.</li><li>retrieve_shortest_path.m: Retrieval of shortest path between source and   target nodes.</li><li>search_information.m: Search information based on shortest paths.</li><li>demo_efficiency_measures.m: Demonstration of efficiency measures.</li></ul></div><p><b>Removed functions</b></p><div><ul><li>rentian_scaling.m: Replaced with rentian_scaling_2d.m and   rentian_scaling_3d.m.</li></ul></div><p><b>Bug fixes and/or code improvements and/or documentation improvements</b></p><div><ul><li>efficiency_wei.m: Included a modified weighted variant of the local   efficiency.</li><li>partition_distance.m: Generalized computation of distances to input   partition matrices.</li><li>clustering_coef_wu_sign.m: Fixed computation of the denominator in the   Constantini and Perugini versions of the weighted clustering   coefficient.</li><li>modularity_dir.m and modularity_und.m: Updated documentation and   simplified code to clarify that these are deterministic algorithms.</li><li>weight_conversion.m: Corrected bug in weight autofix.</li></ul></div><p><b>Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions</b></p><h2 id="3">Version 2016-16-01: Major update</h2><p><b>New network models</b></p><div><ul><li>generative_model.m: Implements more than 10 generative network models.</li><li>evaluate_generative_model.m: Implements and evaluates the accuracy of more than 10 generative network models.</li><li>demo_generative_models_geometric.m and demo_generative_models_neighbors.m: Demonstrate the capabilities of the new generative model functions.</li></ul></div><p><b>New network measures</b></p><div><ul><li>clustering_coef_wu_sign.m: Multiple generalizations of the clustering coefficient for networks with positive and negative weights.</li><li>core_periphery_dir.m: Optimal core structure and core-ness statistic.</li><li>gateway_coef_sign.m: Gateway coefficient (a variant of the participation coefficient) for networks with positive and negative weights.</li><li>local_assortativity_sign.m: Local (nodal) assortativity for networks with positive and negative weights.</li><li>randmio_dir_signed.m: Random directed graph with preserved signed in- and out- degree distribution.</li></ul></div><p><b>Removed network measures</b></p><div><ul><li>modularity_louvain_und_sign.m, modularity_finetune_und_sign.m: This functionality is now provided by community_louvain.m.</li><li>modularity_probtune_und_sign.m: Similar functionality is provided by consensus_und.m</li></ul></div><p><b>Bug fixes and/or code improvements and/or documentation improvements</b></p><div><ul><li>charpath.m: Changed default behavior, such that infinitely long paths (i.e. paths between disconnected nodes) are now included in computations by default, but may be excluded manually.</li><li>community_louvain.m: Included generalization for negative weights, enforced binary network input for Potts-model Hamiltonian, streamlined code.</li><li>eigenvector_centrality_und.m: Ensured the use of leading eigenvector for computations of eigenvector centrality.</li><li>modularity_und.m, modularity_dir.m: Enforced single node moves during fine-tuning step.</li><li>null_model_und_sign.m and null_model_dir_sign.m: Fixed preservation of negative degrees in sparse networks with negative weights.</li><li>randmio_und_signed.m: Now allows unbiased exploration of all network configurations.</li><li>transitivity_bd.m, transitivity_wu.m, transitivity_wd.m: removed tests for absence of nodewise 3-cycles. Expanded documentation.</li><li>clustering_coef_wu.m, clustering_coef_wd.m: Expanded documentation.</li><li>motif3-m and motif4-m functions: Expanded documentation.</li><li>rich_club_wu.m, rich_club_wd.m. Expanded documentation.</li></ul></div><p><b>Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions</b></p><h2 id="4">Version 2015-25-01: Major update</h2><p>Includes two new community-detection scripts and multiple improvements</p><div><ul><li>New community detection scripts: 1. community_louvain.m (supersedes modularity_louvain.m and modularity_finetune.m scripts); 2. link_communities.m.</li><li>added autofix flag to weight_conversion.m for fixing common weight problems.</li><li>other function improvements: participation_coef.m, charpath.m, reorder_mod.m.</li><li>bug fixes: modularity_finetune_und_sign.m, modularity_probtune_und_sign.m, threshold_proportional.m</li><li>changed help files: assortativity_wei.m, distance_wei.m</li></ul></div><h2 id="5">Version 2014-04-05: Minor update</h2><div><ul><li>consensus_und.m is now a self-contained function</li><li>headers in charpath.m and in threshold_proportional.m have been corrected</li></ul></div><p class="footer"><br><a href="https://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018b</a><br></p></div><!--
+##### SOURCE BEGIN #####
+
+%% Version 2019-03-03: Minor update
+% *New network models*
+%
+% * navigation_wu.m: Navigation of connectivity length matrix guided by
+%   nodal distance 
+% * quasi_idempotence.m: Connection matrix quasi-idempotence
+% 
+% *Bug fixes and/or code improvements*
+% * gateway_coef_sign.m: Bugfix, change in handling of weighted matrices.
+% * search_information.m: Modified to make user directly specify
+%                         weight-to-length transformations.
+%
+%% Version 2017-15-01: Major update
+% *New network models*
+%
+% * generate_fc.m: Generation of synthetic functional connectivity matrices
+%   based on structural network measures.
+% * predict_fc.m: Prediction of functional connectivity matrices from
+%   structural connectivity matrices.
+% * mleme_constraint_model.m: Unbiased sampling of networks with soft
+%   module and hub constraints (maximum-likelihood estimation of maximum
+%   entropy networks).
+%
+% *New measures and demos*
+%
+% * clique_communities.m: Overlapping community structure via the clique
+%   percolation method.
+% * rentian_scaling_2d.m and rentian_scaling_3d.m: Updated rentian scaling
+%   functions to replace rentian_scaling.m.
+% * diffusion_efficiency.m:  Global mean and pair-wise effiency based on
+%   a diffusion process.
+% * distance_wei_floyd.m: All pairs shortest paths via the Floyd-Warshall
+%   algorithm.
+% * mean_first_passage_time.m: Mean first passage time.
+% * path_transitivity.m: Transitivity based on shortest paths.
+% * resource_efficiency_bin.m: Resource efficiency and shortest path
+%   probability.
+% * rout_efficiency.m: Mean, pair-wise and local routing efficiency.
+% * retrieve_shortest_path.m: Retrieval of shortest path between source and
+%   target nodes.
+% * search_information.m: Search information based on shortest paths.
+% * demo_efficiency_measures.m: Demonstration of efficiency measures.
+%
+% *Removed functions*
+%
+% * rentian_scaling.m: Replaced with rentian_scaling_2d.m and
+%   rentian_scaling_3d.m.
+%
+% *Bug fixes and/or code improvements and/or documentation improvements*
+%
+% * efficiency_wei.m: Included a modified weighted variant of the local
+%   efficiency. 
+% * partition_distance.m: Generalized computation of distances to input
+%   partition matrices. 
+% * clustering_coef_wu_sign.m: Fixed computation of the denominator in the
+%   Constantini and Perugini versions of the weighted clustering
+%   coefficient.
+% * modularity_dir.m and modularity_und.m: Updated documentation and
+%   simplified code to clarify that these are deterministic algorithms.
+% * weight_conversion.m: Corrected bug in weight autofix.
+%
+% *Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions*
+%
+%% Version 2016-16-01: Major update
+% *New network models*
+%
+% * generative_model.m: Implements more than 10 generative network models.
+% * evaluate_generative_model.m: Implements and evaluates the accuracy of
+% more than 10 generative network models.
+% * demo_generative_models_geometric.m and
+% demo_generative_models_neighbors.m: Demonstrate the capabilities of the
+% new generative model functions.
+%
+% *New network measures*
+%
+% * clustering_coef_wu_sign.m: Multiple generalizations of the clustering
+% coefficient for networks with positive and negative weights.
+% * core_periphery_dir.m: Optimal core structure and core-ness statistic.
+% * gateway_coef_sign.m: Gateway coefficient (a variant of the
+% participation coefficient) for networks with positive and negative
+% weights.
+% * local_assortativity_sign.m: Local (nodal) assortativity for networks
+% with positive and negative weights.
+% * randmio_dir_signed.m: Random directed graph with preserved signed in-
+% and out- degree distribution.
+%
+% *Removed network measures*
+%
+% * modularity_louvain_und_sign.m, modularity_finetune_und_sign.m: This
+% functionality is now provided by community_louvain.m.
+% * modularity_probtune_und_sign.m: Similar functionality is provided by
+% consensus_und.m
+%
+% *Bug fixes and/or code improvements and/or documentation improvements*
+%
+% * charpath.m: Changed default behavior, such that infinitely long paths
+% (i.e. paths between disconnected nodes) are now included in computations
+% by default, but may be excluded manually.
+% * community_louvain.m: Included generalization for negative weights,
+% enforced binary network input for Potts-model Hamiltonian, streamlined
+% code.
+% * eigenvector_centrality_und.m: Ensured the use of leading eigenvector
+% for computations of eigenvector centrality.
+% * modularity_und.m, modularity_dir.m: Enforced single node moves during
+% fine-tuning step.
+% * null_model_und_sign.m and null_model_dir_sign.m: Fixed preservation
+% of negative degrees in sparse networks with negative weights.
+% * randmio_und_signed.m: Now allows unbiased exploration of all network
+% configurations.
+% * transitivity_bd.m, transitivity_wu.m, transitivity_wd.m: removed tests
+% for absence of nodewise 3-cycles. Expanded documentation.
+% * clustering_coef_wu.m, clustering_coef_wd.m: Expanded documentation.
+% * motif3-m and motif4-m functions: Expanded documentation.
+% * rich_club_wu.m, rich_club_wd.m. Expanded documentation.
+%
+% *Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions*
+%
+%% Version 2015-25-01: Major update
+% Includes two new community-detection scripts and multiple improvements
+%
+% * New community detection scripts: 1. community_louvain.m (supersedes
+% modularity_louvain.m and modularity_finetune.m scripts); 2.
+% link_communities.m.
+% * added autofix flag to weight_conversion.m for fixing common weight
+% problems.
+% * other function improvements: participation_coef.m, charpath.m,
+% reorder_mod.m.
+% * bug fixes: modularity_finetune_und_sign.m,
+% modularity_probtune_und_sign.m, threshold_proportional.m
+% * changed help files: assortativity_wei.m, distance_wei.m
+%
+%
+%% Version 2014-04-05: Minor update
+%
+% * consensus_und.m is now a self-contained function
+% * headers in charpath.m and in threshold_proportional.m have been corrected
+
+##### SOURCE END #####
+--></body></html>
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/BCT/release_notes.m NeuroMiner-1-main.clara/graphkernels_Clara/BCT/release_notes.m
--- NeuroMiner-1-main/graphkernels_Clara/BCT/release_notes.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/BCT/release_notes.m	2019-03-03 21:13:28.000000000 +0100
@@ -0,0 +1,137 @@
+
+%% Version 2019-03-03: Minor update
+% *New network models*
+%
+% * navigation_wu.m: Navigation of connectivity length matrix guided by
+%   nodal distance 
+% * quasi_idempotence.m: Connection matrix quasi-idempotence
+% 
+% *Bug fixes and/or code improvements*
+% * gateway_coef_sign.m: Bugfix, change in handling of weighted matrices.
+% * search_information.m: Modified to make user directly specify
+%                         weight-to-length transformations.
+%
+%% Version 2017-15-01: Major update
+% *New network models*
+%
+% * generate_fc.m: Generation of synthetic functional connectivity matrices
+%   based on structural network measures.
+% * predict_fc.m: Prediction of functional connectivity matrices from
+%   structural connectivity matrices.
+% * mleme_constraint_model.m: Unbiased sampling of networks with soft
+%   module and hub constraints (maximum-likelihood estimation of maximum
+%   entropy networks).
+%
+% *New measures and demos*
+%
+% * clique_communities.m: Overlapping community structure via the clique
+%   percolation method.
+% * rentian_scaling_2d.m and rentian_scaling_3d.m: Updated rentian scaling
+%   functions to replace rentian_scaling.m.
+% * diffusion_efficiency.m:  Global mean and pair-wise effiency based on
+%   a diffusion process.
+% * distance_wei_floyd.m: All pairs shortest paths via the Floyd-Warshall
+%   algorithm.
+% * mean_first_passage_time.m: Mean first passage time.
+% * path_transitivity.m: Transitivity based on shortest paths.
+% * resource_efficiency_bin.m: Resource efficiency and shortest path
+%   probability.
+% * rout_efficiency.m: Mean, pair-wise and local routing efficiency.
+% * retrieve_shortest_path.m: Retrieval of shortest path between source and
+%   target nodes.
+% * search_information.m: Search information based on shortest paths.
+% * demo_efficiency_measures.m: Demonstration of efficiency measures.
+%
+% *Removed functions*
+%
+% * rentian_scaling.m: Replaced with rentian_scaling_2d.m and
+%   rentian_scaling_3d.m.
+%
+% *Bug fixes and/or code improvements and/or documentation improvements*
+%
+% * efficiency_wei.m: Included a modified weighted variant of the local
+%   efficiency. 
+% * partition_distance.m: Generalized computation of distances to input
+%   partition matrices. 
+% * clustering_coef_wu_sign.m: Fixed computation of the denominator in the
+%   Constantini and Perugini versions of the weighted clustering
+%   coefficient.
+% * modularity_dir.m and modularity_und.m: Updated documentation and
+%   simplified code to clarify that these are deterministic algorithms.
+% * weight_conversion.m: Corrected bug in weight autofix.
+%
+% *Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions*
+%
+%% Version 2016-16-01: Major update
+% *New network models*
+%
+% * generative_model.m: Implements more than 10 generative network models.
+% * evaluate_generative_model.m: Implements and evaluates the accuracy of
+% more than 10 generative network models.
+% * demo_generative_models_geometric.m and
+% demo_generative_models_neighbors.m: Demonstrate the capabilities of the
+% new generative model functions.
+%
+% *New network measures*
+%
+% * clustering_coef_wu_sign.m: Multiple generalizations of the clustering
+% coefficient for networks with positive and negative weights.
+% * core_periphery_dir.m: Optimal core structure and core-ness statistic.
+% * gateway_coef_sign.m: Gateway coefficient (a variant of the
+% participation coefficient) for networks with positive and negative
+% weights.
+% * local_assortativity_sign.m: Local (nodal) assortativity for networks
+% with positive and negative weights.
+% * randmio_dir_signed.m: Random directed graph with preserved signed in-
+% and out- degree distribution.
+%
+% *Removed network measures*
+%
+% * modularity_louvain_und_sign.m, modularity_finetune_und_sign.m: This
+% functionality is now provided by community_louvain.m.
+% * modularity_probtune_und_sign.m: Similar functionality is provided by
+% consensus_und.m
+%
+% *Bug fixes and/or code improvements and/or documentation improvements*
+%
+% * charpath.m: Changed default behavior, such that infinitely long paths
+% (i.e. paths between disconnected nodes) are now included in computations
+% by default, but may be excluded manually.
+% * community_louvain.m: Included generalization for negative weights,
+% enforced binary network input for Potts-model Hamiltonian, streamlined
+% code.
+% * eigenvector_centrality_und.m: Ensured the use of leading eigenvector
+% for computations of eigenvector centrality.
+% * modularity_und.m, modularity_dir.m: Enforced single node moves during
+% fine-tuning step.
+% * null_model_und_sign.m and null_model_dir_sign.m: Fixed preservation
+% of negative degrees in sparse networks with negative weights.
+% * randmio_und_signed.m: Now allows unbiased exploration of all network
+% configurations.
+% * transitivity_bd.m, transitivity_wu.m, transitivity_wd.m: removed tests
+% for absence of nodewise 3-cycles. Expanded documentation.
+% * clustering_coef_wu.m, clustering_coef_wd.m: Expanded documentation.
+% * motif3-m and motif4-m functions: Expanded documentation.
+% * rich_club_wu.m, rich_club_wd.m. Expanded documentation.
+%
+% *Cosmetic and MATLAB code analyzer (mlint) improvements to many other functions*
+%
+%% Version 2015-25-01: Major update
+% Includes two new community-detection scripts and multiple improvements
+%
+% * New community detection scripts: 1. community_louvain.m (supersedes
+% modularity_louvain.m and modularity_finetune.m scripts); 2.
+% link_communities.m.
+% * added autofix flag to weight_conversion.m for fixing common weight
+% problems.
+% * other function improvements: participation_coef.m, charpath.m,
+% reorder_mod.m.
+% * bug fixes: modularity_finetune_und_sign.m,
+% modularity_probtune_und_sign.m, threshold_proportional.m
+% * changed help files: assortativity_wei.m, distance_wei.m
+%
+%
+%% Version 2014-04-05: Minor update
+%
+% * consensus_und.m is now a self-contained function
+% * headers in charpath.m and in threshold_proportional.m have been corrected
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/GraphKernel_matrixInput.m NeuroMiner-1-main.clara/graphkernels_Clara/GraphKernel_matrixInput.m
--- NeuroMiner-1-main/graphkernels_Clara/GraphKernel_matrixInput.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/GraphKernel_matrixInput.m	2021-07-10 12:49:32.940000000 +0200
@@ -0,0 +1,25 @@
+function K = GraphKernel_matrixInput(Y1, Y2, kernelf, param1) % in case of test kernel, Y1 should be Ytest
+    if ~isequal(Y1,Y2)
+        test = 1;
+        Y = vertcat(Y1,Y2);
+    else
+        test = 0;
+        Y = Y1;
+    end
+    gList = [];
+    for i = 1:size(Y,1) 
+        % apply sparsity threshold
+        % spArray = apply_sparsity_thres(Y(i,:), sp);
+        % transform to 2D matrix
+        graph = array_to_graph(Y(i,:)); 
+        gList = [gList, graph]; 
+    end
+    lab = 1; 
+    KM = feval(kernelf, gList, param1, lab);
+    K = KM{size(KM,2)}; % which one to choose?
+    if test % we want test kernel of dimension n_test x n_train
+        K = K(1:size(Y1,1),size(Y1,1)+1:size(Y1,1) + size(Y2,1));
+    else
+        K = [(1:size(K,1))', normalizekm(K)];
+    end     
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/Computing_And_Testing_S_On_Data_Network.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/Computing_And_Testing_S_On_Data_Network.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/Computing_And_Testing_S_On_Data_Network.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/Computing_And_Testing_S_On_Data_Network.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,61 @@
+%% script to compute small-world-ness and do statistical testing on example data network
+% Mark Humphries 3/2/2017
+
+clear all; close all;
+
+% analysis parameters
+Num_ER_repeats = 100;  % to estimate C and L numerically for E-R random graph
+Num_S_repeats = 1000; % to get P-value for S; min P = 0.001 for 1000 samples
+I = 0.95;
+
+FLAG_Cws = 1;
+FLAG_Ctransitive = 2;
+
+%% load the adjacency matrix for the Lusseau bottle-nose dolphin social
+% network
+load dolphins  % loads struct of data in "Problem"; adjacency matrix is Problem.A
+
+A = full(Problem.A); % convert into full from sparse format
+
+% get its basic properties
+n = size(A,1);  % number of nodes
+k = sum(A);  % degree distribution of undirected network
+m = sum(k)/2;
+K = mean(k); % mean degree of network
+
+%% computing small-world-ness using the analytical approximations for the E-R graph
+
+[expectedC,expectedL] = ER_Expected_L_C(K,n);  % L_rand and C_rand
+
+[S_ws,C_ws,L] = small_world_ness(A,expectedL,expectedC,FLAG_Cws);  % Using WS clustering coefficient
+[S_trans,C_trans,L] = small_world_ness(A,expectedL,expectedC,FLAG_Ctransitive);  %  Using transitive clustering coefficient
+
+
+
+%% computing small-world-ness by estimating L_rand and C_rand from an ensemble of random graphs
+% check when using small networks...
+
+[Lrand,CrandWS] = NullModel_L_C(n,m,Num_ER_repeats,FLAG_Cws);
+[Lrand,CrandTrans] = NullModel_L_C(n,m,Num_ER_repeats,FLAG_Ctransitive);
+
+% Note: if using a different random graph null model, e.g. the
+% configuration model, then use this form
+
+% compute small-world-ness using mean value over Monte-Carlo realisations
+
+% NB: some path lengths in L will be INF if the ER network was not fully
+% connected: we disregard these here as the dolphin network is fully
+% connected.
+Lrand_mean = mean(Lrand(Lrand < inf));
+
+[S_ws_MC,~,~] = small_world_ness(A,Lrand_mean,mean(CrandWS),FLAG_Cws);  % Using WS clustering coefficient
+[S_trans_MC,~,~] = small_world_ness(A,Lrand_mean,mean(CrandTrans),FLAG_Ctransitive);  %  Using transitive clustering coefficient
+
+
+%% do Monte-Carlo test of disitribution of S in ER random graph
+
+[I,P,Sb] = SsampleER(n,K,m,I,Num_S_repeats,S_ws,FLAG_Cws);  % for Sws here
+
+% check how many samples ended up being used to calculate P-value
+Nsamps = numel(Sb);
+Pmax = 1 / Nsamps;
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/ER_Expected_L_C.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/ER_Expected_L_C.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/ER_Expected_L_C.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/ER_Expected_L_C.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,13 @@
+function [expectedC,expectedL] = ER_Expected_L_C(k,n)
+    
+% ER_EXPECTED_L_C the expected path-length and clustering of an ER random graph
+% [C,L] = ER_EXPECTED_L_C(K,N) for a network of N nodes and mena degree K, 
+% computes the expected shortest path-length L and expected clustering 
+% coefficient C if that network was an Erdos-Renyi random graph
+%
+% Mark Humphries 3/2/2017
+
+expectedC = k / n;
+z1 = k;
+z2 = k^2;
+expectedL = (log((n-1)*(z2 - z1) + z1^2) - log(z1^2)) / log(z2/z1);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/LICENSE.txt NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/LICENSE.txt
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/LICENSE.txt	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/LICENSE.txt	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,21 @@
+MIT License
+
+Copyright (c) 2017 Mark Daniel Humphries
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/NullModel_L_C.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/NullModel_L_C.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/NullModel_L_C.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/NullModel_L_C.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,33 @@
+function [Lrand,Crand] = NullModel_L_C(n,m,Nrepeats,FLAG)
+
+% NULLMODEL_L_C Monte Carlo estimates of path-length and clustering of an ER random graph
+% [C,L] = NULLMODEL_L_C(N,M,R,FLAG) for N nodes and M edges,
+% creates R realisations of an Erdos-Renyi random graph (N,M), and
+% computes the shortest path-length L and clustering coefficient C 
+% for each one
+%
+% FLAG is a number indicating which clustering coefficient to compute:
+%   1 - Cws 
+%   2 - transitivity C (no. of triangles)
+
+% Mark Humphries 3/2/2017
+
+
+for iE = 1:Nrepeats
+    ER = random_graph(n,0,m,'undirected');  % make E-R random graph
+    [~,D] = reachdist(ER);  % returns Distance matrix of all pairwise distances
+    Lrand(iE) = mean(D(:));  % mean shortest path-length: including self-loops
+    
+%     if isinf(Lrand(iE))
+%         keyboard
+%     end
+    
+    % calculate required form of C
+    switch FLAG
+        case 1
+            c = clustering_coef_bu(ER);  % vector of each node's C_ws
+            Crand(iE) = mean(c);  % mean C
+        case 2
+            Crand(iE) = clusttriang(ER);
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/README.md NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/README.md
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/README.md	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/README.md	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,28 @@
+Small-World-Ness
+============================
+
+MATLAB code for computing small-world-ness
+
+Includes functions for: <br>
+computing small-world-ness from data networks <br>
+statistically testing the data network divergence from an E-R random graph
+
+See Computing_And_Testing_S_On_Data_Network.m for examples of all analyses on a test network
+
+============================
+Includes PDFs of the original papers: <br>
+* Humphries, M. D.; Gurney, K. & Prescott, T. J. (2006) The brainstem reticular formation is a small-world, not scale-free, network. _Proceedings of the Royal Society B. Biological Sciences_, 273, 503-511
+
+  This paper introduced the small-world-ness measure
+* Humphries, M. D. & Gurney, K. (2008) Network 'small-world-ness': A quantitative method for determining canonical network equivalence _PLoS One_, 3, e0002051
+
+  This paper analysed the behaviour of the small-world-ness measure, and explored its insights for a range of real-world networks
+
+=============================
+Uses: <br>
+reachdist.m <br>
+clustering_coef_bu
+
+from the Brain Connectivity Toolbox: https://sites.google.com/site/bctnet/Home
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/SanalyticER.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/SanalyticER.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/SanalyticER.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/SanalyticER.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,36 @@
+function [S,lambda,gamma] = SanalyticER(n,k,C,L,FLAG)
+
+% SANALYTICER computes small-world-ness for analytic E-R graph values
+%   [S,lambda,gamma] = SANALYTICER(N,K,C,L,FLAG) computes small-world-ness S, for networks of
+%   size N, mean degree K, clustering coefficient C, and path length L,
+%   where each is an array of values. The S score is based on the 
+%   corresponding C and L values for the equivalent
+%   Erdos-Renyi random graph (computed analytically). The form of S
+%   computed depends on the value of FLAG:
+%       1 - raw form (just S = (lambda / gamma))
+%       2 - magnitude form (S = log10(lambda / gamma))
+%
+%   Will also return the path length ratio (lambda), and the clustering
+%   coefficient ratio (gamma).
+%
+%   Note#1: Whether the resulting S value is for S(tri) or S(ws) depends
+%   entirely on which C value is supplied by the user - the E-R graph
+%   value is equivalent for either C(tri) or C(ws)
+%
+%   Note#2: for directed E-R random graphs, k should be mean in-degree; note that this
+%   analytic form more susceptible to error as it relies on a giant component 
+%   covering all of the graph, with very small in- and out-only populations.	
+%
+%   Mark Humphries 30/2/2007
+
+C_ER = k ./ n;
+L_ER = log(n) ./ log(k);
+
+lambda = L ./ L_ER;
+gamma = C ./ C_ER;
+
+S = gamma ./ lambda;
+if FLAG == 2
+    S = log10(S);
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/SsampleER.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/SsampleER.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/SsampleER.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/SsampleER.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,53 @@
+function [alpha,P,Sb] = SsampleER(n,k,m,I,ns,S,FLAG)
+
+% SSAMPLEER generate small-world-ness values for E-R random graph
+% [A,P,V] = SSAMPLEER(N,K,M,I,SAMP,S,FLAG) computes SAMP realisations of an
+%   E-R random graph, of size N, mean degree K, and number of unique edges M. 
+%   For each, the small-world-ness is computed. 
+%   Returned:
+%       A: the I% interval of the null model Small-world-ness distribution.  I should be given as proportion
+%           i.e. I = 0.95 is the 95% interval. This measures the extremes in variation of S under the null model 
+%       P: the P-value for the one-sided test for the hypothesis that the data small-world-ness S is 
+%           greater than expected from the null-model small-world-ness distribution.
+%           (Technically, that S is incorrectly rejected from the null-model distirbution) 
+%       V: the vector of all null model small-world-ness values computed
+%
+%   The value of FLAG determines which clustering coefficient is computed:
+%       1 - raw S: clustering coefficient of Watts & Strogatz (1998)
+%       2 - raw S: transitivity clustering coefficient (no. of triangles) e.g.
+%           Barat & Weigt(2000); Newman et al (2001);   
+%
+%   Notes:
+%   (1) The S-score computed here is based on comparison with the E-R random graph, following the
+%       Watts & Strogatz (1998) definition.
+%   (2) The expected C and L values for an E-R random graph are computed, implying:
+%       (i) Small n would lead to errors, as path length is underestimated 
+%       (ii) This procedure is currently only defined for undirected graphs,
+%
+%   Mark Humphries 3/2/2017
+
+[expectedC,expectedL] = ER_Expected_L_C(k,n);
+
+for i = 1:ns
+    A = random_graph(n,0,m,'undirected');
+    [SER(i),~,~] = small_world_ness(A,expectedL,expectedC,FLAG);
+end
+
+% remove any SER = 0, due to INF path-length
+Sb = SER(SER > 0);
+Nsamps = numel(Sb);
+
+% confidence interval on the null model value of S
+sig = (1 - I);
+
+
+% keyboard
+
+alpha = prctile(Sb,100*[sig/2 1-sig/2]);
+
+% P-value for S
+P = sum(Sb > S)+1/Nsamps;
+
+
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/clustering_coef_bu.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/clustering_coef_bu.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/clustering_coef_bu.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/clustering_coef_bu.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,28 @@
+function C=clustering_coef_bu(G)
+%CLUSTERING_COEF_BU     Clustering coefficient
+%
+%   C = clustering_coef_bu(A);
+%
+%   The clustering coefficient is the fraction of triangles around a node
+%   (equiv. the fraction of node's neighbors that are neighbors of each other).
+%
+%   Input:      A,      binary undirected connection matrix
+%
+%   Output:     C,      clustering coefficient vector
+%
+%   Reference: Watts and Strogatz (1998) Nature 393:440-442.
+%
+%
+%   Mika Rubinov, UNSW, 2007-2010
+
+n=length(G);
+C=zeros(n,1);
+
+for u=1:n
+    V=find(G(u,:));
+    k=length(V);
+    if k>=2                 %degree must be at least 2
+        S=G(V,V);
+        C(u)=sum(S(:))/(k^2-k);
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/clusttriang.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/clusttriang.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/clusttriang.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/clusttriang.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,22 @@
+function C = clusttriang(A)
+
+% CLUSTTRIANG computes clustering coefficient based on number of triangles
+%   C = CLUSTTRIANG(A) compute the clustering coefficient C of the
+%   adjacency matrix A, based on the ratio (number of triangles)/(number of triples).
+%   
+%   Notes:
+%   (1)This is a subtly different definition than that of Watts & Strogatz (1998). 
+%   In the present form, it does give the fraction of neighbouring nodes
+%   that are also neighbours of each other. (Also note: for directed
+%   graphs, this means *any* direction of connection, as along as all three
+%   nodes are connected).
+%
+%   (2) This function uses the method of Keeling (1999) to compute C. 
+%
+%   Mark Humphries 22/8/2006
+
+A2 = A * A;
+A3 = A2 * A;
+sumA2 = sum(sum(A2));
+
+C = trace(A3) / (sumA2 - trace(A2));
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/rand_pair.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/rand_pair.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/rand_pair.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/rand_pair.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,152 @@
+function y = rand_pair(a,b,n,varargin)
+
+% RAND_PAIR Uniformly-sampled random pair combinations
+%
+%   RAND_PAIR(A,B,N) is a 2-column matrix of length N, where each row is a unique pair of numbers
+%   chosen at random from the arrays A and B.
+%
+%   RAND_PAIR(A,B,N,S) starts the pseudo-random number generator using seed S. 
+%
+%   RAND_PAIR(A,B,N,S,'bi') ensures that each pair is unique in both directions 
+%   (i.e. if [2,3] already chosen then [3,2] rejected). If random seed not required then
+%   use the empty matrix [] instead of S.
+%
+%   RAND_PAIR(....,'diff') ensures that no pair is chosen which consists of the same numbers (i.e. [2,2] rejected);
+%   set S and 'bi' flag to [] if not required. 
+%
+%   Mark Humphries 15/10/2004
+
+if nargin > 3 & ~isempty(varargin{1})
+    rand('state',varargin{1});
+end
+
+if n > length(a) * length(b)
+    error('Cannot produce that many permutations of pairs')
+end
+
+if nargin == 6 & strcmp('diff',varargin{3})
+    diff = 1;
+else
+    diff = 0;
+end
+    
+y = zeros(n,2);
+
+if nargin >= 5 & strcmp('bi',varargin{2})
+    % bidirectional uniqueness - generate pair first then check
+    for loop = 1:n
+        isunique = 0;
+        while ~isunique
+            first_idx = ceil(rand * length(a));           
+            second_idx = ceil(rand * length(b));
+            
+            % check for uniqueness if either (a) not checking for sameness OR (b) are checking, and not the same
+            if ~diff | (diff & a(first_idx) ~= b(second_idx))
+                % test both combinations
+                n1 = find(y(:,1) == a(first_idx));
+                exists = find(b(second_idx) == y(n1,2));    % is second number in second column at same row number?
+                
+                if exists
+                    % this pair exists
+                    isunique = 0;
+                else
+                    % otherwise test reversed pair
+                    n2 = find(y(:,2) == a(first_idx));
+                    exists = find(b(second_idx) == y(n2,1));    % is second number in *first* column at same row number?
+                    if exists
+                        isunique = 0;
+                    else
+                        % pair does not exist in either configuration
+                        isunique = 1;
+                    end
+                end                    
+            end
+               
+        end
+        y(loop,:) = [a(first_idx) b(second_idx)];
+	end    
+elseif diff
+    % sameness checking without bidirection checking...
+    
+    % check for fast version if pairs drawn from same distribution and
+    % there are many links to find
+    if length(a)==length(b) & a == b % & n > 8000
+        % all possible pair numbers
+        possible_pair_nos = 1:length(a)^2;
+
+        % pair numbers which are duplicates
+        duplicates = 1:length(a)+1:length(a)^2;
+        
+        % remove duplicates
+        possible_pair_nos(duplicates) = [];
+        
+        % create random permutation sequence
+        seq = randperm(length(possible_pair_nos));
+        
+        % select first n of that sequence to create pairs
+        selected_pairs = possible_pair_nos(seq(1:n));
+        
+        first_idx = ceil(selected_pairs ./ length(b));
+        second_idx = rem(selected_pairs,length(b));
+        second_idx(second_idx==0) = length(b);
+        y = [a(first_idx)',b(second_idx)']; 
+        
+    else
+        old_pairs = zeros(n,1);
+	
+        % number of unique permutations
+        perms = length(a) * length(b);
+        
+        % fastish version for no reverse duplicate checking
+		for loop = 1:n
+            isunique = 0;
+            while ~isunique
+                pair_no = ceil(rand * perms);              % assumes pairs generated thus [a1,b1; a1,b2; a1,b3; a2,b1; a2,b2....]
+                if all(old_pairs ~= pair_no)
+                    isunique = 1;
+                       
+                    first_idx = ceil(pair_no / length(b));      % index for first column
+                    second_idx = rem(pair_no,length(b)); 
+                    if second_idx == 0 second_idx = length(b); end
+                
+                    first_num = a(first_idx);
+                    second_num = b(second_idx);    % number for second column
+                    
+                    if first_num == second_num      % if same then no longer unique
+                        is_unique = 0;
+                    end
+                end
+            end
+            old_pairs(loop) = pair_no;
+            y(loop,:) = [first_num second_num];
+        end
+    end % version if..then...
+else
+    old_pairs = zeros(n,1);
+
+    % number of unique permutations
+    perms = length(a) * length(b);
+    
+    % fast version for no reverse duplicate or sameness checking
+	for loop = 1:n
+        isunique = 0;
+        while ~isunique
+            pair_no = ceil(rand * perms);              % assumes pairs generated thus [a1,b1; a1,b2; a1,b3; a2,b1; a2,b2....]
+            if all(old_pairs ~= pair_no)
+                isunique = 1;
+            end            
+        end
+        old_pairs(loop) = pair_no;
+        
+        first_idx = ceil(pair_no / length(b));      % number for first column
+        second_idx = rem(pair_no,length(b)); 
+        if second_idx == 0 second_idx = length(b); end
+        
+        first_num = a(first_idx);
+        second_num = b(second_idx);    % number for second column
+        y(loop,:) = [first_num second_num];
+	end
+end
+
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/random_graph.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/random_graph.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/random_graph.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/random_graph.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,96 @@
+function A = random_graph(N,I,P,type,varargin)
+
+% RANDOM_GRAPH create a random graph
+%
+%   A = RANDOM_GRAPH(N,I,P,T) where N is the number of vertices in
+%   the graph, I is the proportion of inhibitory vertices (useful for asymmetry or sign problems),
+%   P is the connection probability, and T is the type of edges that 
+%   constitute the graph ('directed' or 'undirected'). The graph created is entirely random,
+%   with the probability of connection tested for each pair of vertices
+%   (Erdos-Renyi random graph). Only single edges and no self-loops are
+%   allowed - hence, P is adjusted to account for the slight reduction in
+%   the number of maximum edges.
+%
+%   If P > 1 then it is assumed that P is the number of unique edges to be made. That is, the number of edges is
+%   specified and connect randomly selected pairs - this allows the creation of 'equivalent' random
+%   graphs where the average probability P(k) (connections per node) is unknown or meaningless (such as when an equivalent random
+%   graph is required for a structure with a double Gaussian distribution of P(k)). Note that
+%   in this case P < N(N-1)/2 for undirected graphs, and P < N(N-1) for
+%   directed graphs. 
+%
+%   A = RANDOM_GRAPH(...,S) initialises the random number generator with seed S, allowing for repeated
+%   creation of same graph (set = [] to omit) 
+%
+%   Returns an adjacency matrix A which defines every individual connection (ROW is connections
+%   from the neuron, COLUMN connections to the neuron); for undirected graphs, the connections are
+%   symmetrical about the diagonal. Entries are {0,1,-1}. 
+%
+%   Mark Humphries 29/11/2006
+
+if ~strcmp(lower(type),'directed') & ~strcmp(lower(type),'undirected')
+    error('Incorrect graph type specified')
+end
+
+% set seed for random number generator
+if nargin >= 5
+    S = varargin{1};
+else
+    S = [];    
+end
+
+    
+if P <= 1    
+    % set random number generator
+    if S
+        rand('state',S);
+    end
+
+	P = (N^2 * P) / (N^2 - N);     %% re-calculate connection probability to account for removal of self-connections below
+                                               %% note: this approximation fails as P -> 1 for small N    
+	%%%%% adjacency matrix
+    if strcmp(type,'undirected')
+        A = triu(rand(N));
+    else
+        A = rand(N);
+    end
+  
+    A(A > 0 & A <= P) = 1;                 
+    A(A < 1) = 0;
+    A(eye(N)==1) = 0;
+    
+else    
+    if strcmp(type,'undirected')
+        if P >= N*(N-1)/2
+             error('Too many edges for the specified number of nodes in an undirected graph');
+        end
+        link_pairs = rand_pair(1:N,1:N,P,S,'bi','diff');                       % only unique node pairs allowed for undirected  
+    else
+        if P >= N*(N-1)
+             error('Too many edges for the specified number of nodes in a directed graph');
+        end
+        link_pairs = rand_pair(1:N,1:N,P,S,[],'diff');                         % any pairs allowed
+    end
+
+    link_ind = sub2ind([N,N],link_pairs(:,1),link_pairs(:,2));                 % convert to linear index for use in matrix
+    
+    A = zeros(N,N);
+    A(link_ind) = 1;
+end
+                                                                
+if strcmp(type,'undirected')
+    % duplicate rows in columns (so that all out connections are also in connections)
+    A = A + A';
+    %A(A>1) = 1;     % if already symmetric then reset to 1!
+end
+
+%%% determine which neurons are inhibitory
+num_I = round(N*I); % number of inhibitory neurons
+I_indices = randperm(N);
+I_indices = I_indices(1:num_I);
+
+%%%%% weight matrix
+A(I_indices,:) = A(I_indices,:) .* -1; 
+if strcmp(type,'undirected')
+    A(:,I_indices) = A(:,I_indices) .* -1;
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/reachdist.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/reachdist.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/reachdist.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/reachdist.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,68 @@
+function  [R,D] = reachdist(CIJ)
+%REACHDIST      Reachability and distance matrices
+%
+%   [R,D] = reachdist(CIJ);
+%
+%   The binary reachability matrix describes reachability between all pairs
+%   of nodes. An entry (u,v)=1 means that there exists a path from node u
+%   to node v; alternatively (u,v)=0.
+%
+%   The distance matrix contains lengths of shortest paths between all
+%   pairs of nodes. An entry (u,v) represents the length of shortest path 
+%   from node u to  node v. The average shortest path length is the 
+%   characteristic path length of the network.
+%
+%   Input:      CIJ,     binary (directed/undirected) connection matrix
+%
+%   Outputs:    R,       reachability matrix
+%               D,       distance matrix
+%
+%   Note: faster but more memory intensive than "breadthdist.m".
+%
+%   Algorithm: algebraic path count.
+%
+%
+%   Olaf Sporns, Indiana University, 2002/2007/2008
+
+% initialize
+R = CIJ;
+D = CIJ;
+powr = 2;
+N = size(CIJ,1);
+CIJpwr = CIJ;
+
+% Check for vertices that have no incoming or outgoing connections.
+% These are "ignored" by 'reachdist'.
+id = sum(CIJ,1);       % indegree = column sum of CIJ
+od = sum(CIJ,2)';      % outdegree = row sum of CIJ
+id_0 = find(id==0);    % nothing goes in, so column(R) will be 0
+od_0 = find(od==0);    % nothing comes out, so row(R) will be 0
+% Use these columns and rows to check for reachability:
+col = setxor(1:N,id_0);
+row = setxor(1:N,od_0);
+
+[R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row);
+
+% "invert" CIJdist to get distances
+D = powr - D+1;
+
+% Put 'Inf' if no path found
+D(D==(N+2)) = Inf;
+D(:,id_0) = Inf;
+D(od_0,:) = Inf;
+
+
+%----------------------------------------------------------------------------
+
+function  [R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row)
+
+% Olaf Sporns, Indiana University, 2002/2008
+
+CIJpwr = CIJpwr*CIJ;
+R = double(R | ((CIJpwr)~=0));
+D = D+R;
+
+if ((powr<=N)&&(~isempty(nonzeros(R(row,col)==0)))) 
+   powr = powr+1;
+   [R,D,powr] = reachdist2(CIJ,CIJpwr,R,D,N,powr,col,row); 
+end;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/small_world_ness.m NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/small_world_ness.m
--- NeuroMiner-1-main/graphkernels_Clara/SmallWorldNess-master/small_world_ness.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/SmallWorldNess-master/small_world_ness.m	2017-03-10 02:49:40.000000000 +0100
@@ -0,0 +1,33 @@
+function [S,C,L] = small_world_ness(A,LR,CR,FLAG)
+
+% SMALL_WORLD_NESS computes small-world-ness of graph
+% [S,C,L] = SMALL_WORLD_NESS(A,LR,CR,FLAG) computes small-world-ness score S of
+% graph described by adjacency matrix A, given mean shortest path
+% length LR and mean clustering coefficient CR averaged over a random graph ensemble
+% of the same (n,m) or (n,<k>) as A [vertices, edges or mean degree].
+%
+% FLAG is a number indicating which small-world-ness value to compute:
+%   1 - raw form with Cws 
+%   2 - raw form with transitivity C (no. of triangles)
+%
+% Also returns a 2 element array O  [C L], which are the mean clustering coefficient C 
+% and mean shortest path length L of A.
+%
+% Mark Humphries 3/02/2017
+
+% [L,P] = path_length3(A);
+[~,D] = reachdist(A);  % returns Distance matrix of all pairwise distances
+L = mean(D(:));  % mean shortest path-length: including self-loops
+
+% calculate required form of C
+switch FLAG
+    case 1
+        c = clustering_coef_bu(A);  % vector of each node's C_ws
+        C = mean(c);  % mean C
+    case 2
+        C = clusttriang(A);
+end
+
+Ls = L / LR;
+Cs =  C / CR;
+S = Cs / Ls;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/WLKernel_rowsInput.m NeuroMiner-1-main.clara/graphkernels_Clara/WLKernel_rowsInput.m
--- NeuroMiner-1-main/graphkernels_Clara/WLKernel_rowsInput.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/WLKernel_rowsInput.m	2021-05-03 15:31:52.230000000 +0200
@@ -0,0 +1,18 @@
+function K = WLKernel_rowsInput(Y, height)
+    %K = zeros(size(Y,1));
+    %size(K)
+    gList = []
+    for i = 1:size(Y,1)
+        i
+        %for j = 1:size(Y,1)
+            
+        graph = array_to_graph(Y(i,:)); 
+            %gB = array_to_graph(Y(j,:)); 
+        gList = [gList, graph]; 
+    end
+    %height = 3;
+    lab = 0; 
+    KM = WL(gList, height, lab);
+    K = KM{size(KM,1)}; % which one to choose?
+        
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/WLKernel_setsInput.m NeuroMiner-1-main.clara/graphkernels_Clara/WLKernel_setsInput.m
--- NeuroMiner-1-main/graphkernels_Clara/WLKernel_setsInput.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/WLKernel_setsInput.m	2021-06-15 17:49:08.950000000 +0200
@@ -0,0 +1,22 @@
+function K = WLKernel_setsInput(Y1, Y2, height) % in case of test kernel, Y1 should be Ytest
+    if ~isequal(Y1,Y2)
+        test = 1;
+        Y = vertcat(Y1,Y2);
+    else
+        test = 0;
+        Y = Y1;
+    end
+    gList = [];
+    for i = 1:size(Y,1)  
+        graph = array_to_graph(Y(i,:)); 
+        gList = [gList, graph]; 
+    end
+    lab = 0; 
+    KM = WL(gList, height, lab);
+    K = KM{size(KM,2)}; % which one to choose?
+    if test % we want test kernel of dimension n_test x n_train
+        K = K(1:size(Y1,1),1:size(Y2,1));
+    else
+        K = [(1:size(K,1))', normalizekm(K)];
+    end     
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/WL_with_prior_sparsityThres_Ttest.m NeuroMiner-1-main.clara/graphkernels_Clara/WL_with_prior_sparsityThres_Ttest.m
--- NeuroMiner-1-main/graphkernels_Clara/WL_with_prior_sparsityThres_Ttest.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/WL_with_prior_sparsityThres_Ttest.m	2021-06-14 12:40:26.730000000 +0200
@@ -0,0 +1,49 @@
+function K = WL_with_prior_sparsityThres_Ttest(y, Y1, Y2, sp, param1) % in case of test kernel, Y1 should be Ytest
+    
+    if ~isequal(Y1,Y2)
+        test = 1;
+        spY1 = Y1;
+        for i = 1:size(Y1,1)
+            % apply sparsity threshold
+            spArray = apply_sparsity_thres(Y1(i,:), sp);
+            spY1(i,:) = spArray;
+        end
+        spY2 = Y2; 
+        for i = 1:size(Y2,1)
+            spArray = apply_sparsity_thres(Y2(i,:),sp);
+            spY2(i,:) = spArray;
+        end
+        % feature selection with t-test
+        selectedY1 = featSelectT(spY1, spY1, y, 1); % training
+        selectedY1 = featSelectT(spY1, spY2, y, 0); % testing
+        Y = vertcat(selectedY1,selectedY2);
+    else
+        test = 0;
+        spY1 = Y1;
+        for i = 1:size(Y1,1)
+            % apply sparsity threshold
+            spArray = apply_sparsity_thres(Y1(i,:), sp);
+            spY1(i,:) = spArray;
+        end
+        selectedY1 = featSelectT(spY1, spY1, y, 1); % training
+        Y = selectedY1;
+    end
+    %gCell = {};
+    gList = [];
+    for i = 1:size(Y,1)
+        graph = array_to_graph(Y(i,:)); 
+        %gCell(1,i) = {graph};
+        gList = [gList, graph];
+    end
+    lab = 0; 
+    KM = feval("WL", gList, param1, lab);
+    KM
+    K = KM{size(KM,2)}; % which one to choose?
+    
+    if test % we want test kernel of dimension n_test x n_train
+        K = K(1:size(Y1,1),1:size(Y2,1));
+    else
+        K = [(1:size(K,1))', normalizekm(K)];
+    
+    end     
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/WL_with_prior_sparsity_thresholding.m NeuroMiner-1-main.clara/graphkernels_Clara/WL_with_prior_sparsity_thresholding.m
--- NeuroMiner-1-main/graphkernels_Clara/WL_with_prior_sparsity_thresholding.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/WL_with_prior_sparsity_thresholding.m	2021-06-18 14:50:38.080000000 +0200
@@ -0,0 +1,27 @@
+function K = WL_with_prior_sparsity_thresholding(Y1, Y2, sp, param1) % in case of test kernel, Y1 should be Ytest
+    if ~isequal(Y1,Y2)
+        test = 1;
+        Y = vertcat(Y1,Y2);
+    else
+        test = 0;
+        Y = Y1;
+    end
+    gList = [];
+    for i = 1:size(Y,1) 
+        % apply sparsity threshold
+        spArray = apply_sparsity_thres2(Y(i,:), sp);
+        % transform to 2D matrix
+        graph = array_to_graph(spArray); 
+        
+        gList = [gList, graph]; 
+        
+    end
+    lab = 0; 
+    KM = feval("WL", gList, param1, lab);
+    K = KM{size(KM,2)}; % which one to choose?
+    if test % we want test kernel of dimension n_test x n_train
+        K = K(1:size(Y1,1),1:size(Y2,1));
+    else
+        K = [(1:size(K,1))', normalizekm(K)];
+    end     
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/apply_sparsity_thres.m NeuroMiner-1-main.clara/graphkernels_Clara/apply_sparsity_thres.m
--- NeuroMiner-1-main/graphkernels_Clara/apply_sparsity_thres.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/apply_sparsity_thres.m	2021-06-18 15:05:42.790000000 +0200
@@ -0,0 +1,18 @@
+function S = apply_sparsity_thres(A, thres)
+% if mod(sqrt(size(A,2),1)) == 0 % the full connectivity matrix is entered
+%     nEdges = (size(A,2)*thres)-size(A,2); % number of edges minus diagonal
+% else % only upper triangle (without diagonal was entered)
+nEdges = size(A,2)*thres; % number of edges
+% end
+nEdges = ceil(nEdges);
+%sorteA = sort(A, 'descend');
+
+S = A;
+for i = 1:size(A,1)
+    row = A(i,:);
+    [B,I] = maxk(row,nEdges);
+    sparseRow = row; 
+    sparseRow(setdiff(1:length(row),I)) = 0;
+    S(i,:) = sparseRow;
+end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/array_to_graph.m NeuroMiner-1-main.clara/graphkernels_Clara/array_to_graph.m
--- NeuroMiner-1-main/graphkernels_Clara/array_to_graph.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/array_to_graph.m	2021-07-02 18:55:56.500000000 +0200
@@ -0,0 +1,15 @@
+function graph = array_to_graph(a)
+
+NODES = (1+sqrt(1+(8*length(a))))/2;
+adj_m = zeros(NODES);
+
+%a(a>0) = 1;
+adj_m(triu(true(NODES),1)) = a;
+adj_m = adj_m + transpose(adj_m);
+graph.am = adj_m; %reshape(a,[sqrt(length(a)),sqrt(length(a))]);
+graph.al = cellfun(@(x) find(x),num2cell(graph.am,2),'un',0);
+nodes = 1:size(adj_m,1);
+graph.nl.values = nodes';
+end
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/array_to_graph2.m NeuroMiner-1-main.clara/graphkernels_Clara/array_to_graph2.m
--- NeuroMiner-1-main/graphkernels_Clara/array_to_graph2.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/array_to_graph2.m	2021-06-14 15:22:44.380000000 +0200
@@ -0,0 +1,23 @@
+
+function graph = array_to_graph2(a)
+% binarize adjacency matrix
+a(a>0) = 1;
+
+am = reshape(a,[sqrt(length(a)),sqrt(length(a))]);
+graph.edges = adj2edge(am);
+%graph.edges = cellfun(@(x) find(x),num2cell(am,2),'un',0);
+nodes = 1:sqrt(length(a));
+graph.nodelabels = nodes';
+end
+
+
+function el=adj2edge(adj)
+    n=length(adj); % number of nodes
+    edges=find(triu(adj>0));  % indices of all edges
+    el=[];
+    for e=1:length(edges)
+        [i,j]=ind2sub([n,n],edges(e)); % node indices of edge e  
+        el=[el; i j adj(i,j)];
+    end
+    el = el(:,1:2);
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/featSelectT.m NeuroMiner-1-main.clara/graphkernels_Clara/featSelectT.m
--- NeuroMiner-1-main/graphkernels_Clara/featSelectT.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/featSelectT.m	2021-06-14 12:59:14.230000000 +0200
@@ -0,0 +1,28 @@
+function selectedY = featSelectT(X1, X2, y, train) % y = labels
+
+    if train
+        
+        g1_idx = find(y==-1);
+        g2_idx = find(y==1);
+        selectedY = X1; 
+        for i = 1:size(X1,2)
+            g1 = X1(g1_idx,i);
+            g2 = X1(g2_idx,i);
+            [h,p] = ttest2(g1,g2);
+            if h == 0 || isnan(h)
+                selectedY(:,i) = 0;
+            else 
+                i
+                h
+            end
+                
+        end
+    else
+        selectedY = X2;
+        for i = 1:size(X2,2)
+            if sum(X1(:,i) == 0
+                selectedY(:,i) = 0;
+            end
+        end
+    end
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/graphMetrics_config.m NeuroMiner-1-main.clara/graphkernels_Clara/graphMetrics_config.m
--- NeuroMiner-1-main/graphkernels_Clara/graphMetrics_config.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/graphMetrics_config.m	2021-06-26 14:11:47.160000000 +0200
@@ -0,0 +1,36 @@
+function [ GRAPHMETRICS, PX, act ] = graphMetrics_config(GRAPHMETRICS, PX, parentstr, defaultsfl)
+
+MetricsYes = 1; 
+
+if ~exist('defaultsfl','var') || isempty(defaultsfl); defaultsfl = false; end
+
+if ~defaultsfl
+   
+
+    if isfield(GRAPHMETRICS,'metric'), MetricsYes = GRAPHMETRICS.metrics; end
+
+    if MetricsYes == 1, METRICSSTR = 'yes'; else METRICSSTR = 'no'; end
+        
+    menustr = ['Compute network metrics [' METRICSSTR ']'];
+    menuact = 1;
+    
+    nk_PrintLogo
+    mestr = 'Graph metrics'; navistr = [parentstr ' >>> ' mestr]; cprintf('*blue','\nYou are here: %s >>> ',parentstr); 
+    act = nk_input(mestr,0,'mq', menustr, menuact);
+    
+    switch act
+        case 1
+            if MetricsYes == 1, MetricsYes = 2; elseif MetricsYes == 2, MetricsYes = 1; end
+    end
+else
+    act = 0;
+end
+GRAPHMETRICS.metric = MetricsYes;
+if exist('PX','var') && ~isempty(PX) && ...
+        isfield (PX,'Px') && ...
+        isfield(PX.Px,'Params') && ...
+        ~isempty(PX.Px.Params)
+    PX.opt = allcomb(PX.Px.Params,'matlab'); 
+else
+    PX.opt = [];
+end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/graphSparsity_config.m NeuroMiner-1-main.clara/graphkernels_Clara/graphSparsity_config.m
--- NeuroMiner-1-main/graphkernels_Clara/graphSparsity_config.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/graphSparsity_config.m	2021-06-26 14:09:40.540000000 +0200
@@ -0,0 +1,51 @@
+function [ GRAPHSPARSITY, PX, act ] = graphSparsity_config(GRAPHSPARSITY, PX, parentstr, defaultsfl)
+
+SparsityPerc = []; 
+
+if ~exist('defaultsfl','var') || isempty(defaultsfl); defaultsfl = false; end
+
+if ~defaultsfl
+   
+
+    if isfield(GRAPHSPARSITY,'perc') && ~isempty(GRAPHSPARSITY.perc), SparsityPerc = GRAPHSPARSITY.perc; end
+
+    if ~isempty(SparsityPerc)
+        PercDef = 1;
+        SPARSITYPERCSTR = ['yes, ' nk_ConcatParamstr(SparsityPerc)];
+    else
+        PercDef = 2;
+        SPARSITYPERCSTR = 'no';
+    end
+        
+    menustr = ['Define sparsity threshold(s) [' SPARSITYPERCSTR ']'];
+    menuact = 1;
+    
+    nk_PrintLogo
+    mestr = 'Graph sparsity threshold setup'; navistr = [parentstr ' >>> ' mestr]; cprintf('*blue','\nYou are here: %s >>> ',parentstr); 
+    act = nk_input(mestr,0,'mq', menustr, menuact);
+    
+    switch act
+        case 1
+            if PercDef == 1, PercDef = 2; elseif PercDef == 2, PercDef = 1; end
+            if PercDef == 1 
+                SparsityPerc = nk_input('Define percentage(s) to determine edge weight cutoff',0, 'e', SparsityPerc);
+                PX = nk_AddParam(SparsityPerc, 'SparsityPerc', 1, []);
+            else
+                SparsityPerc = [];
+            end
+    end
+else
+    act = 0;
+end
+
+GRAPHSPARSITY.perc = SparsityPerc;
+% Generate parameter array for preprocessing pipeline runner
+if exist('PX','var') && ~isempty(PX) && ...
+        isfield (PX,'Px') && ...
+        isfield(PX.Px,'Params') && ...
+        ~isempty(PX.Px.Params)
+    PX.opt = allcomb(PX.Px.Params,'matlab'); 
+else
+    PX.opt = [];
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/graph_PerfGraphMetrics.m NeuroMiner-1-main.clara/graphkernels_Clara/graph_PerfGraphMetrics.m
--- NeuroMiner-1-main/graphkernels_Clara/graph_PerfGraphMetrics.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/graph_PerfGraphMetrics.m	2021-06-30 12:25:48.260000000 +0200
@@ -0,0 +1,74 @@
+function [sY,IN] = graph_PerfGraphMetrics(Y,IN)
+
+% =========================== WRAPPER FUNCTION ============================ 
+    if ~exist('IN','var'), IN = []; end
+    if iscell(Y) 
+        sY = cell(1,numel(Y)); 
+        for i=1:numel(Y), [sY{i}, IN] =  PerfGraphMetrics(Y{i}, IN); end
+    else
+        [ sY, IN ] = PerfGraphMetrics(Y, IN );
+    end
+end
+function [Y, IN] = PerfGraphMetrics(Y, IN)
+    n_nodes = (1+sqrt(1+(8*size(Y,2))))/2;
+    metricsY = zeros(size(Y,1), (n_nodes*7+3));%zeros(size(Y,1),(n_nodes*6+3));
+    for i = 1:size(Y,1)
+        G_struct = array_to_graph(Y(i,:));
+        %G = graph(G_struct.am);
+        A = G_struct.am;
+        degree = degrees_und(A);
+        strength = strengths_und(A);
+        betweenness = betweenness_wei(A);
+        %pagerank = pagerank_centrality(A);
+        clustering_coef = clustering_coef_wu(A);
+        transitivity = transitivity_wu(A);
+        efficiency_loc = efficiency_wei(A,2);
+        %distance = distance_wei(A);
+        [lambda,efficiency,ecc,radius,diameter] = charpath(A);
+        efficiency = efficiency_wei(A);
+        eigenvec = eigenvector_centrality_und(A);
+        %closeness = centrality(G, 'closeness', 'Cost', G.Edges.Weight);
+        %strength = centrality(G, 'degree', 'Importance',G.Edges.Weight); 
+        %degree = centrality(G, 'degree');
+        %betweenness = centrality(G, 'betweenness', 'Cost', G.Edges.Weight);
+        %eigenvector = centrality(G, 'eigenvector', 'Importance', G.Edges.Weight);
+        
+%         A = G_struct.am;
+%         A(A >0) = 1; 
+%         clustcoef = clustering_coef_bu(A);
+        
+        % smallworldness: https://github.com/mdhumphries/SmallWorldNess/blob/master/Computing_And_Testing_S_On_Data_Network.m
+        % get its basic properties
+%         n = size(A,1);  % number of nodes
+%         k = sum(A);  % degree distribution of undirected network
+%         m = sum(k)/2;
+%         K = mean(k); % mean degree of network
+%         FLAG_Cws = 1;
+%         [expectedC,expectedL] = ER_Expected_L_C(K,n);  % L_rand and C_rand
+%         [S_ws,C_ws,L] = small_world_ness(A,expectedL,expectedC,FLAG_Cws);  % Using WS clustering coefficient
+        
+%         metrics_vector = [closeness', ...
+%             strength', ...
+% %             degree', ...
+% %             betweenness', ...
+% %             eigenvector', ...
+% %             clustcoef', ...
+%             S_ws, C_ws, L];
+        metrics_vector = [degree, ...
+            strength, ...
+            betweenness', ...
+            clustering_coef', ...
+            eigenvec', ...
+            efficiency_loc', ...
+            ecc', ...
+            efficiency, ...
+            diameter, ...
+            transitivity];
+            
+            
+        metricsY(i,:) = metrics_vector;
+    end
+Y = metricsY;
+end
+
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/graph_PerfSparsityThres.m NeuroMiner-1-main.clara/graphkernels_Clara/graph_PerfSparsityThres.m
--- NeuroMiner-1-main/graphkernels_Clara/graph_PerfSparsityThres.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/graph_PerfSparsityThres.m	2021-06-26 16:29:00.190000000 +0200
@@ -0,0 +1,37 @@
+function [sY, IN] = graph_PerfSparsityThres(Y, IN)
+% =========================================================================
+% FORMAT function [sY, IN] = nk_PerfElimZeroObj(Y, IN)
+% =========================================================================
+% Remove features with zero-variance, and ANY Infs and NaNs.
+% Furthermore, remove features with highly skewed distributions
+% I/O arguments:
+% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+% (c) Nikolaos Koutsouleris, 05/2018
+
+% =========================== WRAPPER FUNCTION ============================ 
+    if ~exist('IN','var'), IN = []; end
+    if iscell(Y) 
+        sY = cell(1,numel(Y)); 
+        for i=1:numel(Y), [sY{i}, IN] =  PerfSparsityThres(Y{i}, IN); end
+    else
+        [ sY, IN ] = PerfSparsityThres(Y, IN );
+    end
+end
+
+% =========================================================================
+function [Y, IN] = PerfSparsityThres(Y, IN)
+
+    %global VERBOSE SVM
+% Defaults
+%if isempty(IN),eIN=true; else eIN=false; end
+
+%if eIN  
+
+    if ~isempty(IN.perc)
+        R = apply_sparsity_thres(Y, IN.p); 
+    else
+        R = Y;
+    end
+    
+Y = R;
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/graphkernels_Clara/sparseCellArray.m NeuroMiner-1-main.clara/graphkernels_Clara/sparseCellArray.m
--- NeuroMiner-1-main/graphkernels_Clara/sparseCellArray.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/graphkernels_Clara/sparseCellArray.m	2021-06-14 15:13:04.690000000 +0200
@@ -0,0 +1,19 @@
+function gCell = sparseCellArray(Y1, Y2, sp) % in case of test kernel, Y1 should be Ytest
+    if ~isequal(Y1,Y2)
+        test = 1;
+        Y = vertcat(Y1,Y2);
+    else
+        test = 0;
+        Y = Y1;
+    end
+    gCell= {};
+    for i = 1:size(Y,1) 
+        % apply sparsity threshold
+        spArray = apply_sparsity_thres(Y(i,:), sp);
+        % transform to 2D matrix
+        graph = array_to_graph2(spArray); 
+        gCell(1,i) = {graph};
+        %gList = [gList, graph]; 
+        
+    end
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/gridsearch/nk_MLOptimizer.m NeuroMiner-1-main.clara/gridsearch/nk_MLOptimizer.m
--- NeuroMiner-1-main/gridsearch/nk_MLOptimizer.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/gridsearch/nk_MLOptimizer.m	2021-05-04 13:53:49.660000000 +0200
@@ -7,7 +7,8 @@
 % (c) Nikolaos Koutsouleris, 01/2020
 function GDanalysis = nk_MLOptimizer(inp, strout, id, GridAct, batchflag)
 
-global SVM RFE SAV GRD MULTI MODEFL BATCH ENSEMBLE MKLRVM CV DATID CL RAND MULTILABEL PREPROC TEMPL W2AVAIL
+global SVM RFE SAV GRD MULTI MODEFL BATCH ENSEMBLE MKLRVM CV DATID CL RAND MULTILABEL PREPROC TEMPL W2AVAIL 
+
 
 CL = {'b*-','r*-','g*-','y*-','m*-','c*-','k*-'};
 W2AVAIL = false;
@@ -440,6 +441,7 @@
             if detrendfl, GD.Detrend = cell(nPs(1),hx); end
             if isfield(RFE.Wrapper,'optflag') && RFE.Wrapper.optflag == 1, RFE.Wrapper.flag = 0; end
             
+            
             %%%%%%%%%%%%%%%% PARAMETER OPTIMIZATION %%%%%%%%%%%%%%%%
             [ GD, MD ] = nk_MLOptimizer_ParamCycler(GD, MD, DISP, Ps, Params_desc, mapY, algostr, f, d, n_preml, nclass, ngroups, batchflag, [], combcell);
             
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/gridsearch/nk_MLOptimizer_main.m NeuroMiner-1-main.clara/gridsearch/nk_MLOptimizer_main.m
--- NeuroMiner-1-main/gridsearch/nk_MLOptimizer_main.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/gridsearch/nk_MLOptimizer_main.m	2021-05-19 19:31:00.640000000 +0200
@@ -1,6 +1,6 @@
 function analysis = nk_MLOptimizer_main(inp, datid, PreML)
 
-global SVM GRD MODEFL W2AVAIL VERBOSE BATCH
+global SVM GRD MODEFL W2AVAIL VERBOSE BATCH inparams
 
 % Write some info to command line
 clc
@@ -108,10 +108,20 @@
             if VERBOSE, fprintf('\n%s #%g: no slack parameters needed.',strout,i); end
     end
 
+    %% Add precomputed kernel 
+%     switch inparams.stranalysis
+%         case 'precomputed'
+%             SVM.kernel.kernstr = '-t 4';
+%             global WL; 
+%             WL = '-t 4';
+%             fprintf('Precomputed kernel');
+%         otherwise
+%             fprintf('No precomputed kernel');
+%     end
     %% Define kernel parameter settings
     switch SVM.kernel.kernstr
         case {' -t 0', 'lin','linear','lin_kernel','none','lin_elm'}
-            fprintf('\n%s #%g: no kernel parameters needed.',strout,i)     
+            fprintf('\n%s #%g: no kernel parameters needed.',strout,i)   
         case {' -t 1', 'poly', 'polynomial', 'Polynomial', 'polyN', 'hpolyN'}
             inparams.Params{i}{end+1} = GRD.Gparams;
             inparams.Params_desc{i}{end+1} = 'Kernel';
@@ -127,6 +137,23 @@
             inparams.Params_desc{i}{end+1} = 'Kernel';
             inparams.Params{i}{end+1} = GRD.PolyCoefparams;
             inparams.Params_desc{i}{end+1} = 'Sigmoid coef';   
+        case ' -t 4'
+            if SVM.kernel.kerndef < 8
+                inparams.Params{i}{end+1} = GRD.WLiters;
+                inparams.Params_desc{i}{end+1} = 'Kernel';
+            else
+                if isfield(SVM.kernel,'customfunc_nargin')
+                    if SVM.kernel.customfunc_nargin >= 1
+                        for n = 1:SVM.kernel.customfunc_nargin
+                            argName = sprintf('customkernel_arg%d', n); 
+                            %GRD.(argName) = 0;
+                            inparams.Params{i}{end+1} = GRD.(argName);
+                            inparams.Params_desc{i}{end+1} = sprintf('Kernel function argument %d', n);
+                        end
+                    end
+                end
+            end
+            %fprintf('\n%s #%g: no kernel parameters for now.',strout,i) 
         otherwise
             switch SVM.prog
                 case 'LIBSVM'
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/COPYRIGHT NeuroMiner-1-main.clara/libsvm-string-3.25/COPYRIGHT
--- NeuroMiner-1-main/libsvm-string-3.25/COPYRIGHT	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/COPYRIGHT	2021-04-26 09:41:34.000000000 +0200
@@ -0,0 +1,31 @@
+
+Copyright (c) 2000-2021 Chih-Chung Chang and Chih-Jen Lin
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in the
+documentation and/or other materials provided with the distribution.
+
+3. Neither name of copyright holders nor the names of its contributors
+may be used to endorse or promote products derived from this software
+without specific prior written permission.
+
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/Makefile NeuroMiner-1-main.clara/libsvm-string-3.25/Makefile
--- NeuroMiner-1-main/libsvm-string-3.25/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/Makefile	2021-04-26 09:41:34.000000000 +0200
@@ -0,0 +1,25 @@
+CXX ?= g++
+CFLAGS = -Wall -Wconversion -O3 -fPIC -D _STRING
+SHVER = 2
+OS = $(shell uname)
+
+all: svm-train svm-predict svm-scale
+
+lib: svm.o
+	if [ "$(OS)" = "Darwin" ]; then \
+		SHARED_LIB_FLAG="-dynamiclib -Wl,-install_name,libsvm.so.$(SHVER)"; \
+	else \
+		SHARED_LIB_FLAG="-shared -Wl,-soname,libsvm.so.$(SHVER)"; \
+	fi; \
+	$(CXX) $${SHARED_LIB_FLAG} svm.o -o libsvm.so.$(SHVER)
+
+svm-predict: svm-predict.c svm.o
+	$(CXX) $(CFLAGS) svm-predict.c svm.o -o svm-predict -lm
+svm-train: svm-train.c svm.o
+	$(CXX) $(CFLAGS) svm-train.c svm.o -o svm-train -lm
+svm-scale: svm-scale.c
+	$(CXX) $(CFLAGS) svm-scale.c -o svm-scale
+svm.o: svm.cpp svm.h
+	$(CXX) $(CFLAGS) -c svm.cpp
+clean:
+	rm -f *~ svm.o svm-train svm-predict svm-scale libsvm.so.$(SHVER)
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/README NeuroMiner-1-main.clara/libsvm-string-3.25/README
--- NeuroMiner-1-main/libsvm-string-3.25/README	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/README	2021-04-26 09:41:34.000000000 +0200
@@ -0,0 +1,778 @@
+Libsvm is a simple, easy-to-use, and efficient software for SVM
+classification and regression. It solves C-SVM classification, nu-SVM
+classification, one-class-SVM, epsilon-SVM regression, and nu-SVM
+regression. It also provides an automatic model selection tool for
+C-SVM classification. This document explains the use of libsvm.
+
+Libsvm is available at
+http://www.csie.ntu.edu.tw/~cjlin/libsvm
+Please read the COPYRIGHT file before using libsvm.
+
+Table of Contents
+=================
+
+- Quick Start
+- Installation and Data Format
+- `svm-train' Usage
+- `svm-predict' Usage
+- `svm-scale' Usage
+- Tips on Practical Use
+- Examples
+- Precomputed Kernels
+- Library Usage
+- Java Version
+- Building Windows Binaries
+- Additional Tools: Sub-sampling, Parameter Selection, Format checking, etc.
+- MATLAB/OCTAVE Interface
+- Python Interface
+- Additional Information
+
+Quick Start
+===========
+
+If you are new to SVM and if the data is not large, please go to
+`tools' directory and use easy.py after installation. It does
+everything automatic -- from data scaling to parameter selection.
+
+Usage: easy.py training_file [testing_file]
+
+More information about parameter selection can be found in
+`tools/README.'
+
+Installation and Data Format
+============================
+
+On Unix systems, type `make' to build the `svm-train', `svm-predict',
+and `svm-scale' programs. Run them without arguments to show the
+usages of them.
+
+On other systems, consult `Makefile' to build them (e.g., see
+'Building Windows binaries' in this file) or use the pre-built
+binaries (Windows binaries are in the directory `windows').
+
+The format of training and testing data files is:
+
+<label> <index1>:<value1> <index2>:<value2> ...
+.
+.
+.
+
+Each line contains an instance and is ended by a '\n' character. For
+<label> in the training set, we have the following cases.
+
+* classification: <label> is an integer indicating the class label
+  (multi-class is supported).
+
+* For regression, <label> is the target value which can be any real
+  number.
+
+* For one-class SVM, <label> is not used and can be any number.
+
+In the test set, <label> is used only to calculate accuracy or
+errors. If it's unknown, any number is fine. For one-class SVM, if
+non-outliers/outliers are known, their labels in the test file must be
++1/-1 for evaluation.
+
+The pair <index>:<value> gives a feature (attribute) value: <index> is
+an integer starting from 1 and <value> is a real number. The only
+exception is the precomputed kernel, where <index> starts from 0; see
+the section of precomputed kernels. Indices must be in ASCENDING
+order.
+
+A sample classification data included in this package is
+`heart_scale'. To check if your data is in a correct form, use
+`tools/checkdata.py' (details in `tools/README').
+
+Type `svm-train heart_scale', and the program will read the training
+data and output the model file `heart_scale.model'. If you have a test
+set called heart_scale.t, then type `svm-predict heart_scale.t
+heart_scale.model output' to see the prediction accuracy. The `output'
+file contains the predicted class labels.
+
+For classification, if training data are in only one class (i.e., all
+labels are the same), then `svm-train' issues a warning message:
+`Warning: training data in only one class. See README for details,'
+which means the training data is very unbalanced. The label in the
+training data is directly returned when testing.
+
+There are some other useful programs in this package.
+
+svm-scale:
+
+	This is a tool for scaling input data file.
+
+svm-toy:
+
+	This is a simple graphical interface which shows how SVM
+	separate data in a plane. You can click in the window to
+	draw data points. Use "change" button to choose class
+	1, 2 or 3 (i.e., up to three classes are supported), "load"
+	button to load data from a file, "save" button to save data to
+	a file, "run" button to obtain an SVM model, and "clear"
+	button to clear the window.
+
+	You can enter options in the bottom of the window, the syntax of
+	options is the same as `svm-train'.
+
+	Note that "load" and "save" consider dense data format both in
+	classification and the regression cases. For classification,
+	each data point has one label (the color) that must be 1, 2,
+	or 3 and two attributes (x-axis and y-axis values) in
+	[0,1). For regression, each data point has one target value
+	(y-axis) and one attribute (x-axis values) in [0, 1).
+
+	Type `make' in respective directories to build them.
+
+	You need Qt library to build the Qt version.
+	(available from http://www.trolltech.com)
+
+	You need GTK+ library to build the GTK version.
+	(available from http://www.gtk.org)
+
+	The pre-built Windows binaries are in the `windows'
+	directory. We use Visual C++ on a 64-bit machine.
+
+`svm-train' Usage
+=================
+
+Usage: svm-train [options] training_set_file [model_file]
+options:
+-s svm_type : set type of SVM (default 0)
+	0 -- C-SVC		(multi-class classification)
+	1 -- nu-SVC		(multi-class classification)
+	2 -- one-class SVM
+	3 -- epsilon-SVR	(regression)
+	4 -- nu-SVR		(regression)
+-t kernel_type : set type of kernel function (default 2)
+	0 -- linear: u'*v
+	1 -- polynomial: (gamma*u'*v + coef0)^degree
+	2 -- radial basis function: exp(-gamma*|u-v|^2)
+	3 -- sigmoid: tanh(gamma*u'*v + coef0)
+	4 -- precomputed kernel (kernel values in training_set_file)
+-d degree : set degree in kernel function (default 3)
+-g gamma : set gamma in kernel function (default 1/num_features)
+-r coef0 : set coef0 in kernel function (default 0)
+-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)
+-n nu : set the parameter nu of nu-SVC, one-class SVM, and nu-SVR (default 0.5)
+-p epsilon : set the epsilon in loss function of epsilon-SVR (default 0.1)
+-m cachesize : set cache memory size in MB (default 100)
+-e epsilon : set tolerance of termination criterion (default 0.001)
+-h shrinking : whether to use the shrinking heuristics, 0 or 1 (default 1)
+-b probability_estimates : whether to train a SVC or SVR model for probability estimates, 0 or 1 (default 0)
+-wi weight : set the parameter C of class i to weight*C, for C-SVC (default 1)
+-v n: n-fold cross validation mode
+-q : quiet mode (no outputs)
+
+
+option -v randomly splits the data into n parts and calculates cross
+validation accuracy/mean squared error on them.
+
+See libsvm FAQ for the meaning of outputs.
+
+`svm-predict' Usage
+===================
+
+Usage: svm-predict [options] test_file model_file output_file
+options:
+-b probability_estimates: whether to predict probability estimates, 0 or 1 (default 0); for one-class SVM only 0 is supported
+
+model_file is the model file generated by svm-train.
+test_file is the test data you want to predict.
+svm-predict will produce output in the output_file.
+
+`svm-scale' Usage
+=================
+
+Usage: svm-scale [options] data_filename
+options:
+-l lower : x scaling lower limit (default -1)
+-u upper : x scaling upper limit (default +1)
+-y y_lower y_upper : y scaling limits (default: no y scaling)
+-s save_filename : save scaling parameters to save_filename
+-r restore_filename : restore scaling parameters from restore_filename
+
+See 'Examples' in this file for examples.
+
+Tips on Practical Use
+=====================
+
+* Scale your data. For example, scale each attribute to [0,1] or [-1,+1].
+* For C-SVC, consider using the model selection tool in the tools directory.
+* nu in nu-SVC/one-class-SVM/nu-SVR approximates the fraction of training
+  errors and support vectors.
+* If data for classification are unbalanced (e.g. many positive and
+  few negative), try different penalty parameters C by -wi (see
+  examples below).
+* Specify larger cache size (i.e., larger -m) for huge problems.
+
+Examples
+========
+
+> svm-scale -l -1 -u 1 -s range train > train.scale
+> svm-scale -r range test > test.scale
+
+Scale each feature of the training data to be in [-1,1]. Scaling
+factors are stored in the file range and then used for scaling the
+test data.
+
+> svm-train -s 0 -c 5 -t 2 -g 0.5 -e 0.1 data_file
+
+Train a classifier with RBF kernel exp(-0.5|u-v|^2), C=10, and
+stopping tolerance 0.1.
+
+> svm-train -s 3 -p 0.1 -t 0 data_file
+
+Solve SVM regression with linear kernel u'v and epsilon=0.1
+in the loss function.
+
+> svm-train -c 10 -w1 1 -w-2 5 -w4 2 data_file
+
+Train a classifier with penalty 10 = 1 * 10 for class 1, penalty 50 =
+5 * 10 for class -2, and penalty 20 = 2 * 10 for class 4.
+
+> svm-train -s 0 -c 100 -g 0.1 -v 5 data_file
+
+Do five-fold cross validation for the classifier using
+the parameters C = 100 and gamma = 0.1
+
+> svm-train -s 0 -b 1 data_file
+> svm-predict -b 1 test_file data_file.model output_file
+
+Obtain a model with probability information and predict test data with
+probability estimates
+
+Precomputed Kernels
+===================
+
+Users may precompute kernel values and input them as training and
+testing files.  Then libsvm does not need the original
+training/testing sets.
+
+Assume there are L training instances x1, ..., xL and.
+Let K(x, y) be the kernel
+value of two instances x and y. The input formats
+are:
+
+New training instance for xi:
+
+<label> 0:i 1:K(xi,x1) ... L:K(xi,xL)
+
+New testing instance for any x:
+
+<label> 0:? 1:K(x,x1) ... L:K(x,xL)
+
+That is, in the training file the first column must be the "ID" of
+xi. In testing, ? can be any value.
+
+All kernel values including ZEROs must be explicitly provided.  Any
+permutation or random subsets of the training/testing files are also
+valid (see examples below).
+
+Note: the format is slightly different from the precomputed kernel
+package released in libsvmtools earlier.
+
+Examples:
+
+	Assume the original training data has three four-feature
+	instances and testing data has one instance:
+
+	15  1:1 2:1 3:1 4:1
+	45      2:3     4:3
+	25          3:1
+
+	15  1:1     3:1
+
+	If the linear kernel is used, we have the following new
+	training/testing sets:
+
+	15  0:1 1:4 2:6  3:1
+	45  0:2 1:6 2:18 3:0
+	25  0:3 1:1 2:0  3:1
+
+	15  0:? 1:2 2:0  3:1
+
+	? can be any value.
+
+	Any subset of the above training file is also valid. For example,
+
+	25  0:3 1:1 2:0  3:1
+	45  0:2 1:6 2:18 3:0
+
+	implies that the kernel matrix is
+
+		[K(2,2) K(2,3)] = [18 0]
+		[K(3,2) K(3,3)] = [0  1]
+
+Library Usage
+=============
+
+These functions and structures are declared in the header file
+`svm.h'.  You need to #include "svm.h" in your C/C++ source files and
+link your program with `svm.cpp'. You can see `svm-train.c' and
+`svm-predict.c' for examples showing how to use them. We define
+LIBSVM_VERSION and declare `extern int libsvm_version;' in svm.h, so
+you can check the version number.
+
+Before you classify test data, you need to construct an SVM model
+(`svm_model') using training data. A model can also be saved in
+a file for later use. Once an SVM model is available, you can use it
+to classify new data.
+
+- Function: struct svm_model *svm_train(const struct svm_problem *prob,
+					const struct svm_parameter *param);
+
+    This function constructs and returns an SVM model according to
+    the given training data and parameters.
+
+    struct svm_problem describes the problem:
+
+	struct svm_problem
+	{
+		int l;
+		double *y;
+		struct svm_node **x;
+	};
+
+    where `l' is the number of training data, and `y' is an array containing
+    their target values. (integers in classification, real numbers in
+    regression) `x' is an array of pointers, each of which points to a sparse
+    representation (array of svm_node) of one training vector.
+
+    For example, if we have the following training data:
+
+    LABEL    ATTR1    ATTR2    ATTR3    ATTR4    ATTR5
+    -----    -----    -----    -----    -----    -----
+      1        0        0.1      0.2      0        0
+      2        0        0.1      0.3     -1.2      0
+      1        0.4      0        0        0        0
+      2        0        0.1      0        1.4      0.5
+      3       -0.1     -0.2      0.1      1.1      0.1
+
+    then the components of svm_problem are:
+
+    l = 5
+
+    y -> 1 2 1 2 3
+
+    x -> [ ] -> (2,0.1) (3,0.2) (-1,?)
+         [ ] -> (2,0.1) (3,0.3) (4,-1.2) (-1,?)
+         [ ] -> (1,0.4) (-1,?)
+         [ ] -> (2,0.1) (4,1.4) (5,0.5) (-1,?)
+         [ ] -> (1,-0.1) (2,-0.2) (3,0.1) (4,1.1) (5,0.1) (-1,?)
+
+    where (index,value) is stored in the structure `svm_node':
+
+	struct svm_node
+	{
+		int index;
+		double value;
+	};
+
+    index = -1 indicates the end of one vector. Note that indices must
+    be in ASCENDING order.
+
+    struct svm_parameter describes the parameters of an SVM model:
+
+	struct svm_parameter
+	{
+		int svm_type;
+		int kernel_type;
+		int degree;	/* for poly */
+		double gamma;	/* for poly/rbf/sigmoid */
+		double coef0;	/* for poly/sigmoid */
+
+		/* these are for training only */
+		double cache_size; /* in MB */
+		double eps;	/* stopping criteria */
+		double C;	/* for C_SVC, EPSILON_SVR, and NU_SVR */
+		int nr_weight;		/* for C_SVC */
+		int *weight_label;	/* for C_SVC */
+		double* weight;		/* for C_SVC */
+		double nu;	/* for NU_SVC, ONE_CLASS, and NU_SVR */
+		double p;	/* for EPSILON_SVR */
+		int shrinking;	/* use the shrinking heuristics */
+		int probability; /* do probability estimates */
+	};
+
+    svm_type can be one of C_SVC, NU_SVC, ONE_CLASS, EPSILON_SVR, NU_SVR.
+
+    C_SVC:		C-SVM classification
+    NU_SVC:		nu-SVM classification
+    ONE_CLASS:		one-class-SVM
+    EPSILON_SVR:	epsilon-SVM regression
+    NU_SVR:		nu-SVM regression
+
+    kernel_type can be one of LINEAR, POLY, RBF, SIGMOID.
+
+    LINEAR:	u'*v
+    POLY:	(gamma*u'*v + coef0)^degree
+    RBF:	exp(-gamma*|u-v|^2)
+    SIGMOID:	tanh(gamma*u'*v + coef0)
+    PRECOMPUTED: kernel values in training_set_file
+
+    cache_size is the size of the kernel cache, specified in megabytes.
+    C is the cost of constraints violation.
+    eps is the stopping criterion. (we usually use 0.00001 in nu-SVC,
+    0.001 in others). nu is the parameter in nu-SVM, nu-SVR, and
+    one-class-SVM. p is the epsilon in epsilon-insensitive loss function
+    of epsilon-SVM regression. shrinking = 1 means shrinking is conducted;
+    = 0 otherwise. probability = 1 means model with probability
+    information is obtained; = 0 otherwise.
+
+    nr_weight, weight_label, and weight are used to change the penalty
+    for some classes (If the weight for a class is not changed, it is
+    set to 1). This is useful for training classifier using unbalanced
+    input data or with asymmetric misclassification cost.
+
+    nr_weight is the number of elements in the array weight_label and
+    weight. Each weight[i] corresponds to weight_label[i], meaning that
+    the penalty of class weight_label[i] is scaled by a factor of weight[i].
+
+    If you do not want to change penalty for any of the classes,
+    just set nr_weight to 0.
+
+    *NOTE* Because svm_model contains pointers to svm_problem, you can
+    not free the memory used by svm_problem if you are still using the
+    svm_model produced by svm_train().
+
+    *NOTE* To avoid wrong parameters, svm_check_parameter() should be
+    called before svm_train().
+
+    struct svm_model stores the model obtained from the training procedure.
+    It is not recommended to directly access entries in this structure.
+    Programmers should use the interface functions to get the values.
+
+	struct svm_model
+	{
+		struct svm_parameter param;	/* parameter */
+		int nr_class;		/* number of classes, = 2 in regression/one class svm */
+		int l;			/* total #SV */
+		struct svm_node **SV;		/* SVs (SV[l]) */
+		double **sv_coef;	/* coefficients for SVs in decision functions (sv_coef[k-1][l]) */
+		double *rho;		/* constants in decision functions (rho[k*(k-1)/2]) */
+		double *probA;		/* pairwise probability information */
+		double *probB;
+		int *sv_indices;        /* sv_indices[0,...,nSV-1] are values in [1,...,num_traning_data] to indicate SVs in the training set */
+
+		/* for classification only */
+
+		int *label;		/* label of each class (label[k]) */
+		int *nSV;		/* number of SVs for each class (nSV[k]) */
+					/* nSV[0] + nSV[1] + ... + nSV[k-1] = l */
+		/* XXX */
+		int free_sv;		/* 1 if svm_model is created by svm_load_model*/
+					/* 0 if svm_model is created by svm_train */
+	};
+
+    param describes the parameters used to obtain the model.
+
+    nr_class is the number of classes. It is 2 for regression and one-class SVM.
+
+    l is the number of support vectors. SV and sv_coef are support
+    vectors and the corresponding coefficients, respectively. Assume there are
+    k classes. For data in class j, the corresponding sv_coef includes (k-1) y*alpha vectors,
+    where alpha's are solutions of the following two class problems:
+    1 vs j, 2 vs j, ..., j-1 vs j, j vs j+1, j vs j+2, ..., j vs k
+    and y=1 for the first j-1 vectors, while y=-1 for the remaining k-j
+    vectors. For example, if there are 4 classes, sv_coef and SV are like:
+
+        +-+-+-+--------------------+
+        |1|1|1|                    |
+        |v|v|v|  SVs from class 1  |
+        |2|3|4|                    |
+        +-+-+-+--------------------+
+        |1|2|2|                    |
+        |v|v|v|  SVs from class 2  |
+        |2|3|4|                    |
+        +-+-+-+--------------------+
+        |1|2|3|                    |
+        |v|v|v|  SVs from class 3  |
+        |3|3|4|                    |
+        +-+-+-+--------------------+
+        |1|2|3|                    |
+        |v|v|v|  SVs from class 4  |
+        |4|4|4|                    |
+        +-+-+-+--------------------+
+
+    See svm_train() for an example of assigning values to sv_coef.
+
+    rho is the bias term (-b). probA and probB are parameters used in
+    probability outputs. If there are k classes, there are k*(k-1)/2
+    binary problems as well as rho, probA, and probB values. They are
+    aligned in the order of binary problems:
+    1 vs 2, 1 vs 3, ..., 1 vs k, 2 vs 3, ..., 2 vs k, ..., k-1 vs k.
+
+    sv_indices[0,...,nSV-1] are values in [1,...,num_traning_data] to
+    indicate support vectors in the training set.
+
+    label contains labels in the training data.
+
+    nSV is the number of support vectors in each class.
+
+    free_sv is a flag used to determine whether the space of SV should
+    be released in free_model_content(struct svm_model*) and
+    free_and_destroy_model(struct svm_model**). If the model is
+    generated by svm_train(), then SV points to data in svm_problem
+    and should not be removed. For example, free_sv is 0 if svm_model
+    is created by svm_train, but is 1 if created by svm_load_model.
+
+- Function: double svm_predict(const struct svm_model *model,
+                               const struct svm_node *x);
+
+    This function does classification or regression on a test vector x
+    given a model.
+
+    For a classification model, the predicted class for x is returned.
+    For a regression model, the function value of x calculated using
+    the model is returned. For an one-class model, +1 or -1 is
+    returned.
+
+- Function: void svm_cross_validation(const struct svm_problem *prob,
+	const struct svm_parameter *param, int nr_fold, double *target);
+
+    This function conducts cross validation. Data are separated to
+    nr_fold folds. Under given parameters, sequentially each fold is
+    validated using the model from training the remaining. Predicted
+    labels (of all prob's instances) in the validation process are
+    stored in the array called target.
+
+    The format of svm_prob is same as that for svm_train().
+
+- Function: int svm_get_svm_type(const struct svm_model *model);
+
+    This function gives svm_type of the model. Possible values of
+    svm_type are defined in svm.h.
+
+- Function: int svm_get_nr_class(const svm_model *model);
+
+    For a classification model, this function gives the number of
+    classes. For a regression or an one-class model, 2 is returned.
+
+- Function: void svm_get_labels(const svm_model *model, int* label)
+
+    For a classification model, this function outputs the name of
+    labels into an array called label. For regression and one-class
+    models, label is unchanged.
+
+- Function: void svm_get_sv_indices(const struct svm_model *model, int *sv_indices)
+
+    This function outputs indices of support vectors into an array called sv_indices.
+    The size of sv_indices is the number of support vectors and can be obtained by calling svm_get_nr_sv.
+    Each sv_indices[i] is in the range of [1, ..., num_traning_data].
+
+- Function: int svm_get_nr_sv(const struct svm_model *model)
+
+    This function gives the number of total support vector.
+
+- Function: double svm_get_svr_probability(const struct svm_model *model);
+
+    For a regression model with probability information, this function
+    outputs a value sigma > 0. For test data, we consider the
+    probability model: target value = predicted value + z, z: Laplace
+    distribution e^(-|z|/sigma)/(2sigma)
+
+    If the model is not for svr or does not contain required
+    information, 0 is returned.
+
+- Function: double svm_predict_values(const svm_model *model,
+				    const svm_node *x, double* dec_values)
+
+    This function gives decision values on a test vector x given a
+    model, and return the predicted label (classification) or
+    the function value (regression).
+
+    For a classification model with nr_class classes, this function
+    gives nr_class*(nr_class-1)/2 decision values in the array
+    dec_values, where nr_class can be obtained from the function
+    svm_get_nr_class. The order is label[0] vs. label[1], ...,
+    label[0] vs. label[nr_class-1], label[1] vs. label[2], ...,
+    label[nr_class-2] vs. label[nr_class-1], where label can be
+    obtained from the function svm_get_labels. The returned value is
+    the predicted class for x. Note that when nr_class = 1, this
+    function does not give any decision value.
+
+    For a regression model, dec_values[0] and the returned value are
+    both the function value of x calculated using the model. For a
+    one-class model, dec_values[0] is the decision value of x, while
+    the returned value is +1/-1.
+
+- Function: double svm_predict_probability(const struct svm_model *model,
+	    const struct svm_node *x, double* prob_estimates);
+
+    This function does classification or regression on a test vector x
+    given a model with probability information.
+
+    For a classification model with probability information, this
+    function gives nr_class probability estimates in the array
+    prob_estimates. nr_class can be obtained from the function
+    svm_get_nr_class. The class with the highest probability is
+    returned. For regression/one-class SVM, the array prob_estimates
+    is unchanged and the returned value is the same as that of
+    svm_predict.
+
+- Function: const char *svm_check_parameter(const struct svm_problem *prob,
+                                            const struct svm_parameter *param);
+
+    This function checks whether the parameters are within the feasible
+    range of the problem. This function should be called before calling
+    svm_train() and svm_cross_validation(). It returns NULL if the
+    parameters are feasible, otherwise an error message is returned.
+
+- Function: int svm_check_probability_model(const struct svm_model *model);
+
+    This function checks whether the model contains required
+    information to do probability estimates. If so, it returns
+    +1. Otherwise, 0 is returned. This function should be called
+    before calling svm_get_svr_probability and
+    svm_predict_probability.
+
+- Function: int svm_save_model(const char *model_file_name,
+			       const struct svm_model *model);
+
+    This function saves a model to a file; returns 0 on success, or -1
+    if an error occurs.
+
+- Function: struct svm_model *svm_load_model(const char *model_file_name);
+
+    This function returns a pointer to the model read from the file,
+    or a null pointer if the model could not be loaded.
+
+- Function: void svm_free_model_content(struct svm_model *model_ptr);
+
+    This function frees the memory used by the entries in a model structure.
+
+- Function: void svm_free_and_destroy_model(struct svm_model **model_ptr_ptr);
+
+    This function frees the memory used by a model and destroys the model
+    structure. It is equivalent to svm_destroy_model, which
+    is deprecated after version 3.0.
+
+- Function: void svm_destroy_param(struct svm_parameter *param);
+
+    This function frees the memory used by a parameter set.
+
+- Function: void svm_set_print_string_function(void (*print_func)(const char *));
+
+    Users can specify their output format by a function. Use
+        svm_set_print_string_function(NULL);
+    for default printing to stdout.
+
+Java Version
+============
+
+The pre-compiled java class archive `libsvm.jar' and its source files are
+in the java directory. To run the programs, use
+
+java -classpath libsvm.jar svm_train <arguments>
+java -classpath libsvm.jar svm_predict <arguments>
+java -classpath libsvm.jar svm_toy
+java -classpath libsvm.jar svm_scale <arguments>
+
+Note that you need Java 1.5 (5.0) or above to run it.
+
+You may need to add Java runtime library (like classes.zip) to the classpath.
+You may need to increase maximum Java heap size.
+
+Library usages are similar to the C version. These functions are available:
+
+public class svm {
+	public static final int LIBSVM_VERSION=325;
+	public static svm_model svm_train(svm_problem prob, svm_parameter param);
+	public static void svm_cross_validation(svm_problem prob, svm_parameter param, int nr_fold, double[] target);
+	public static int svm_get_svm_type(svm_model model);
+	public static int svm_get_nr_class(svm_model model);
+	public static void svm_get_labels(svm_model model, int[] label);
+	public static void svm_get_sv_indices(svm_model model, int[] indices);
+	public static int svm_get_nr_sv(svm_model model);
+	public static double svm_get_svr_probability(svm_model model);
+	public static double svm_predict_values(svm_model model, svm_node[] x, double[] dec_values);
+	public static double svm_predict(svm_model model, svm_node[] x);
+	public static double svm_predict_probability(svm_model model, svm_node[] x, double[] prob_estimates);
+	public static void svm_save_model(String model_file_name, svm_model model) throws IOException
+	public static svm_model svm_load_model(String model_file_name) throws IOException
+	public static String svm_check_parameter(svm_problem prob, svm_parameter param);
+	public static int svm_check_probability_model(svm_model model);
+	public static void svm_set_print_string_function(svm_print_interface print_func);
+}
+
+The library is in the "libsvm" package.
+Note that in Java version, svm_node[] is not ended with a node whose index = -1.
+
+Users can specify their output format by
+
+	your_print_func = new svm_print_interface()
+	{
+		public void print(String s)
+		{
+			// your own format
+		}
+	};
+	svm.svm_set_print_string_function(your_print_func);
+
+Building Windows Binaries
+=========================
+
+Windows binaries are available in the directory `windows'. To re-build
+them via Visual C++, use the following steps:
+
+1. Open a DOS command box (or Visual Studio Command Prompt) and change
+to libsvm directory. If environment variables of VC++ have not been
+set, type
+
+"C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvars64.bat"
+
+You may have to modify the above command according which version of
+VC++ or where it is installed.
+
+2. Type
+
+nmake -f Makefile.win clean all
+
+3. (optional) To build shared library libsvm.dll, type
+
+nmake -f Makefile.win lib
+
+4. (optional) To build 32-bit windows binaries, you must
+	(1) Setup "C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvars32.bat" instead of vcvars64.bat
+	(2) Change CFLAGS in Makefile.win: /D _WIN64 to /D _WIN32
+
+Another way is to build them from Visual C++ environment. See details
+in libsvm FAQ.
+
+- Additional Tools: Sub-sampling, Parameter Selection, Format checking, etc.
+============================================================================
+
+See the README file in the tools directory.
+
+MATLAB/OCTAVE Interface
+=======================
+
+Please check the file README in the directory `matlab'.
+
+Python Interface
+================
+
+See the README file in python directory.
+
+Additional Information
+======================
+
+If you find LIBSVM helpful, please cite it as
+
+Chih-Chung Chang and Chih-Jen Lin, LIBSVM : a library for support
+vector machines. ACM Transactions on Intelligent Systems and
+Technology, 2:27:1--27:27, 2011. Software available at
+http://www.csie.ntu.edu.tw/~cjlin/libsvm
+
+LIBSVM implementation document is available at
+http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf
+
+For any questions and comments, please email cjlin@csie.ntu.edu.tw
+
+Acknowledgments:
+This work was supported in part by the National Science
+Council of Taiwan via the grant NSC 89-2213-E-002-013.
+The authors thank their group members and users
+for many helpful discussions and comments. They are listed in
+http://www.csie.ntu.edu.tw/~cjlin/libsvm/acknowledgements
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/README.string NeuroMiner-1-main.clara/libsvm-string-3.25/README.string
--- NeuroMiner-1-main/libsvm-string-3.25/README.string	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/README.string	2021-04-26 09:41:34.000000000 +0200
@@ -0,0 +1,41 @@
+LIBSVM data format is as follows,
+
+<label> <index1>:<value1> <index2>:<value2> <index3>:<value3>...(instance#1)
+<label> <index1>:<value1> <index2>:<value2> <index3>:<value3>...(instance#2)
+................................................................(etc.)
+
+The above values <value#> must be real numbers. Now, we extend those
+attribute values into string data type. The data format is shown
+below:
+
+<class-label> <string-value> (instance#1)
+<class-label> <string-value> (instance#2)
+<class-label> <string-value> (instance#3)
+............................ (etc.)
+
+An example is
+
+1 abcca
+1 abcac
+2 bcadc
+
+Two sample data sets are included in this package: word.data
+and bit.data.
+
+The usage is the same libsvm, but you must specify `-t 5' to use a string kernel:
+
+> svm-train -t 5 ...
+
+Currently we implement 
+
+exp(-gamma * edit(x,y)^2)
+
+where edit(x,y) is the edit distance between two strings. This
+function may not always produce a PSD kernel matrix, but practically
+it seems to work well. You can easily modify svm.cpp to implement
+other string kernels.
+
+This code is extended from standard libsvm. Our experience on string
+data is still limited, so your comments are very welcome.  Please
+email your comments/questions to cjlin@csie.ntu.edu.tw
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/bit.data NeuroMiner-1-main.clara/libsvm-string-3.25/bit.data
--- NeuroMiner-1-main/libsvm-string-3.25/bit.data	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/bit.data	2021-04-26 09:41:34.000000000 +0200
@@ -0,0 +1,200 @@
+0 0001010111121111121112112112212221222232222323333303303030303033003033003003121212121212112121211100000003003303303333333322
+0 000112112122122121211010010000003030303323333233030101010111111121121212121221222221232223232323233303303303030303003003
+0 00011010010111111112112112112121222223233332333330333303303303031211121121110110100303033303033333232222
+0 000001101111011111121112121122122122223232332333333330330330330030031212222112121112111111011010100030030303303333333333332322
+0 00000110110111112111212112212121230303030303303330333233323212122221011221221212112111111101030000122122222323323333033033033030303030303003
+0 001000001222222212121111111011001010001000300030303333332233223230001010110110111112121212222123222222323232233233233303230330300303
+0 00000010010000001010101011011111121121212121222222223223232323323323333333333303121110111110111010101001000300030330330333233232323222121122333032223222
+0 0001111111111211211122112121222212222232323333303033033030330300330030010303121212121121212112112101000003003003303032303223333332
+0 00001111012111121121121212121212122212222322333303303303030303030303030303031221212121212121121212112101000300030303030330330330333322
+0 00000100101010110110111112121212121212221222232232332332333333033033033030031221211212111112101110101000030323232330101000030333233003323232323222
+0 000100101010111012112121221212121212212122122223232333330330330330303030303030031222121212112121121211111030030030030303030300330332332232
+0 0001011111111121121211212121221221222223233303303033030330303033030303031121211212121121121100300030303303033333032322
+0 00000001001101101111211211212221212121232212232232232332333300330303300330030312212121221121211001003003000303030333332222
+0 0001001010101101111011211112122121212222232232323232323333033230333030303000031221211011121101010100103003003003333323323323222222
+0 0001011011110112111111211221212221222223223333230303303303030303303030331212121121211211110301030030330333333332
+0 0001001001011111121111211212112121222232332333323330333033303303031211211111110110030330303330303333222222
+0 00010111111111211212121212121221221222223330330330303303033003303033030311212121211212121121212110030030003030303303303303323322
+1 00111121121111211121121111122323333030333303333033033033
+1 0011121112121121211212112112121122333303030330303303033030330333
+1 000122121121121121121121111111122333333303303303303330303303
+1 011111111011111111110121223333233333333333333303
+1 0111112111111111111111232333333033333333330333
+1 00111121121121121112121112121212232330303330303303030330333033030123
+1 01111112111111211111211122333330333033333033333303
+1 01111101112111111111030000001112222222212222232222330000000333303333303332212222330003030303
+1 000111211112112111211211211211222333033303033033303033303333
+1 00112111121112112112112112121212333033030303303303330333032303
+1 000001112112111212111121121211212122323033033033033033033033033323
+1 001101111011111111011101012122323333233333333323332333
+1 00100121211101111111111111212233333333333333333333
+1 00111111111211121121121121122330333033033033303333323303
+1 00111111111111111111112233333333333333333333
+1 0012111112111121112101211101122332330332303330333303333033
+1 010111111111111121111121223333333033333303333333
+1 00110111111121121112112111122223033330333033333033333033
+1 00011121121211211212112121121112122232333003033030330330303303033303
+1 00111112111211111121111112323303333333333033303333
+1 011011111111110111121111223333033333233333333333
+1 0010121121121121111211112112112233330303333303333033033033
+1 010122111112111211211212112111223330330330330303330330333033
+2 000000010101010111111211101100101111222322332233212212212222322333330030003003000333332222232333
+2 0000101011111121111011011223223221212122122122122223233333003030030003000003033033322212221222330330030003
+2 000110111112121121211010101123232223212212122122121222333303030003000003030303030330333221221232330300031221212100303003003222
+2 00001010101111111121211210110101122323232212212212222323333030330030000101000330303333323232322212211232303030031212111003000303323222
+2 0010101011011211112111010110111223332332321212121221222233333030300300000033033333232322212212123303030003122121121000003033032322
+2 000000010111112121121210000000001112222222223223222222223233030000000300303033221223223303
+2 000010111111211121121121100101011222232221222222222223030030003000030330330333033322112212121222330303033033030003
+2 0000100101101111111211212121000011222122221222330303030303030330333332323232122333
+2 0000010010101011101121111101101011223323232121212122122223230330303003000030303333233323223222222122303003122122110003030332
+2 00001010111111112110001000101123222232223221212212122222323330303000000000330333333223212212221232303003000312110030033222
+2 0001001011101211111212112121001001000001212223222322322212221222232333030300000010030330303333333232222122121233030300031221100003032322
+2 0001111121121121121211221212100000000001212222222122222333030303030303033030330333321222121222303030030003
+2 0010100100110111111111010112232221212212122232332330330003000003303333232322212121222333303003031032
+2 000010010111011121112111010101101123232232322121222222323033030030000030333333323223212221211123230303303003121000303222
+2 000001010111121212121221212212121100000030001001012222222221222122232330333033030300303003030033322221212323303003
+3 0000000101011211221100010101011011112121122222222222222222303300000100010000300303333323222322122222122323033000303030030322222212222122330300030003
+3 000000010111112121101011111212121212212222222322323333001101000000030303033322223230303003303332211123223333
+3 000001001010111212122121010010010101121121212222222232232233230001010000000003332232322232333030300303322222232303
+3 0001011211212121210101010112112122121221222222322333300000112221000000300300330332322221222330300030303030332122122330030303
+3 00000100010010000111112110111111121122122222222223223233333010110100010000003003303323322222122222333033030100000003323222223222222333
+3 00103001001112121212110111112121212122122122222223232330303001001001000303003303323222222330030103003030303322122221221222330003003003101000322322
+3 000000001001011121011101111121212121221222122222323233000000003003303030332212221222323330003000332222122122232303030303
+3 010111121212100100001010112112212212222223232233001010000003003332223222232323000303303222212221222222303000030003000003
+3 00000100111212122122211010101011011111212222223223233303011100100003333232323232333030300030332212222212232303000003
+3 000000100010101211221210100110101101121121221222232330000103033032332323222222323030003003030322322222221222330003
+3 000100111011121110101011121121212212222223223223233000100100000030330323232223233003030322222233300303
+3 0000001112112212101010101111212121221222212222212222333000000300003000303033323222222223303000300300303222122222232300000003
+3 00000100011011121122121210110111121212122222222232322333330100101000000303032322223330030003033032321222121212233333003003
+3 00000001010011111221122210100001010112112212212221222223330100003000300333322222222222303003000300300330323223222221221232303003
+3 000000000010101121211221210000000000100101111212121222222223233300110000003033322322222222212232233003030303030322222321232323
+3 00000010111212121221210110111012112121221223223233001000303033323232232330030030030033222212221233030003
+3 0001000010111210101011101211121121212122222223333000000033030332322212232333030332232333
+3 00010001011011121121001001101101121221222212222222332303011000000300030332323222222222333003000303003232232221233303
+3 0000001111121212212111111211212221222232222323230300010000300003032232222123233303000000003032233303
+4 001011111011030103333300110111011111111011112223333332333232221232332333333233
+4 00111211211000300303033030300010121221121212121211212121211121212122333030330330303222322323033030303303
+4 001111121121121121212121211211212233333030333222322121223233030303030303030300011111212210001030330303303303
+4 001121211211110003330330330330010121121112111001212221112111112232333303303232223233033303330303
+4 0011211121211000010003303303303030111211211011212121211111212233333033322232222221222233030003030303033303
+4 0112112111210112211212112112112122330330330330332223223303033030301111211000330330330303
+4 011101111110012121111101211123333233322222232333333323303030101121111111000033330333332303
+4 000101211211210030000033033033000112121111211211211121111121223333303333332122222232330330303333
+4 000111101110100100003333333300110111011100101212121101111212232333333322222322223233233233332303
+4 001211121111111010111210111122323333223222332233330300111101010333333333033303
+4 0011112111111211211121111122333330333322223223233303303301012111100100030333303333
+4 0011121121112101003000003033033330111111111111111101212233333333332212222122322323330303333033
+4 0111111112110111211111112233333332322322222122233033033033330101111210010000033033333303
+4 0111111121121112111211112122333303303332222122232303330330330330011211121121003000003303330333
+4 001121121121112112121121211121212233330303032322222212122233300303030330303011121121000001003033303303303033
+4 0101211121121121000300033333001111111111211111122333333333221221221222333030330330303330331032
+4 0012111211211121121121121121112223033033033322122223222233033030330301112112101000003003030330330330331032
+4 001121111121111111211110121223233333033330322212222223233303330300111211100003003030333303
+4 001121211212112110000121222222121211212121212330303303033222221223330030330303001121211000030303030330303303
+4 001211121211211121211211212111212330333033222222323330333033300111211211010000030303303303303033
+4 00121121121121121003003033033030301121121211121211211211121123330333033032222333033033033033
+4 001111101111111111111121223333333321222222222323333033033303010111211100030001033332333303
+4 000112121211100000330333001111121112111111121121223233033303330322222323233033303033
+4 00111111111111001221211111122303233332221222222323033330333333010111112101000000303333333333
+4 0010121121111101121211111111222333303233221222122222232233330303030303030300303000112122121211221212121010300000300300333330333033
+5 001210122221222222222122110001001001001101112112121222221232212323233233010100001030000300332322323222222323303303030030003000000003
+5 00010122212222122221221101010101010101101111221222323230010033332323232233232332303030030000300003
+5 000011212221222222122221211000001000101011112121221222222322223233030000100000003232222223222332303303300303000300000003
+5 0000010101112122222211211010101101111211222222232223232233333000103323303303333030003003
+5 000112121221222330300303000112121212101000011010111121212122223232232333010100100003333222322232330330303033
+5 0000000010012222222221110100100101010101111112122122122223333030121100003030303323323232232322222333033033
+5 00001121000000000122122222222321233321221211110010010010111111221222232232323001001000303332322232233233330330300003
+5 00000010101212323222221222121211003000010001011101111121121222122232232333303030012121211001000300303333332323222222122122333033030303003003
+5 0000001001001011222232222212121010101110110111212212221222232233233230010010000033323323223222332333030000030303
+5 001010000100100100122223223222121223303221121212121121010000001011121212212221223300030030332222232323303303303030303033
+5 0100010112322222111211210010101010111112121222232323010000303303232323232233330330330303
+5 00111221223222111211100010101111111212221222223232300100003030333322223233330333303030000003
+5 00000112222212222122121110000000100100101010111121212122222232232232301001000000003033323232323222322122122323333333003000300003
+5 01011001000300003032300101112212221222222232211010101101011112122212222232332333010110103000303323233232323033033033
+5 01030000000011232212222222222212111010030003010000100100100110111211221212222222223222323230100010000103000030030333232232322322222212122123233332330330000003
+5 0000100011222222323300101212111010010010010010101111122122122222222322323000100000300030303232232232232232232333303303
+5 0001122122112112111000003000101011212121122122122122323301003003003030333212222222332333303303303003
+5 000011222122212222121000300000100101111111212112122122222122322223232333333001121101010000000003003033033332322322212221222233303030300300300003
+5 0000001011222222122111101000101101111212122122222212233300000300030333232222323333233303003003
+5 00011122212221221212100010101011111112122122222322233233010001000000303332323223212223330303303033000003030123
+5 00000000000112212322222121212110000000100001010110111121212122122222323223233233010101010010300003030303233232232222222223232333033033
+6 0010121121212121211121112110003303303030000001012110121212121221222222322232333333303330303030303303031211210030033322
+6 000121211112111211112110100330303030030001001111211121222122222223232333323333033303333030331221212121100000303030333222
+6 000112121112121121121121111100333033303030300001101111121212122122222232232333303303303303303303303312111211003030333322
+6 001121212121212112111100303030030000101011121112121212222232232332332333333030330303033003121211100000303033232222
+6 0012111211212112112111210030030001011212121221222232333333033033033033030330331221000332
+6 01121121212121112111111010100000303033222211223330030000101111122122122223223233233333303303303033030303
+6 0001121211111211111111010003333033030303003010001101011211121221221212222222223232323332333033333033033033121211110300030330323222
+6 000111121212212121121110100030030000011101212212221221222233232323233333330303303030300303
+6 001121211211112111121111030303030300000111212121221212223223233333303333033330303303121003321032
+6 000112221212121212112112111103003000001111212121222222223330323303330330330330303030030311030322
+6 00111112111030300300100100010101011112112121212222122222323233323333333333303333121211121111010000000303030333232322322322
+6 001211111211121121111110100033303303030303003001111121212121211212222222323333233303303333033303331221121211030303030332
+6 0010112211211211111110101003303303000001001001101111221222222222322232232323233333333330303303031211001000003232322322
+6 00111111111111111010100333232323000011010111112212223223233233333323333303323303
+6 0001121212112121211211030003010001001010101111212222222223222322323323330333030303303030303312221210100010000000332323222232
+6 01121121211212111210000100111212122122223233330333033033033030303303111010300303322222
+6 0112112112121121111030003000101111211221212222223233233303333033030333030303031032122112100030303322
+6 01012212112112112112111111033033030300001011121121212212222323232333033330303303303303030311211210030033033222
+6 000112221221212121121121101030330303030300300000000101111212121212122212221222212323323232333033303303003030300312221212121212111000030003003030303033232222
+7 000101121112112121212112212112121212122330303030303303030303303303032221222212221222330300300000030003
+7 001000100030000300011111112110111111101121123333333233333333303233212212212232223211111011111233233333233333
+7 00103010300000001011211211211211121212121121121223330330303323003033030330332222222232230300112323
+7 01000030000000111211212121212121121212121122112212230330303033030033030303303030322222122222222330003003
+7 000011011112111121121211212121122123333030303303033033033322122122233300030303
+7 00100003000000000101111211121121211211211011012232323333033303303303303332221222222222232303
+7 01000100300000000003000101212111211121121211211111122233303330333033030333033222222122222222232333
+7 011000000010111121111121101121211212233033303330333033333222212222333033
+7 0010121121121112112112112112112122303303303303303303330330332122221222123321112122330330303001103000030003
+7 00011211112111111211111111223333333033333303333032221222221112233333001003000003
+7 0000100000011111112111211212112112112232330330330330330333323222322333
+7 000000000000000010111212121221212122303030030303030323222222222222122221010112323232330003
+7 00011000010111112111212121112112211223333030330303303033303232222112122333303303
+7 00101030030011111121111211121111112122333330333033033330333222122112112122330333033033
+7 0010100000300301012121221212112121121212112111212122333033033030330303030330322232122123303033
+7 000111011111121112111211211212333033033303330333333232122212112123330330030003
+7 0000111112121121211212121212121121212233033030303303033030303032222222222333300000000003
+7 00000000000000100101111121211212112121212121221212223303030030303303030303303033332222222222222233
+7 001000000000030000110111111112111111211122330333333330332333222222222222233233
+7 0011121212121212112121212121211211212233303303303030303030303322221222122122233033030303010100030000003003
+7 0100000000000101111121211112121211211211223303303330303303033033332222222222223303
+7 00000101121211212112112112121212112121222330303030330303033033030330322122212212121212112122333303300330033000030003
+7 0000111121121121121121121121112122333033033303303303303303222122121121223333030303000003
+7 00000000100110111121212121212121212121221211223303033030303003030303030303323232212222223303
+7 00000000100010101111121211212121211212101221222333030303030303033030303333223222222221223303
+7 00100000010112112112121211212112121211122333033033033030303303030322221222121222333003030303
+7 0000000000001111211121121112111121121212233303303303330330333033221221212112223233333033
+7 001001001001001000001012112112112112112111223303330303330330332322232232232211112233333233
+8 0101121112121212111011112112122122222323330330303030323232333030303000001103033310321101003032322212121121100030333332
+8 0000001010111212212121212121111111221221222323330330303303233333033030300312111030303032221032
+8 00101011030300111212121121211101011112122122223233330330332323323333303030031212111010103033030332322211210003032322
+8 000001011121221122122121110110121121212122222232333333033033333303030300030003112100303322
+8 010111010122323232111101000001212221111112111212233233333333033333333030031211011033033322
+8 00111211212112212121221221211111222222332323030030332333033030300000012222212121111010000300303030033030333311010003233222
+8 0001011222212211211000003030030010121212212111012111212121222323323303330332233230333030303003121212101030303332
+8 00000101112223211101003000001121212121111111221222122223232333033003030332332333330303111030303222
+8 001122112121212121121011101211121212322333323033323232303010003003033030030303
+8 00001001112323221211010030003000030000030012122122212221222212101101121222232323323303033323303310110003233222
+8 000011111121211211211010101112122122232323333033232322333233303003003010030312121110010033032332321032103211003232
+8 00111111212110001010101101101112122222232322323233233033323323303030330003103212111010001001000332323232232232
+8 0000112232122221222121211101003003003030012122121221210101111121212212222332333033030303323323303303000300000312121211110000300333323232
+8 000000001011121121211212101010110112121222122222323323303030303232323232333303030003122111010101003033030333222222112121010000030323232322
+8 00000101112112112212122111111121122122222323333033030030303323333030303003121100303032221212110030303322
+8 001030000001001112122122121221110100110112112122212222232323333033030303323233223330030003101010103003000303222222222212121111000003003323232322
+8 00000101110111110110111111212122122222232323333333232323233333030300000312212101101000003003033221223323221221212110100103003033333222
+8 0000011101211112111101110121212122122222323233303303003032323323333030030311221101003303332212122110000303032322
+8 000101101221212121211111101121121212222323333303303033030333330303031121103030303222121211121100003303332332
+8 00001010111011110011010111112121222212222222322332323330330323233303330303000311003322122222110010000300332322
+8 00001010112112121212111101111212122122232233233303030300303333233033300311211010303033322211221212110100003033333232
+8 0103001101112112121212121211111112122222322223233300303000332332333303303000003003122221211011010030300303033322221221210000033232
+9 0000010112121111101111111111123233333333230322212222323000003033303003223212223003001121121223033303
+9 0000010111011111211110111111123233333322122223223323233333303030300312221211121011000030303303333222
+9 0012212212212121121110003000303033300112111211211211121111233333033033322212222323303303030303030003
+9 0010101112112121121121121121121122233033033033303232323233030303030003122111003300033222
+9 00000111030011121121212112121121121212121222333303030303030323222323330330303003
+9 00000011210112121112111121111211122323330330333332222333330303030310321212110300303322
+9 0000100111212322222211100030000111111212112112111222333033303303322222233233330330030003
+9 0000011222212212211212100000303033030101112111211111121111223230333330333222123222233333030303003003
+9 00001011223221222122121121100003000303000112212121121221121212112122233333010033003303223212323333033030030300031032
+9 00001011101121121221211221121121211221223033030303303033222233330303303030031222112121100300030030332322
+9 0000101011101111211211211211121122333033303322122222223233330330303300303003122121221121110000030030303033232322
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm-predict.c NeuroMiner-1-main.clara/libsvm-string-3.25/svm-predict.c
--- NeuroMiner-1-main/libsvm-string-3.25/svm-predict.c	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm-predict.c	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,271 @@
+#include <stdio.h>
+#include <ctype.h>
+#include <stdlib.h>
+#include <string.h>
+#include <errno.h>
+#include "svm.h"
+
+int print_null(const char *s,...) {return 0;}
+
+static int (*info)(const char *fmt,...) = &printf;
+
+#ifdef _STRING
+struct svm_data xx;
+#endif
+
+struct svm_node *x;
+int max_nr_attr = 64;
+
+struct svm_model* model;
+int predict_probability=0;
+
+static char *line = NULL;
+static int max_line_len;
+
+static char* readline(FILE *input)
+{
+	int len;
+
+	if(fgets(line,max_line_len,input) == NULL)
+		return NULL;
+
+	while(strrchr(line,'\n') == NULL)
+	{
+		max_line_len *= 2;
+		line = (char *) realloc(line,max_line_len);
+		len = (int) strlen(line);
+		if(fgets(line+len,max_line_len-len,input) == NULL)
+			break;
+	}
+	return line;
+}
+
+void exit_input_error(int line_num)
+{
+	fprintf(stderr,"Wrong input format at line %d\n", line_num);
+	exit(1);
+}
+
+void predict(FILE *input, FILE *output)
+{
+	int correct = 0;
+	int total = 0;
+	double error = 0;
+	double sump = 0, sumt = 0, sumpp = 0, sumtt = 0, sumpt = 0;
+
+#ifdef _STRING
+	int data_type=svm_get_data_type(model);
+#endif
+	int svm_type=svm_get_svm_type(model);
+	int nr_class=svm_get_nr_class(model);
+	double *prob_estimates=NULL;
+	int j;
+
+	if(predict_probability)
+	{
+		if (svm_type==NU_SVR || svm_type==EPSILON_SVR)
+			info("Prob. model for test data: target value = predicted value + z,\nz: Laplace distribution e^(-|z|/sigma)/(2sigma),sigma=%g\n",svm_get_svr_probability(model));
+		else
+		{
+			int *labels=(int *) malloc(nr_class*sizeof(int));
+			svm_get_labels(model,labels);
+			prob_estimates = (double *) malloc(nr_class*sizeof(double));
+			fprintf(output,"labels");
+			for(j=0;j<nr_class;j++)
+				fprintf(output," %d",labels[j]);
+			fprintf(output,"\n");
+			free(labels);
+		}
+	}
+
+	max_line_len = 1024;
+	line = (char *)malloc(max_line_len*sizeof(char));
+	while(readline(input) != NULL)
+	{
+		int i = 0;
+		double target_label, predict_label;
+		char *idx, *val, *label, *endptr;
+		int inst_max_index = -1; // strtol gives 0 if wrong format, and precomputed kernel has <index> start from 0
+
+		label = strtok(line," \t\n");
+		if(label == NULL) // empty line
+			exit_input_error(total+1);
+
+		target_label = strtod(label,&endptr);
+		if(endptr == label || *endptr != '\0')
+			exit_input_error(total+1);
+
+#ifdef _STRING
+		if (data_type==STRING)
+		{
+			xx.s = strtok(NULL,"\n");
+			xx.v = NULL;
+		}
+		else
+		{
+#endif
+
+		while(1)
+		{
+			if(i>=max_nr_attr-1)	// need one more for index = -1
+			{
+				max_nr_attr *= 2;
+				x = (struct svm_node *) realloc(x,max_nr_attr*sizeof(struct svm_node));
+			}
+
+			idx = strtok(NULL,":");
+			val = strtok(NULL," \t");
+
+			if(val == NULL)
+				break;
+			errno = 0;
+			x[i].index = (int) strtol(idx,&endptr,10);
+			if(endptr == idx || errno != 0 || *endptr != '\0' || x[i].index <= inst_max_index)
+				exit_input_error(total+1);
+			else
+				inst_max_index = x[i].index;
+
+			errno = 0;
+			x[i].value = strtod(val,&endptr);
+			if(endptr == val || errno != 0 || (*endptr != '\0' && !isspace(*endptr)))
+				exit_input_error(total+1);
+
+			++i;
+		}
+		x[i].index = -1;
+
+#ifdef _STRING
+		xx.v = x;
+		xx.s = NULL;
+		// end of else
+		}
+#endif
+
+		if (predict_probability && (svm_type==C_SVC || svm_type==NU_SVC))
+		{
+#ifdef _STRING
+			predict_label = svm_predict_probability(model,xx,prob_estimates);
+#else
+			predict_label = svm_predict_probability(model,x,prob_estimates);
+#endif
+			fprintf(output,"%g",predict_label);
+			for(j=0;j<nr_class;j++)
+				fprintf(output," %g",prob_estimates[j]);
+			fprintf(output,"\n");
+		}
+		else
+		{
+#ifdef _STRING
+			predict_label = svm_predict(model,xx);
+#else
+			predict_label = svm_predict(model,x);
+#endif
+			fprintf(output,"%.17g\n",predict_label);
+		}
+
+		if(predict_label == target_label)
+			++correct;
+		error += (predict_label-target_label)*(predict_label-target_label);
+		sump += predict_label;
+		sumt += target_label;
+		sumpp += predict_label*predict_label;
+		sumtt += target_label*target_label;
+		sumpt += predict_label*target_label;
+		++total;
+	}
+	if (svm_type==NU_SVR || svm_type==EPSILON_SVR)
+	{
+		info("Mean squared error = %g (regression)\n",error/total);
+		info("Squared correlation coefficient = %g (regression)\n",
+			((total*sumpt-sump*sumt)*(total*sumpt-sump*sumt))/
+			((total*sumpp-sump*sump)*(total*sumtt-sumt*sumt))
+			);
+	}
+	else
+		info("Accuracy = %g%% (%d/%d) (classification)\n",
+			(double)correct/total*100,correct,total);
+	if(predict_probability)
+		free(prob_estimates);
+}
+
+void exit_with_help()
+{
+	printf(
+	"Usage: svm-predict [options] test_file model_file output_file\n"
+	"options:\n"
+	"-b probability_estimates: whether to predict probability estimates, 0 or 1 (default 0); for one-class SVM only 0 is supported\n"
+	"-q : quiet mode (no outputs)\n"
+	);
+	exit(1);
+}
+
+int main(int argc, char **argv)
+{
+	FILE *input, *output;
+	int i;
+	// parse options
+	for(i=1;i<argc;i++)
+	{
+		if(argv[i][0] != '-') break;
+		++i;
+		switch(argv[i-1][1])
+		{
+			case 'b':
+				predict_probability = atoi(argv[i]);
+				break;
+			case 'q':
+				info = &print_null;
+				i--;
+				break;
+			default:
+				fprintf(stderr,"Unknown option: -%c\n", argv[i-1][1]);
+				exit_with_help();
+		}
+	}
+
+	if(i>=argc-2)
+		exit_with_help();
+
+	input = fopen(argv[i],"r");
+	if(input == NULL)
+	{
+		fprintf(stderr,"can't open input file %s\n",argv[i]);
+		exit(1);
+	}
+
+	output = fopen(argv[i+2],"w");
+	if(output == NULL)
+	{
+		fprintf(stderr,"can't open output file %s\n",argv[i+2]);
+		exit(1);
+	}
+
+	if((model=svm_load_model(argv[i+1]))==0)
+	{
+		fprintf(stderr,"can't open model file %s\n",argv[i+1]);
+		exit(1);
+	}
+
+	x = (struct svm_node *) malloc(max_nr_attr*sizeof(struct svm_node));
+	if(predict_probability)
+	{
+		if(svm_check_probability_model(model)==0)
+		{
+			fprintf(stderr,"Model does not support probabiliy estimates\n");
+			exit(1);
+		}
+	}
+	else
+	{
+		if(svm_check_probability_model(model)!=0)
+			info("Model supports probability estimates, but disabled in prediction.\n");
+	}
+
+	predict(input,output);
+	svm_free_and_destroy_model(&model);
+	free(x);
+	free(line);
+	fclose(input);
+	fclose(output);
+	return 0;
+}
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm-scale.c NeuroMiner-1-main.clara/libsvm-string-3.25/svm-scale.c
--- NeuroMiner-1-main/libsvm-string-3.25/svm-scale.c	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm-scale.c	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,405 @@
+#include <float.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <ctype.h>
+#include <string.h>
+
+void exit_with_help()
+{
+	printf(
+	"Usage: svm-scale [options] data_filename\n"
+	"options:\n"
+	"-l lower : x scaling lower limit (default -1)\n"
+	"-u upper : x scaling upper limit (default +1)\n"
+	"-y y_lower y_upper : y scaling limits (default: no y scaling)\n"
+	"-s save_filename : save scaling parameters to save_filename\n"
+	"-r restore_filename : restore scaling parameters from restore_filename\n"
+	);
+	exit(1);
+}
+
+char *line = NULL;
+int max_line_len = 1024;
+double lower=-1.0,upper=1.0,y_lower,y_upper;
+int y_scaling = 0;
+double *feature_max;
+double *feature_min;
+double y_max = -DBL_MAX;
+double y_min = DBL_MAX;
+int max_index;
+int min_index;
+long int num_nonzeros = 0;
+long int new_num_nonzeros = 0;
+
+#define max(x,y) (((x)>(y))?(x):(y))
+#define min(x,y) (((x)<(y))?(x):(y))
+
+void output_target(double value);
+void output(int index, double value);
+char* readline(FILE *input);
+int clean_up(FILE *fp_restore, FILE *fp, const char *msg);
+
+int main(int argc,char **argv)
+{
+	int i,index;
+	FILE *fp, *fp_restore = NULL;
+	char *save_filename = NULL;
+	char *restore_filename = NULL;
+
+	for(i=1;i<argc;i++)
+	{
+		if(argv[i][0] != '-') break;
+		++i;
+		switch(argv[i-1][1])
+		{
+			case 'l': lower = atof(argv[i]); break;
+			case 'u': upper = atof(argv[i]); break;
+			case 'y':
+				y_lower = atof(argv[i]);
+				++i;
+				y_upper = atof(argv[i]);
+				y_scaling = 1;
+				break;
+			case 's': save_filename = argv[i]; break;
+			case 'r': restore_filename = argv[i]; break;
+			default:
+				fprintf(stderr,"unknown option\n");
+				exit_with_help();
+		}
+	}
+
+	if(!(upper > lower) || (y_scaling && !(y_upper > y_lower)))
+	{
+		fprintf(stderr,"inconsistent lower/upper specification\n");
+		exit(1);
+	}
+
+	if(restore_filename && save_filename)
+	{
+		fprintf(stderr,"cannot use -r and -s simultaneously\n");
+		exit(1);
+	}
+
+	if(argc != i+1)
+		exit_with_help();
+
+	fp=fopen(argv[i],"r");
+
+	if(fp==NULL)
+	{
+		fprintf(stderr,"can't open file %s\n", argv[i]);
+		exit(1);
+	}
+
+	line = (char *) malloc(max_line_len*sizeof(char));
+
+#define SKIP_TARGET\
+	while(isspace(*p)) ++p;\
+	while(!isspace(*p)) ++p;
+
+#define SKIP_ELEMENT\
+	while(*p!=':') ++p;\
+	++p;\
+	while(isspace(*p)) ++p;\
+	while(*p && !isspace(*p)) ++p;
+
+	/* assumption: min index of attributes is 1 */
+	/* pass 1: find out max index of attributes */
+	max_index = 0;
+	min_index = 1;
+
+	if(restore_filename)
+	{
+		int idx, c;
+
+		fp_restore = fopen(restore_filename,"r");
+		if(fp_restore==NULL)
+		{
+			fprintf(stderr,"can't open file %s\n", restore_filename);
+			exit(1);
+		}
+
+		c = fgetc(fp_restore);
+		if(c == 'y')
+		{
+			readline(fp_restore);
+			readline(fp_restore);
+			readline(fp_restore);
+		}
+		readline(fp_restore);
+		readline(fp_restore);
+
+		while(fscanf(fp_restore,"%d %*f %*f\n",&idx) == 1)
+			max_index = max(idx,max_index);
+		rewind(fp_restore);
+	}
+
+	while(readline(fp)!=NULL)
+	{
+		char *p=line;
+
+		SKIP_TARGET
+
+		while(sscanf(p,"%d:%*f",&index)==1)
+		{
+			max_index = max(max_index, index);
+			min_index = min(min_index, index);
+			SKIP_ELEMENT
+			num_nonzeros++;
+		}
+	}
+
+	if(min_index < 1)
+		fprintf(stderr,
+			"WARNING: minimal feature index is %d, but indices should start from 1\n", min_index);
+
+	rewind(fp);
+
+	feature_max = (double *)malloc((max_index+1)* sizeof(double));
+	feature_min = (double *)malloc((max_index+1)* sizeof(double));
+
+	if(feature_max == NULL || feature_min == NULL)
+	{
+		fprintf(stderr,"can't allocate enough memory\n");
+		exit(1);
+	}
+
+	for(i=0;i<=max_index;i++)
+	{
+		feature_max[i]=-DBL_MAX;
+		feature_min[i]=DBL_MAX;
+	}
+
+	/* pass 2: find out min/max value */
+	while(readline(fp)!=NULL)
+	{
+		char *p=line;
+		int next_index=1;
+		double target;
+		double value;
+
+		if (sscanf(p,"%lf",&target) != 1)
+			return clean_up(fp_restore, fp, "ERROR: failed to read labels\n");
+		y_max = max(y_max,target);
+		y_min = min(y_min,target);
+
+		SKIP_TARGET
+
+		while(sscanf(p,"%d:%lf",&index,&value)==2)
+		{
+			for(i=next_index;i<index;i++)
+			{
+				feature_max[i]=max(feature_max[i],0);
+				feature_min[i]=min(feature_min[i],0);
+			}
+
+			feature_max[index]=max(feature_max[index],value);
+			feature_min[index]=min(feature_min[index],value);
+
+			SKIP_ELEMENT
+			next_index=index+1;
+		}
+
+		for(i=next_index;i<=max_index;i++)
+		{
+			feature_max[i]=max(feature_max[i],0);
+			feature_min[i]=min(feature_min[i],0);
+		}
+	}
+
+	rewind(fp);
+
+	/* pass 2.5: save/restore feature_min/feature_max */
+
+	if(restore_filename)
+	{
+		/* fp_restore rewinded in finding max_index */
+		int idx, c;
+		double fmin, fmax;
+		int next_index = 1;
+
+		if((c = fgetc(fp_restore)) == 'y')
+		{
+			if(fscanf(fp_restore, "%lf %lf\n", &y_lower, &y_upper) != 2 ||
+			   fscanf(fp_restore, "%lf %lf\n", &y_min, &y_max) != 2)
+				return clean_up(fp_restore, fp, "ERROR: failed to read scaling parameters\n");
+			y_scaling = 1;
+		}
+		else
+			ungetc(c, fp_restore);
+
+		if (fgetc(fp_restore) == 'x')
+		{
+			if(fscanf(fp_restore, "%lf %lf\n", &lower, &upper) != 2)
+				return clean_up(fp_restore, fp, "ERROR: failed to read scaling parameters\n");
+			while(fscanf(fp_restore,"%d %lf %lf\n",&idx,&fmin,&fmax)==3)
+			{
+				for(i = next_index;i<idx;i++)
+					if(feature_min[i] != feature_max[i])
+					{
+						fprintf(stderr,
+							"WARNING: feature index %d appeared in file %s was not seen in the scaling factor file %s. The feature is scaled to 0.\n",
+							i, argv[argc-1], restore_filename);
+						feature_min[i] = 0;
+						feature_max[i] = 0;
+					}
+
+				feature_min[idx] = fmin;
+				feature_max[idx] = fmax;
+
+				next_index = idx + 1;
+			}
+
+			for(i=next_index;i<=max_index;i++)
+				if(feature_min[i] != feature_max[i])
+				{
+					fprintf(stderr,
+						"WARNING: feature index %d appeared in file %s was not seen in the scaling factor file %s. The feature is scaled to 0.\n",
+						i, argv[argc-1], restore_filename);
+					feature_min[i] = 0;
+					feature_max[i] = 0;
+				}
+		}
+		fclose(fp_restore);
+	}
+
+	if(save_filename)
+	{
+		FILE *fp_save = fopen(save_filename,"w");
+		if(fp_save==NULL)
+		{
+			fprintf(stderr,"can't open file %s\n", save_filename);
+			exit(1);
+		}
+		if(y_scaling)
+		{
+			fprintf(fp_save, "y\n");
+			fprintf(fp_save, "%.17g %.17g\n", y_lower, y_upper);
+			fprintf(fp_save, "%.17g %.17g\n", y_min, y_max);
+		}
+		fprintf(fp_save, "x\n");
+		fprintf(fp_save, "%.17g %.17g\n", lower, upper);
+		for(i=1;i<=max_index;i++)
+		{
+			if(feature_min[i]!=feature_max[i])
+				fprintf(fp_save,"%d %.17g %.17g\n",i,feature_min[i],feature_max[i]);
+		}
+
+		if(min_index < 1)
+			fprintf(stderr,
+				"WARNING: scaling factors with indices smaller than 1 are not stored to the file %s.\n", save_filename);
+
+		fclose(fp_save);
+	}
+
+	/* pass 3: scale */
+	while(readline(fp)!=NULL)
+	{
+		char *p=line;
+		int next_index=1;
+		double target;
+		double value;
+
+		if (sscanf(p,"%lf",&target) != 1)
+			return clean_up(NULL, fp, "ERROR: failed to read labels\n");
+		output_target(target);
+
+		SKIP_TARGET
+
+		while(sscanf(p,"%d:%lf",&index,&value)==2)
+		{
+			for(i=next_index;i<index;i++)
+				output(i,0);
+
+			output(index,value);
+
+			SKIP_ELEMENT
+			next_index=index+1;
+		}
+
+		for(i=next_index;i<=max_index;i++)
+			output(i,0);
+
+		printf("\n");
+	}
+
+	if (new_num_nonzeros > num_nonzeros)
+		fprintf(stderr,
+			"WARNING: original #nonzeros %ld\n"
+			"       > new      #nonzeros %ld\n"
+			"If feature values are non-negative and sparse, use -l 0 rather than the default -l -1\n",
+			num_nonzeros, new_num_nonzeros);
+
+	free(line);
+	free(feature_max);
+	free(feature_min);
+	fclose(fp);
+	return 0;
+}
+
+char* readline(FILE *input)
+{
+	int len;
+
+	if(fgets(line,max_line_len,input) == NULL)
+		return NULL;
+
+	while(strrchr(line,'\n') == NULL)
+	{
+		max_line_len *= 2;
+		line = (char *) realloc(line, max_line_len);
+		len = (int) strlen(line);
+		if(fgets(line+len,max_line_len-len,input) == NULL)
+			break;
+	}
+	return line;
+}
+
+void output_target(double value)
+{
+	if(y_scaling)
+	{
+		if(value == y_min)
+			value = y_lower;
+		else if(value == y_max)
+			value = y_upper;
+		else value = y_lower + (y_upper-y_lower) *
+			     (value - y_min)/(y_max-y_min);
+	}
+	printf("%.17g ",value);
+}
+
+void output(int index, double value)
+{
+	/* skip single-valued attribute */
+	if(feature_max[index] == feature_min[index])
+		return;
+
+	if(value == feature_min[index])
+		value = lower;
+	else if(value == feature_max[index])
+		value = upper;
+	else
+		value = lower + (upper-lower) *
+			(value-feature_min[index])/
+			(feature_max[index]-feature_min[index]);
+
+	if(value != 0)
+	{
+		printf("%d:%g ",index, value);
+		new_num_nonzeros++;
+	}
+}
+
+int clean_up(FILE *fp_restore, FILE *fp, const char* msg)
+{
+	fprintf(stderr,	"%s", msg);
+	free(line);
+	free(feature_max);
+	free(feature_min);
+	fclose(fp);
+	if (fp_restore)
+		fclose(fp_restore);
+	return -1;
+}
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm-train.c NeuroMiner-1-main.clara/libsvm-string-3.25/svm-train.c
--- NeuroMiner-1-main/libsvm-string-3.25/svm-train.c	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm-train.c	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,441 @@
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <ctype.h>
+#include <errno.h>
+#include "svm.h"
+#define Malloc(type,n) (type *)malloc((n)*sizeof(type))
+
+void print_null(const char *s) {}
+
+void exit_with_help()
+{
+	printf(
+	"Usage: svm-train [options] training_set_file [model_file]\n"
+	"options:\n"
+	"-s svm_type : set type of SVM (default 0)\n"
+	"	0 -- C-SVC		(multi-class classification)\n"
+	"	1 -- nu-SVC		(multi-class classification)\n"
+	"	2 -- one-class SVM\n"
+	"	3 -- epsilon-SVR	(regression)\n"
+	"	4 -- nu-SVR		(regression)\n"
+	"-t kernel_type : set type of kernel function (default 2)\n"
+	"	0 -- linear: u'*v\n"
+	"	1 -- polynomial: (gamma*u'*v + coef0)^degree\n"
+	"	2 -- radial basis function: exp(-gamma*|u-v|^2)\n"
+	"	3 -- sigmoid: tanh(gamma*u'*v + coef0)\n"
+	"	4 -- precomputed kernel (kernel values in training_set_file)\n"
+#ifdef _STRING
+	"	5 -- edit kernel: edit distance for strings\n"
+#endif
+	"-d degree : set degree in kernel function (default 3)\n"
+	"-g gamma : set gamma in kernel function (default 1/num_features)\n"
+	"-r coef0 : set coef0 in kernel function (default 0)\n"
+	"-c cost : set the parameter C of C-SVC, epsilon-SVR, and nu-SVR (default 1)\n"
+	"-n nu : set the parameter nu of nu-SVC, one-class SVM, and nu-SVR (default 0.5)\n"
+	"-p epsilon : set the epsilon in loss function of epsilon-SVR (default 0.1)\n"
+	"-m cachesize : set cache memory size in MB (default 100)\n"
+	"-e epsilon : set tolerance of termination criterion (default 0.001)\n"
+	"-h shrinking : whether to use the shrinking heuristics, 0 or 1 (default 1)\n"
+	"-b probability_estimates : whether to train a SVC or SVR model for probability estimates, 0 or 1 (default 0)\n"
+	"-wi weight : set the parameter C of class i to weight*C, for C-SVC (default 1)\n"
+	"-v n: n-fold cross validation mode\n"
+	"-q : quiet mode (no outputs)\n"
+	);
+	exit(1);
+}
+
+void exit_input_error(int line_num)
+{
+	fprintf(stderr,"Wrong input format at line %d\n", line_num);
+	exit(1);
+}
+
+void parse_command_line(int argc, char **argv, char *input_file_name, char *model_file_name);
+void read_problem(const char *filename);
+void do_cross_validation();
+
+struct svm_parameter param;		// set by parse_command_line
+struct svm_problem prob;		// set by read_problem
+struct svm_model *model;
+struct svm_node *x_space;
+int cross_validation;
+int nr_fold;
+
+static char *line = NULL;
+static int max_line_len;
+
+static char* readline(FILE *input)
+{
+	int len;
+
+	if(fgets(line,max_line_len,input) == NULL)
+		return NULL;
+
+	while(strrchr(line,'\n') == NULL)
+	{
+		max_line_len *= 2;
+		line = (char *) realloc(line,max_line_len);
+		len = (int) strlen(line);
+		if(fgets(line+len,max_line_len-len,input) == NULL)
+			break;
+	}
+	return line;
+}
+
+int main(int argc, char **argv)
+{
+	char input_file_name[1024];
+	char model_file_name[1024];
+	const char *error_msg;
+
+	parse_command_line(argc, argv, input_file_name, model_file_name);
+	read_problem(input_file_name);
+	error_msg = svm_check_parameter(&prob,&param);
+
+	if(error_msg)
+	{
+		fprintf(stderr,"ERROR: %s\n",error_msg);
+		exit(1);
+	}
+
+	if(cross_validation)
+	{
+		do_cross_validation();
+	}
+	else
+	{
+		model = svm_train(&prob,&param);
+		if(svm_save_model(model_file_name,model))
+		{
+			fprintf(stderr, "can't save model to file %s\n", model_file_name);
+			exit(1);
+		}
+		svm_free_and_destroy_model(&model);
+	}
+
+#ifdef _STRING
+	int i;
+	for (i=0; i<prob.l; i++)
+		free(prob.x[i].s);
+#endif
+	svm_destroy_param(&param);
+	free(prob.y);
+	free(prob.x);
+	free(x_space);
+	free(line);
+
+	return 0;
+}
+
+void do_cross_validation()
+{
+	int i;
+	int total_correct = 0;
+	double total_error = 0;
+	double sumv = 0, sumy = 0, sumvv = 0, sumyy = 0, sumvy = 0;
+	double *target = Malloc(double,prob.l);
+
+	svm_cross_validation(&prob,&param,nr_fold,target);
+	if(param.svm_type == EPSILON_SVR ||
+	   param.svm_type == NU_SVR)
+	{
+		for(i=0;i<prob.l;i++)
+		{
+			double y = prob.y[i];
+			double v = target[i];
+			total_error += (v-y)*(v-y);
+			sumv += v;
+			sumy += y;
+			sumvv += v*v;
+			sumyy += y*y;
+			sumvy += v*y;
+		}
+		printf("Cross Validation Mean squared error = %g\n",total_error/prob.l);
+		printf("Cross Validation Squared correlation coefficient = %g\n",
+			((prob.l*sumvy-sumv*sumy)*(prob.l*sumvy-sumv*sumy))/
+			((prob.l*sumvv-sumv*sumv)*(prob.l*sumyy-sumy*sumy))
+			);
+	}
+	else
+	{
+		for(i=0;i<prob.l;i++)
+			if(target[i] == prob.y[i])
+				++total_correct;
+		printf("Cross Validation Accuracy = %g%%\n",100.0*total_correct/prob.l);
+	}
+	free(target);
+}
+
+void parse_command_line(int argc, char **argv, char *input_file_name, char *model_file_name)
+{
+	int i;
+	void (*print_func)(const char*) = NULL;	// default printing to stdout
+
+	// default values
+	param.svm_type = C_SVC;
+	param.kernel_type = RBF;
+	param.degree = 3;
+	param.gamma = 0;	// 1/num_features
+	param.coef0 = 0;
+	param.nu = 0.5;
+	param.cache_size = 100;
+	param.C = 1;
+	param.eps = 1e-3;
+	param.p = 0.1;
+	param.shrinking = 1;
+	param.probability = 0;
+	param.nr_weight = 0;
+	param.weight_label = NULL;
+	param.weight = NULL;
+	cross_validation = 0;
+
+	// parse options
+	for(i=1;i<argc;i++)
+	{
+		if(argv[i][0] != '-') break;
+		if(++i>=argc)
+			exit_with_help();
+		switch(argv[i-1][1])
+		{
+			case 's':
+				param.svm_type = atoi(argv[i]);
+				break;
+			case 't':
+				param.kernel_type = atoi(argv[i]);
+				break;
+			case 'd':
+				param.degree = atoi(argv[i]);
+				break;
+			case 'g':
+				param.gamma = atof(argv[i]);
+				break;
+			case 'r':
+				param.coef0 = atof(argv[i]);
+				break;
+			case 'n':
+				param.nu = atof(argv[i]);
+				break;
+			case 'm':
+				param.cache_size = atof(argv[i]);
+				break;
+			case 'c':
+				param.C = atof(argv[i]);
+				break;
+			case 'e':
+				param.eps = atof(argv[i]);
+				break;
+			case 'p':
+				param.p = atof(argv[i]);
+				break;
+			case 'h':
+				param.shrinking = atoi(argv[i]);
+				break;
+			case 'b':
+				param.probability = atoi(argv[i]);
+				break;
+			case 'q':
+				print_func = &print_null;
+				i--;
+				break;
+			case 'v':
+				cross_validation = 1;
+				nr_fold = atoi(argv[i]);
+				if(nr_fold < 2)
+				{
+					fprintf(stderr,"n-fold cross validation: n must >= 2\n");
+					exit_with_help();
+				}
+				break;
+			case 'w':
+				++param.nr_weight;
+				param.weight_label = (int *)realloc(param.weight_label,sizeof(int)*param.nr_weight);
+				param.weight = (double *)realloc(param.weight,sizeof(double)*param.nr_weight);
+				param.weight_label[param.nr_weight-1] = atoi(&argv[i-1][2]);
+				param.weight[param.nr_weight-1] = atof(argv[i]);
+				break;
+			default:
+				fprintf(stderr,"Unknown option: -%c\n", argv[i-1][1]);
+				exit_with_help();
+		}
+	}
+
+	svm_set_print_string_function(print_func);
+
+	// determine filenames
+
+	if(i>=argc)
+		exit_with_help();
+
+	strcpy(input_file_name, argv[i]);
+
+	if(i<argc-1)
+		strcpy(model_file_name,argv[i+1]);
+	else
+	{
+		char *p = strrchr(argv[i],'/');
+		if(p==NULL)
+			p = argv[i];
+		else
+			++p;
+		sprintf(model_file_name,"%s.model",p);
+	}
+}
+
+// read in a problem (in svmlight format)
+
+void read_problem(const char *filename)
+{
+	int max_index, inst_max_index, i;
+	size_t elements, j;
+	FILE *fp = fopen(filename,"r");
+	char *endptr;
+	char *idx, *val, *label;
+
+	if(fp == NULL)
+	{
+		fprintf(stderr,"can't open input file %s\n",filename);
+		exit(1);
+	}
+
+	prob.l = 0;
+	elements = 0;
+
+	max_line_len = 1024;
+	line = Malloc(char,max_line_len);
+	while(readline(fp)!=NULL)
+	{
+		char *p = strtok(line," \t"); // label
+
+		// features
+		while(1)
+		{
+			p = strtok(NULL," \t");
+			if(p == NULL || *p == '\n') // check '\n' as ' ' may be after the last feature
+				break;
+			++elements;
+		}
+		++elements;
+		++prob.l;
+	}
+	rewind(fp);
+
+#ifdef _STRING
+	prob.y = Malloc(double,prob.l);
+	prob.x = Malloc(struct svm_data,prob.l);
+#else
+	prob.y = Malloc(double,prob.l);
+	prob.x = Malloc(struct svm_node *,prob.l);
+#endif
+
+#ifdef _STRING
+	if(param.kernel_type == EDIT)
+	//if(elements == prob.l)
+	{
+		param.data_type = STRING;
+		for (i=0;i<prob.l;i++)
+		{
+			readline(fp);
+			label = strtok(line," \t");
+			prob.y[i] = strtod(label,&endptr);
+			if(endptr == label)
+				exit_input_error(i+1);
+
+			val = strtok(NULL,"\n");
+			prob.x[i].s = Malloc(char, strlen(val)+1);
+			strcpy(prob.x[i].s, val);
+			prob.x[i].v = NULL;
+		}
+
+		if(param.gamma == 0)
+			param.gamma = 0.1;
+	}
+	else
+	{
+	 	param.data_type = VECTOR;
+#endif
+
+	x_space = Malloc(struct svm_node,elements);
+	max_index = 0;
+	j=0;
+	for(i=0;i<prob.l;i++)
+	{
+		inst_max_index = -1; // strtol gives 0 if wrong format, and precomputed kernel has <index> start from 0
+		readline(fp);
+
+#ifdef _STRING
+		prob.x[i].v = &x_space[j];
+		prob.x[i].s = NULL;
+#else
+		prob.x[i] = &x_space[j];
+#endif
+
+		label = strtok(line," \t\n");
+		if(label == NULL) // empty line
+			exit_input_error(i+1);
+
+		prob.y[i] = strtod(label,&endptr);
+		if(endptr == label || *endptr != '\0')
+			exit_input_error(i+1);
+
+		while(1)
+		{
+			idx = strtok(NULL,":");
+			val = strtok(NULL," \t");
+
+			if(val == NULL)
+				break;
+
+			errno = 0;
+			x_space[j].index = (int) strtol(idx,&endptr,10);
+			if(endptr == idx || errno != 0 || *endptr != '\0' || x_space[j].index <= inst_max_index)
+				exit_input_error(i+1);
+			else
+				inst_max_index = x_space[j].index;
+
+			errno = 0;
+			x_space[j].value = strtod(val,&endptr);
+			if(endptr == val || errno != 0 || (*endptr != '\0' && !isspace(*endptr)))
+				exit_input_error(i+1);
+
+			++j;
+		}
+
+		if(inst_max_index > max_index)
+			max_index = inst_max_index;
+		x_space[j++].index = -1;
+	}
+
+	if(param.gamma == 0 && max_index > 0)
+		param.gamma = 1.0/max_index;
+
+	if(param.kernel_type == PRECOMPUTED)
+		for(i=0;i<prob.l;i++)
+		{
+#ifdef _STRING
+			if (prob.x[i].v[0].index != 0)
+#else
+			if (prob.x[i][0].index != 0)
+#endif
+			{
+				fprintf(stderr,"Wrong input format: first column must be 0:sample_serial_number\n");
+				exit(1);
+			}
+#ifdef _STRING
+			if ((int)prob.x[i].v[0].value <= 0 || (int)prob.x[i].v[0].value > max_index)
+#else
+			if ((int)prob.x[i][0].value <= 0 || (int)prob.x[i][0].value > max_index)
+#endif
+			{
+				fprintf(stderr,"Wrong input format: sample_serial_number out of range\n");
+				exit(1);
+			}
+		}
+
+#ifdef _STRING
+	// end of else
+	}
+#endif
+
+	fclose(fp);
+}
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm.cpp NeuroMiner-1-main.clara/libsvm-string-3.25/svm.cpp
--- NeuroMiner-1-main/libsvm-string-3.25/svm.cpp	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm.cpp	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,3503 @@
+#include <math.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <ctype.h>
+#include <float.h>
+#include <string.h>
+#include <stdarg.h>
+#include <limits.h>
+#include <locale.h>
+#include "svm.h"
+int libsvm_version = LIBSVM_VERSION;
+typedef float Qfloat;
+typedef signed char schar;
+#ifndef min
+template <class T> static inline T min(T x,T y) { return (x<y)?x:y; }
+#endif
+#ifndef max
+template <class T> static inline T max(T x,T y) { return (x>y)?x:y; }
+#endif
+template <class T> static inline void swap(T& x, T& y) { T t=x; x=y; y=t; }
+template <class S, class T> static inline void clone(T*& dst, S* src, int n)
+{
+	dst = new T[n];
+	memcpy((void *)dst,(void *)src,sizeof(T)*n);
+}
+static inline double powi(double base, int times)
+{
+	double tmp = base, ret = 1.0;
+
+	for(int t=times; t>0; t/=2)
+	{
+		if(t%2==1) ret*=tmp;
+		tmp = tmp * tmp;
+	}
+	return ret;
+}
+#define INF HUGE_VAL
+#define TAU 1e-12
+#define Malloc(type,n) (type *)malloc((n)*sizeof(type))
+
+static void print_string_stdout(const char *s)
+{
+	fputs(s,stdout);
+	fflush(stdout);
+}
+static void (*svm_print_string) (const char *) = &print_string_stdout;
+#if 1
+static void info(const char *fmt,...)
+{
+	char buf[BUFSIZ];
+	va_list ap;
+	va_start(ap,fmt);
+	vsprintf(buf,fmt,ap);
+	va_end(ap);
+	(*svm_print_string)(buf);
+}
+#else
+static void info(const char *fmt,...) {}
+#endif
+
+//
+// Kernel Cache
+//
+// l is the number of total data items
+// size is the cache size limit in bytes
+//
+class Cache
+{
+public:
+	Cache(int l,long int size);
+	~Cache();
+
+	// request data [0,len)
+	// return some position p where [p,len) need to be filled
+	// (p >= len if nothing needs to be filled)
+	int get_data(const int index, Qfloat **data, int len);
+	void swap_index(int i, int j);
+private:
+	int l;
+	long int size;
+	struct head_t
+	{
+		head_t *prev, *next;	// a circular list
+		Qfloat *data;
+		int len;		// data[0,len) is cached in this entry
+	};
+
+	head_t *head;
+	head_t lru_head;
+	void lru_delete(head_t *h);
+	void lru_insert(head_t *h);
+};
+
+Cache::Cache(int l_,long int size_):l(l_),size(size_)
+{
+	head = (head_t *)calloc(l,sizeof(head_t));	// initialized to 0
+	size /= sizeof(Qfloat);
+	size -= l * sizeof(head_t) / sizeof(Qfloat);
+	size = max(size, 2 * (long int) l);	// cache must be large enough for two columns
+	lru_head.next = lru_head.prev = &lru_head;
+}
+
+Cache::~Cache()
+{
+	for(head_t *h = lru_head.next; h != &lru_head; h=h->next)
+		free(h->data);
+	free(head);
+}
+
+void Cache::lru_delete(head_t *h)
+{
+	// delete from current location
+	h->prev->next = h->next;
+	h->next->prev = h->prev;
+}
+
+void Cache::lru_insert(head_t *h)
+{
+	// insert to last position
+	h->next = &lru_head;
+	h->prev = lru_head.prev;
+	h->prev->next = h;
+	h->next->prev = h;
+}
+
+int Cache::get_data(const int index, Qfloat **data, int len)
+{
+	head_t *h = &head[index];
+	if(h->len) lru_delete(h);
+	int more = len - h->len;
+
+	if(more > 0)
+	{
+		// free old space
+		while(size < more)
+		{
+			head_t *old = lru_head.next;
+			lru_delete(old);
+			free(old->data);
+			size += old->len;
+			old->data = 0;
+			old->len = 0;
+		}
+
+		// allocate new space
+		h->data = (Qfloat *)realloc(h->data,sizeof(Qfloat)*len);
+		size -= more;
+		swap(h->len,len);
+	}
+
+	lru_insert(h);
+	*data = h->data;
+	return len;
+}
+
+void Cache::swap_index(int i, int j)
+{
+	if(i==j) return;
+
+	if(head[i].len) lru_delete(&head[i]);
+	if(head[j].len) lru_delete(&head[j]);
+	swap(head[i].data,head[j].data);
+	swap(head[i].len,head[j].len);
+	if(head[i].len) lru_insert(&head[i]);
+	if(head[j].len) lru_insert(&head[j]);
+
+	if(i>j) swap(i,j);
+	for(head_t *h = lru_head.next; h!=&lru_head; h=h->next)
+	{
+		if(h->len > i)
+		{
+			if(h->len > j)
+				swap(h->data[i],h->data[j]);
+			else
+			{
+				// give up
+				lru_delete(h);
+				free(h->data);
+				size += h->len;
+				h->data = 0;
+				h->len = 0;
+			}
+		}
+	}
+}
+
+//
+// Kernel evaluation
+//
+// the static method k_function is for doing single kernel evaluation
+// the constructor of Kernel prepares to calculate the l*l kernel matrix
+// the member function get_Q is for getting one column from the Q Matrix
+//
+class QMatrix {
+public:
+	virtual Qfloat *get_Q(int column, int len) const = 0;
+	virtual double *get_QD() const = 0;
+	virtual void swap_index(int i, int j) const = 0;
+	virtual ~QMatrix() {}
+};
+
+class Kernel: public QMatrix {
+public:
+#ifdef _STRING
+	Kernel(int l, svm_data const * x, const svm_parameter& param);
+	virtual ~Kernel();
+#else
+	Kernel(int l, svm_node * const * x, const svm_parameter& param);
+	virtual ~Kernel();
+#endif
+
+#ifdef _STRING
+	static double k_function(const svm_data x, const svm_data y,
+				 const svm_parameter& param);
+#else
+	static double k_function(const svm_node *x, const svm_node *y,
+				 const svm_parameter& param);
+#endif
+
+	virtual Qfloat *get_Q(int column, int len) const = 0;
+	virtual double *get_QD() const = 0;
+	virtual void swap_index(int i, int j) const	// no so const...
+	{
+		swap(x[i],x[j]);
+		if(x_square) swap(x_square[i],x_square[j]);
+	}
+protected:
+
+	double (Kernel::*kernel_function)(int i, int j) const;
+
+private:
+
+#ifdef _STRING
+	svm_data *x;
+#else
+	const svm_node **x;
+#endif
+	double *x_square;
+
+	// svm_parameter
+	const int kernel_type;
+	const int degree;
+	const double gamma;
+	const double coef0;
+
+	static double dot(const svm_node *px, const svm_node *py);
+#ifdef _STRING
+	static int edit(const char *px, const char *py);
+	double kernel_linear(int i, int j) const
+	{
+		return dot(x[i].v,x[j].v);
+	}
+	double kernel_poly(int i, int j) const
+	{
+		return powi(gamma*dot(x[i].v,x[j].v)+coef0,degree);
+	}
+	double kernel_rbf(int i, int j) const
+	{
+		return exp(-gamma*(x_square[i]+x_square[j]-2*dot(x[i].v,x[j].v)));
+	}
+	double kernel_sigmoid(int i, int j) const
+	{
+		return tanh(gamma*dot(x[i].v,x[j].v)+coef0);
+	}
+	double kernel_precomputed(int i, int j) const
+	{
+		return x[i].v[(int)(x[j].v[0].value)].value;
+	}
+	double kernel_edit(int i, int j) const
+	{
+		return exp(-gamma*(double)edit(x[i].s,x[j].s));
+	}
+#else
+	double kernel_linear(int i, int j) const
+	{
+		return dot(x[i],x[j]);
+	}
+	double kernel_poly(int i, int j) const
+	{
+		return powi(gamma*dot(x[i],x[j])+coef0,degree);
+	}
+	double kernel_rbf(int i, int j) const
+	{
+		return exp(-gamma*(x_square[i]+x_square[j]-2*dot(x[i],x[j])));
+	}
+	double kernel_sigmoid(int i, int j) const
+	{
+		return tanh(gamma*dot(x[i],x[j])+coef0);
+	}
+	double kernel_precomputed(int i, int j) const
+	{
+		return x[i][(int)(x[j][0].value)].value;
+	}
+#endif
+};
+
+#ifdef _STRING
+Kernel::Kernel(int l, svm_data const * x_, const svm_parameter& param)
+:kernel_type(param.kernel_type), degree(param.degree),
+ gamma(param.gamma), coef0(param.coef0)
+#else
+Kernel::Kernel(int l, svm_node * const * x_, const svm_parameter& param)
+:kernel_type(param.kernel_type), degree(param.degree),
+ gamma(param.gamma), coef0(param.coef0)
+#endif
+{
+	switch(kernel_type)
+	{
+		case LINEAR:
+			kernel_function = &Kernel::kernel_linear;
+			break;
+		case POLY:
+			kernel_function = &Kernel::kernel_poly;
+			break;
+		case RBF:
+			kernel_function = &Kernel::kernel_rbf;
+			break;
+		case SIGMOID:
+			kernel_function = &Kernel::kernel_sigmoid;
+			break;
+		case PRECOMPUTED:
+			kernel_function = &Kernel::kernel_precomputed;
+			break;
+#ifdef _STRING
+		case EDIT:
+			kernel_function = &Kernel::kernel_edit;
+			break;
+#endif
+	}
+
+	clone(x,x_,l);
+
+	if(kernel_type == RBF)
+	{
+		x_square = new double[l];
+		for(int i=0;i<l;i++)
+#ifdef _STRING
+			x_square[i] = dot(x[i].v,x[i].v);
+#else
+			x_square[i] = dot(x[i],x[i]);
+#endif
+	}
+	else
+		x_square = 0;
+}
+
+Kernel::~Kernel()
+{
+	delete[] x;
+	delete[] x_square;
+}
+
+double Kernel::dot(const svm_node *px, const svm_node *py)
+{
+	double sum = 0;
+	while(px->index != -1 && py->index != -1)
+	{
+		if(px->index == py->index)
+		{
+			sum += px->value * py->value;
+			++px;
+			++py;
+		}
+		else
+		{
+			if(px->index > py->index)
+				++py;
+			else
+				++px;
+		}
+	}
+	return sum;
+}
+
+#ifdef _STRING
+int Kernel::edit(const char *px, const char *py)
+{
+	int* row[2];
+	int len1,len2;
+	int now,old;
+	int i,j;
+	int result;
+
+	len1=(int)strlen(px); len2=(int)strlen(py);
+
+	row[0] = (int*) malloc( (len1+1) * sizeof(int) );
+	row[1] = (int*) malloc( (len1+1) * sizeof(int) );
+
+	for(i=0;i<len1+1;i++) row[0][i]=i;
+	now=1; old=0;
+
+	for(j=0;j<len2;j++) /* j=0 is the first char of py */
+	{
+		row[now][0]=j+1;
+		for(i=0;i<len1;i++) /* i=0 is the first char of px */
+		{
+			row[now][i+1] = min(min( row[old][i] + (px[i]==py[j]?0:1) , 
+						 row[old][i+1] + 1),
+					    row[now][i] + 1                        );
+		}
+
+		now=1-now; old=1-old;
+	}
+
+	result=row[old][len1];
+   
+	free(row[0]); free(row[1]);
+   
+	return result;
+}
+#endif
+
+#ifdef _STRING
+double Kernel::k_function(const svm_data x, const svm_data y,
+			  const svm_parameter& param)
+{
+	switch(param.kernel_type)
+	{
+		case LINEAR:
+			return dot(x.v,y.v);
+		case POLY:
+			return powi(param.gamma*dot(x.v,y.v)+param.coef0,param.degree);
+		case RBF:
+		{
+			double sum = 0;
+			svm_node *xv = x.v;
+			svm_node *yv = y.v;
+			while(xv->index != -1 && yv->index !=-1)
+			{
+				if(xv->index == yv->index)
+				{
+					double d = xv->value - yv->value;
+					sum += d*d;
+					++xv;
+					++yv;
+				}
+				else
+				{
+					if(xv->index > yv->index)
+					{	
+						sum += yv->value * yv->value;
+						++yv;
+					}
+					else
+					{
+						sum += xv->value * xv->value;
+						++xv;
+					}
+				}
+			}
+
+			while(xv->index != -1)
+			{
+				sum += xv->value * xv->value;
+				++xv;
+			}
+
+			while(yv->index != -1)
+			{
+				sum += yv->value * yv->value;
+				++yv;
+			}
+			
+			return exp(-param.gamma*sum);
+		}
+		case SIGMOID:
+			return tanh(param.gamma*dot(x.v,y.v)+param.coef0);
+		case PRECOMPUTED:  //x: test (validation), y: SV
+			return x.v[(int)(y.v->value)].value;
+		case EDIT:
+			return exp(-param.gamma*edit(x.s,y.s));
+		default:
+			return 0;	/* Unreachable */
+	}
+}
+#else
+double Kernel::k_function(const svm_node *x, const svm_node *y,
+			  const svm_parameter& param)
+{
+	switch(param.kernel_type)
+	{
+		case LINEAR:
+			return dot(x,y);
+		case POLY:
+			return powi(param.gamma*dot(x,y)+param.coef0,param.degree);
+		case RBF:
+		{
+			double sum = 0;
+			while(x->index != -1 && y->index !=-1)
+			{
+				if(x->index == y->index)
+				{
+					double d = x->value - y->value;
+					sum += d*d;
+					++x;
+					++y;
+				}
+				else
+				{
+					if(x->index > y->index)
+					{
+						sum += y->value * y->value;
+						++y;
+					}
+					else
+					{
+						sum += x->value * x->value;
+						++x;
+					}
+				}
+			}
+
+			while(x->index != -1)
+			{
+				sum += x->value * x->value;
+				++x;
+			}
+
+			while(y->index != -1)
+			{
+				sum += y->value * y->value;
+				++y;
+			}
+
+			return exp(-param.gamma*sum);
+		}
+		case SIGMOID:
+			return tanh(param.gamma*dot(x,y)+param.coef0);
+		case PRECOMPUTED:  //x: test (validation), y: SV
+			return x[(int)(y->value)].value;
+		default:
+			return 0;  // Unreachable
+	}
+}
+#endif
+
+// An SMO algorithm in Fan et al., JMLR 6(2005), p. 1889--1918
+// Solves:
+//
+//	min 0.5(\alpha^T Q \alpha) + p^T \alpha
+//
+//		y^T \alpha = \delta
+//		y_i = +1 or -1
+//		0 <= alpha_i <= Cp for y_i = 1
+//		0 <= alpha_i <= Cn for y_i = -1
+//
+// Given:
+//
+//	Q, p, y, Cp, Cn, and an initial feasible point \alpha
+//	l is the size of vectors and matrices
+//	eps is the stopping tolerance
+//
+// solution will be put in \alpha, objective value will be put in obj
+//
+class Solver {
+public:
+	Solver() {};
+	virtual ~Solver() {};
+
+	struct SolutionInfo {
+		double obj;
+		double rho;
+		double upper_bound_p;
+		double upper_bound_n;
+		double r;	// for Solver_NU
+	};
+
+	void Solve(int l, const QMatrix& Q, const double *p_, const schar *y_,
+		   double *alpha_, double Cp, double Cn, double eps,
+		   SolutionInfo* si, int shrinking);
+protected:
+	int active_size;
+	schar *y;
+	double *G;		// gradient of objective function
+	enum { LOWER_BOUND, UPPER_BOUND, FREE };
+	char *alpha_status;	// LOWER_BOUND, UPPER_BOUND, FREE
+	double *alpha;
+	const QMatrix *Q;
+	const double *QD;
+	double eps;
+	double Cp,Cn;
+	double *p;
+	int *active_set;
+	double *G_bar;		// gradient, if we treat free variables as 0
+	int l;
+	bool unshrink;	// XXX
+
+	double get_C(int i)
+	{
+		return (y[i] > 0)? Cp : Cn;
+	}
+	void update_alpha_status(int i)
+	{
+		if(alpha[i] >= get_C(i))
+			alpha_status[i] = UPPER_BOUND;
+		else if(alpha[i] <= 0)
+			alpha_status[i] = LOWER_BOUND;
+		else alpha_status[i] = FREE;
+	}
+	bool is_upper_bound(int i) { return alpha_status[i] == UPPER_BOUND; }
+	bool is_lower_bound(int i) { return alpha_status[i] == LOWER_BOUND; }
+	bool is_free(int i) { return alpha_status[i] == FREE; }
+	void swap_index(int i, int j);
+	void reconstruct_gradient();
+	virtual int select_working_set(int &i, int &j);
+	virtual double calculate_rho();
+	virtual void do_shrinking();
+private:
+	bool be_shrunk(int i, double Gmax1, double Gmax2);
+};
+
+void Solver::swap_index(int i, int j)
+{
+	Q->swap_index(i,j);
+	swap(y[i],y[j]);
+	swap(G[i],G[j]);
+	swap(alpha_status[i],alpha_status[j]);
+	swap(alpha[i],alpha[j]);
+	swap(p[i],p[j]);
+	swap(active_set[i],active_set[j]);
+	swap(G_bar[i],G_bar[j]);
+}
+
+void Solver::reconstruct_gradient()
+{
+	// reconstruct inactive elements of G from G_bar and free variables
+
+	if(active_size == l) return;
+
+	int i,j;
+	int nr_free = 0;
+
+	for(j=active_size;j<l;j++)
+		G[j] = G_bar[j] + p[j];
+
+	for(j=0;j<active_size;j++)
+		if(is_free(j))
+			nr_free++;
+
+	if(2*nr_free < active_size)
+		info("\nWARNING: using -h 0 may be faster\n");
+
+	if (nr_free*l > 2*active_size*(l-active_size))
+	{
+		for(i=active_size;i<l;i++)
+		{
+			const Qfloat *Q_i = Q->get_Q(i,active_size);
+			for(j=0;j<active_size;j++)
+				if(is_free(j))
+					G[i] += alpha[j] * Q_i[j];
+		}
+	}
+	else
+	{
+		for(i=0;i<active_size;i++)
+			if(is_free(i))
+			{
+				const Qfloat *Q_i = Q->get_Q(i,l);
+				double alpha_i = alpha[i];
+				for(j=active_size;j<l;j++)
+					G[j] += alpha_i * Q_i[j];
+			}
+	}
+}
+
+void Solver::Solve(int l, const QMatrix& Q, const double *p_, const schar *y_,
+		   double *alpha_, double Cp, double Cn, double eps,
+		   SolutionInfo* si, int shrinking)
+{
+	this->l = l;
+	this->Q = &Q;
+	QD=Q.get_QD();
+	clone(p, p_,l);
+	clone(y, y_,l);
+	clone(alpha,alpha_,l);
+	this->Cp = Cp;
+	this->Cn = Cn;
+	this->eps = eps;
+	unshrink = false;
+
+	// initialize alpha_status
+	{
+		alpha_status = new char[l];
+		for(int i=0;i<l;i++)
+			update_alpha_status(i);
+	}
+
+	// initialize active set (for shrinking)
+	{
+		active_set = new int[l];
+		for(int i=0;i<l;i++)
+			active_set[i] = i;
+		active_size = l;
+	}
+
+	// initialize gradient
+	{
+		G = new double[l];
+		G_bar = new double[l];
+		int i;
+		for(i=0;i<l;i++)
+		{
+			G[i] = p[i];
+			G_bar[i] = 0;
+		}
+		for(i=0;i<l;i++)
+			if(!is_lower_bound(i))
+			{
+				const Qfloat *Q_i = Q.get_Q(i,l);
+				double alpha_i = alpha[i];
+				int j;
+				for(j=0;j<l;j++)
+					G[j] += alpha_i*Q_i[j];
+				if(is_upper_bound(i))
+					for(j=0;j<l;j++)
+						G_bar[j] += get_C(i) * Q_i[j];
+			}
+	}
+
+	// optimization step
+
+	int iter = 0;
+	int max_iter = max(10000000, l>INT_MAX/100 ? INT_MAX : 100*l);
+	int counter = min(l,1000)+1;
+
+	while(iter < max_iter)
+	{
+		// show progress and do shrinking
+
+		if(--counter == 0)
+		{
+			counter = min(l,1000);
+			if(shrinking) do_shrinking();
+			info(".");
+		}
+
+		int i,j;
+		if(select_working_set(i,j)!=0)
+		{
+			// reconstruct the whole gradient
+			reconstruct_gradient();
+			// reset active set size and check
+			active_size = l;
+			info("*");
+			if(select_working_set(i,j)!=0)
+				break;
+			else
+				counter = 1;	// do shrinking next iteration
+		}
+
+		++iter;
+
+		// update alpha[i] and alpha[j], handle bounds carefully
+
+		const Qfloat *Q_i = Q.get_Q(i,active_size);
+		const Qfloat *Q_j = Q.get_Q(j,active_size);
+
+		double C_i = get_C(i);
+		double C_j = get_C(j);
+
+		double old_alpha_i = alpha[i];
+		double old_alpha_j = alpha[j];
+
+		if(y[i]!=y[j])
+		{
+			double quad_coef = QD[i]+QD[j]+2*Q_i[j];
+			if (quad_coef <= 0)
+				quad_coef = TAU;
+			double delta = (-G[i]-G[j])/quad_coef;
+			double diff = alpha[i] - alpha[j];
+			alpha[i] += delta;
+			alpha[j] += delta;
+
+			if(diff > 0)
+			{
+				if(alpha[j] < 0)
+				{
+					alpha[j] = 0;
+					alpha[i] = diff;
+				}
+			}
+			else
+			{
+				if(alpha[i] < 0)
+				{
+					alpha[i] = 0;
+					alpha[j] = -diff;
+				}
+			}
+			if(diff > C_i - C_j)
+			{
+				if(alpha[i] > C_i)
+				{
+					alpha[i] = C_i;
+					alpha[j] = C_i - diff;
+				}
+			}
+			else
+			{
+				if(alpha[j] > C_j)
+				{
+					alpha[j] = C_j;
+					alpha[i] = C_j + diff;
+				}
+			}
+		}
+		else
+		{
+			double quad_coef = QD[i]+QD[j]-2*Q_i[j];
+			if (quad_coef <= 0)
+				quad_coef = TAU;
+			double delta = (G[i]-G[j])/quad_coef;
+			double sum = alpha[i] + alpha[j];
+			alpha[i] -= delta;
+			alpha[j] += delta;
+
+			if(sum > C_i)
+			{
+				if(alpha[i] > C_i)
+				{
+					alpha[i] = C_i;
+					alpha[j] = sum - C_i;
+				}
+			}
+			else
+			{
+				if(alpha[j] < 0)
+				{
+					alpha[j] = 0;
+					alpha[i] = sum;
+				}
+			}
+			if(sum > C_j)
+			{
+				if(alpha[j] > C_j)
+				{
+					alpha[j] = C_j;
+					alpha[i] = sum - C_j;
+				}
+			}
+			else
+			{
+				if(alpha[i] < 0)
+				{
+					alpha[i] = 0;
+					alpha[j] = sum;
+				}
+			}
+		}
+
+		// update G
+
+		double delta_alpha_i = alpha[i] - old_alpha_i;
+		double delta_alpha_j = alpha[j] - old_alpha_j;
+
+		for(int k=0;k<active_size;k++)
+		{
+			G[k] += Q_i[k]*delta_alpha_i + Q_j[k]*delta_alpha_j;
+		}
+
+		// update alpha_status and G_bar
+
+		{
+			bool ui = is_upper_bound(i);
+			bool uj = is_upper_bound(j);
+			update_alpha_status(i);
+			update_alpha_status(j);
+			int k;
+			if(ui != is_upper_bound(i))
+			{
+				Q_i = Q.get_Q(i,l);
+				if(ui)
+					for(k=0;k<l;k++)
+						G_bar[k] -= C_i * Q_i[k];
+				else
+					for(k=0;k<l;k++)
+						G_bar[k] += C_i * Q_i[k];
+			}
+
+			if(uj != is_upper_bound(j))
+			{
+				Q_j = Q.get_Q(j,l);
+				if(uj)
+					for(k=0;k<l;k++)
+						G_bar[k] -= C_j * Q_j[k];
+				else
+					for(k=0;k<l;k++)
+						G_bar[k] += C_j * Q_j[k];
+			}
+		}
+	}
+
+	if(iter >= max_iter)
+	{
+		if(active_size < l)
+		{
+			// reconstruct the whole gradient to calculate objective value
+			reconstruct_gradient();
+			active_size = l;
+			info("*");
+		}
+		fprintf(stderr,"\nWARNING: reaching max number of iterations\n");
+	}
+
+	// calculate rho
+
+	si->rho = calculate_rho();
+
+	// calculate objective value
+	{
+		double v = 0;
+		int i;
+		for(i=0;i<l;i++)
+			v += alpha[i] * (G[i] + p[i]);
+
+		si->obj = v/2;
+	}
+
+	// put back the solution
+	{
+		for(int i=0;i<l;i++)
+			alpha_[active_set[i]] = alpha[i];
+	}
+
+	// juggle everything back
+	/*{
+		for(int i=0;i<l;i++)
+			while(active_set[i] != i)
+				swap_index(i,active_set[i]);
+				// or Q.swap_index(i,active_set[i]);
+	}*/
+
+	si->upper_bound_p = Cp;
+	si->upper_bound_n = Cn;
+
+	info("\noptimization finished, #iter = %d\n",iter);
+
+	delete[] p;
+	delete[] y;
+	delete[] alpha;
+	delete[] alpha_status;
+	delete[] active_set;
+	delete[] G;
+	delete[] G_bar;
+}
+
+// return 1 if already optimal, return 0 otherwise
+int Solver::select_working_set(int &out_i, int &out_j)
+{
+	// return i,j such that
+	// i: maximizes -y_i * grad(f)_i, i in I_up(\alpha)
+	// j: minimizes the decrease of obj value
+	//    (if quadratic coefficeint <= 0, replace it with tau)
+	//    -y_j*grad(f)_j < -y_i*grad(f)_i, j in I_low(\alpha)
+
+	double Gmax = -INF;
+	double Gmax2 = -INF;
+	int Gmax_idx = -1;
+	int Gmin_idx = -1;
+	double obj_diff_min = INF;
+
+	for(int t=0;t<active_size;t++)
+		if(y[t]==+1)
+		{
+			if(!is_upper_bound(t))
+				if(-G[t] >= Gmax)
+				{
+					Gmax = -G[t];
+					Gmax_idx = t;
+				}
+		}
+		else
+		{
+			if(!is_lower_bound(t))
+				if(G[t] >= Gmax)
+				{
+					Gmax = G[t];
+					Gmax_idx = t;
+				}
+		}
+
+	int i = Gmax_idx;
+	const Qfloat *Q_i = NULL;
+	if(i != -1) // NULL Q_i not accessed: Gmax=-INF if i=-1
+		Q_i = Q->get_Q(i,active_size);
+
+	for(int j=0;j<active_size;j++)
+	{
+		if(y[j]==+1)
+		{
+			if (!is_lower_bound(j))
+			{
+				double grad_diff=Gmax+G[j];
+				if (G[j] >= Gmax2)
+					Gmax2 = G[j];
+				if (grad_diff > 0)
+				{
+					double obj_diff;
+					double quad_coef = QD[i]+QD[j]-2.0*y[i]*Q_i[j];
+					if (quad_coef > 0)
+						obj_diff = -(grad_diff*grad_diff)/quad_coef;
+					else
+						obj_diff = -(grad_diff*grad_diff)/TAU;
+
+					if (obj_diff <= obj_diff_min)
+					{
+						Gmin_idx=j;
+						obj_diff_min = obj_diff;
+					}
+				}
+			}
+		}
+		else
+		{
+			if (!is_upper_bound(j))
+			{
+				double grad_diff= Gmax-G[j];
+				if (-G[j] >= Gmax2)
+					Gmax2 = -G[j];
+				if (grad_diff > 0)
+				{
+					double obj_diff;
+					double quad_coef = QD[i]+QD[j]+2.0*y[i]*Q_i[j];
+					if (quad_coef > 0)
+						obj_diff = -(grad_diff*grad_diff)/quad_coef;
+					else
+						obj_diff = -(grad_diff*grad_diff)/TAU;
+
+					if (obj_diff <= obj_diff_min)
+					{
+						Gmin_idx=j;
+						obj_diff_min = obj_diff;
+					}
+				}
+			}
+		}
+	}
+
+	if(Gmax+Gmax2 < eps || Gmin_idx == -1)
+		return 1;
+
+	out_i = Gmax_idx;
+	out_j = Gmin_idx;
+	return 0;
+}
+
+bool Solver::be_shrunk(int i, double Gmax1, double Gmax2)
+{
+	if(is_upper_bound(i))
+	{
+		if(y[i]==+1)
+			return(-G[i] > Gmax1);
+		else
+			return(-G[i] > Gmax2);
+	}
+	else if(is_lower_bound(i))
+	{
+		if(y[i]==+1)
+			return(G[i] > Gmax2);
+		else
+			return(G[i] > Gmax1);
+	}
+	else
+		return(false);
+}
+
+void Solver::do_shrinking()
+{
+	int i;
+	double Gmax1 = -INF;		// max { -y_i * grad(f)_i | i in I_up(\alpha) }
+	double Gmax2 = -INF;		// max { y_i * grad(f)_i | i in I_low(\alpha) }
+
+	// find maximal violating pair first
+	for(i=0;i<active_size;i++)
+	{
+		if(y[i]==+1)
+		{
+			if(!is_upper_bound(i))
+			{
+				if(-G[i] >= Gmax1)
+					Gmax1 = -G[i];
+			}
+			if(!is_lower_bound(i))
+			{
+				if(G[i] >= Gmax2)
+					Gmax2 = G[i];
+			}
+		}
+		else
+		{
+			if(!is_upper_bound(i))
+			{
+				if(-G[i] >= Gmax2)
+					Gmax2 = -G[i];
+			}
+			if(!is_lower_bound(i))
+			{
+				if(G[i] >= Gmax1)
+					Gmax1 = G[i];
+			}
+		}
+	}
+
+	if(unshrink == false && Gmax1 + Gmax2 <= eps*10)
+	{
+		unshrink = true;
+		reconstruct_gradient();
+		active_size = l;
+		info("*");
+	}
+
+	for(i=0;i<active_size;i++)
+		if (be_shrunk(i, Gmax1, Gmax2))
+		{
+			active_size--;
+			while (active_size > i)
+			{
+				if (!be_shrunk(active_size, Gmax1, Gmax2))
+				{
+					swap_index(i,active_size);
+					break;
+				}
+				active_size--;
+			}
+		}
+}
+
+double Solver::calculate_rho()
+{
+	double r;
+	int nr_free = 0;
+	double ub = INF, lb = -INF, sum_free = 0;
+	for(int i=0;i<active_size;i++)
+	{
+		double yG = y[i]*G[i];
+
+		if(is_upper_bound(i))
+		{
+			if(y[i]==-1)
+				ub = min(ub,yG);
+			else
+				lb = max(lb,yG);
+		}
+		else if(is_lower_bound(i))
+		{
+			if(y[i]==+1)
+				ub = min(ub,yG);
+			else
+				lb = max(lb,yG);
+		}
+		else
+		{
+			++nr_free;
+			sum_free += yG;
+		}
+	}
+
+	if(nr_free>0)
+		r = sum_free/nr_free;
+	else
+		r = (ub+lb)/2;
+
+	return r;
+}
+
+//
+// Solver for nu-svm classification and regression
+//
+// additional constraint: e^T \alpha = constant
+//
+class Solver_NU: public Solver
+{
+public:
+	Solver_NU() {}
+	void Solve(int l, const QMatrix& Q, const double *p, const schar *y,
+		   double *alpha, double Cp, double Cn, double eps,
+		   SolutionInfo* si, int shrinking)
+	{
+		this->si = si;
+		Solver::Solve(l,Q,p,y,alpha,Cp,Cn,eps,si,shrinking);
+	}
+private:
+	SolutionInfo *si;
+	int select_working_set(int &i, int &j);
+	double calculate_rho();
+	bool be_shrunk(int i, double Gmax1, double Gmax2, double Gmax3, double Gmax4);
+	void do_shrinking();
+};
+
+// return 1 if already optimal, return 0 otherwise
+int Solver_NU::select_working_set(int &out_i, int &out_j)
+{
+	// return i,j such that y_i = y_j and
+	// i: maximizes -y_i * grad(f)_i, i in I_up(\alpha)
+	// j: minimizes the decrease of obj value
+	//    (if quadratic coefficeint <= 0, replace it with tau)
+	//    -y_j*grad(f)_j < -y_i*grad(f)_i, j in I_low(\alpha)
+
+	double Gmaxp = -INF;
+	double Gmaxp2 = -INF;
+	int Gmaxp_idx = -1;
+
+	double Gmaxn = -INF;
+	double Gmaxn2 = -INF;
+	int Gmaxn_idx = -1;
+
+	int Gmin_idx = -1;
+	double obj_diff_min = INF;
+
+	for(int t=0;t<active_size;t++)
+		if(y[t]==+1)
+		{
+			if(!is_upper_bound(t))
+				if(-G[t] >= Gmaxp)
+				{
+					Gmaxp = -G[t];
+					Gmaxp_idx = t;
+				}
+		}
+		else
+		{
+			if(!is_lower_bound(t))
+				if(G[t] >= Gmaxn)
+				{
+					Gmaxn = G[t];
+					Gmaxn_idx = t;
+				}
+		}
+
+	int ip = Gmaxp_idx;
+	int in = Gmaxn_idx;
+	const Qfloat *Q_ip = NULL;
+	const Qfloat *Q_in = NULL;
+	if(ip != -1) // NULL Q_ip not accessed: Gmaxp=-INF if ip=-1
+		Q_ip = Q->get_Q(ip,active_size);
+	if(in != -1)
+		Q_in = Q->get_Q(in,active_size);
+
+	for(int j=0;j<active_size;j++)
+	{
+		if(y[j]==+1)
+		{
+			if (!is_lower_bound(j))
+			{
+				double grad_diff=Gmaxp+G[j];
+				if (G[j] >= Gmaxp2)
+					Gmaxp2 = G[j];
+				if (grad_diff > 0)
+				{
+					double obj_diff;
+					double quad_coef = QD[ip]+QD[j]-2*Q_ip[j];
+					if (quad_coef > 0)
+						obj_diff = -(grad_diff*grad_diff)/quad_coef;
+					else
+						obj_diff = -(grad_diff*grad_diff)/TAU;
+
+					if (obj_diff <= obj_diff_min)
+					{
+						Gmin_idx=j;
+						obj_diff_min = obj_diff;
+					}
+				}
+			}
+		}
+		else
+		{
+			if (!is_upper_bound(j))
+			{
+				double grad_diff=Gmaxn-G[j];
+				if (-G[j] >= Gmaxn2)
+					Gmaxn2 = -G[j];
+				if (grad_diff > 0)
+				{
+					double obj_diff;
+					double quad_coef = QD[in]+QD[j]-2*Q_in[j];
+					if (quad_coef > 0)
+						obj_diff = -(grad_diff*grad_diff)/quad_coef;
+					else
+						obj_diff = -(grad_diff*grad_diff)/TAU;
+
+					if (obj_diff <= obj_diff_min)
+					{
+						Gmin_idx=j;
+						obj_diff_min = obj_diff;
+					}
+				}
+			}
+		}
+	}
+
+	if(max(Gmaxp+Gmaxp2,Gmaxn+Gmaxn2) < eps || Gmin_idx == -1)
+		return 1;
+
+	if (y[Gmin_idx] == +1)
+		out_i = Gmaxp_idx;
+	else
+		out_i = Gmaxn_idx;
+	out_j = Gmin_idx;
+
+	return 0;
+}
+
+bool Solver_NU::be_shrunk(int i, double Gmax1, double Gmax2, double Gmax3, double Gmax4)
+{
+	if(is_upper_bound(i))
+	{
+		if(y[i]==+1)
+			return(-G[i] > Gmax1);
+		else
+			return(-G[i] > Gmax4);
+	}
+	else if(is_lower_bound(i))
+	{
+		if(y[i]==+1)
+			return(G[i] > Gmax2);
+		else
+			return(G[i] > Gmax3);
+	}
+	else
+		return(false);
+}
+
+void Solver_NU::do_shrinking()
+{
+	double Gmax1 = -INF;	// max { -y_i * grad(f)_i | y_i = +1, i in I_up(\alpha) }
+	double Gmax2 = -INF;	// max { y_i * grad(f)_i | y_i = +1, i in I_low(\alpha) }
+	double Gmax3 = -INF;	// max { -y_i * grad(f)_i | y_i = -1, i in I_up(\alpha) }
+	double Gmax4 = -INF;	// max { y_i * grad(f)_i | y_i = -1, i in I_low(\alpha) }
+
+	// find maximal violating pair first
+	int i;
+	for(i=0;i<active_size;i++)
+	{
+		if(!is_upper_bound(i))
+		{
+			if(y[i]==+1)
+			{
+				if(-G[i] > Gmax1) Gmax1 = -G[i];
+			}
+			else	if(-G[i] > Gmax4) Gmax4 = -G[i];
+		}
+		if(!is_lower_bound(i))
+		{
+			if(y[i]==+1)
+			{
+				if(G[i] > Gmax2) Gmax2 = G[i];
+			}
+			else	if(G[i] > Gmax3) Gmax3 = G[i];
+		}
+	}
+
+	if(unshrink == false && max(Gmax1+Gmax2,Gmax3+Gmax4) <= eps*10)
+	{
+		unshrink = true;
+		reconstruct_gradient();
+		active_size = l;
+	}
+
+	for(i=0;i<active_size;i++)
+		if (be_shrunk(i, Gmax1, Gmax2, Gmax3, Gmax4))
+		{
+			active_size--;
+			while (active_size > i)
+			{
+				if (!be_shrunk(active_size, Gmax1, Gmax2, Gmax3, Gmax4))
+				{
+					swap_index(i,active_size);
+					break;
+				}
+				active_size--;
+			}
+		}
+}
+
+double Solver_NU::calculate_rho()
+{
+	int nr_free1 = 0,nr_free2 = 0;
+	double ub1 = INF, ub2 = INF;
+	double lb1 = -INF, lb2 = -INF;
+	double sum_free1 = 0, sum_free2 = 0;
+
+	for(int i=0;i<active_size;i++)
+	{
+		if(y[i]==+1)
+		{
+			if(is_upper_bound(i))
+				lb1 = max(lb1,G[i]);
+			else if(is_lower_bound(i))
+				ub1 = min(ub1,G[i]);
+			else
+			{
+				++nr_free1;
+				sum_free1 += G[i];
+			}
+		}
+		else
+		{
+			if(is_upper_bound(i))
+				lb2 = max(lb2,G[i]);
+			else if(is_lower_bound(i))
+				ub2 = min(ub2,G[i]);
+			else
+			{
+				++nr_free2;
+				sum_free2 += G[i];
+			}
+		}
+	}
+
+	double r1,r2;
+	if(nr_free1 > 0)
+		r1 = sum_free1/nr_free1;
+	else
+		r1 = (ub1+lb1)/2;
+
+	if(nr_free2 > 0)
+		r2 = sum_free2/nr_free2;
+	else
+		r2 = (ub2+lb2)/2;
+
+	si->r = (r1+r2)/2;
+	return (r1-r2)/2;
+}
+
+//
+// Q matrices for various formulations
+//
+class SVC_Q: public Kernel
+{
+public:
+	SVC_Q(const svm_problem& prob, const svm_parameter& param, const schar *y_)
+	:Kernel(prob.l, prob.x, param)
+	{
+		clone(y,y_,prob.l);
+		cache = new Cache(prob.l,(long int)(param.cache_size*(1<<20)));
+		QD = new double[prob.l];
+		for(int i=0;i<prob.l;i++)
+			QD[i] = (this->*kernel_function)(i,i);
+	}
+
+	Qfloat *get_Q(int i, int len) const
+	{
+		Qfloat *data;
+		int start, j;
+		if((start = cache->get_data(i,&data,len)) < len)
+		{
+			for(j=start;j<len;j++)
+				data[j] = (Qfloat)(y[i]*y[j]*(this->*kernel_function)(i,j));
+		}
+		return data;
+	}
+
+	double *get_QD() const
+	{
+		return QD;
+	}
+
+	void swap_index(int i, int j) const
+	{
+		cache->swap_index(i,j);
+		Kernel::swap_index(i,j);
+		swap(y[i],y[j]);
+		swap(QD[i],QD[j]);
+	}
+
+	~SVC_Q()
+	{
+		delete[] y;
+		delete cache;
+		delete[] QD;
+	}
+private:
+	schar *y;
+	Cache *cache;
+	double *QD;
+};
+
+class ONE_CLASS_Q: public Kernel
+{
+public:
+	ONE_CLASS_Q(const svm_problem& prob, const svm_parameter& param)
+	:Kernel(prob.l, prob.x, param)
+	{
+		cache = new Cache(prob.l,(long int)(param.cache_size*(1<<20)));
+		QD = new double[prob.l];
+		for(int i=0;i<prob.l;i++)
+			QD[i] = (this->*kernel_function)(i,i);
+	}
+
+	Qfloat *get_Q(int i, int len) const
+	{
+		Qfloat *data;
+		int start, j;
+		if((start = cache->get_data(i,&data,len)) < len)
+		{
+			for(j=start;j<len;j++)
+				data[j] = (Qfloat)(this->*kernel_function)(i,j);
+		}
+		return data;
+	}
+
+	double *get_QD() const
+	{
+		return QD;
+	}
+
+	void swap_index(int i, int j) const
+	{
+		cache->swap_index(i,j);
+		Kernel::swap_index(i,j);
+		swap(QD[i],QD[j]);
+	}
+
+	~ONE_CLASS_Q()
+	{
+		delete cache;
+		delete[] QD;
+	}
+private:
+	Cache *cache;
+	double *QD;
+};
+
+class SVR_Q: public Kernel
+{
+public:
+	SVR_Q(const svm_problem& prob, const svm_parameter& param)
+	:Kernel(prob.l, prob.x, param)
+	{
+		l = prob.l;
+		cache = new Cache(l,(long int)(param.cache_size*(1<<20)));
+		QD = new double[2*l];
+		sign = new schar[2*l];
+		index = new int[2*l];
+		for(int k=0;k<l;k++)
+		{
+			sign[k] = 1;
+			sign[k+l] = -1;
+			index[k] = k;
+			index[k+l] = k;
+			QD[k] = (this->*kernel_function)(k,k);
+			QD[k+l] = QD[k];
+		}
+		buffer[0] = new Qfloat[2*l];
+		buffer[1] = new Qfloat[2*l];
+		next_buffer = 0;
+	}
+
+	void swap_index(int i, int j) const
+	{
+		swap(sign[i],sign[j]);
+		swap(index[i],index[j]);
+		swap(QD[i],QD[j]);
+	}
+
+	Qfloat *get_Q(int i, int len) const
+	{
+		Qfloat *data;
+		int j, real_i = index[i];
+		if(cache->get_data(real_i,&data,l) < l)
+		{
+			for(j=0;j<l;j++)
+				data[j] = (Qfloat)(this->*kernel_function)(real_i,j);
+		}
+
+		// reorder and copy
+		Qfloat *buf = buffer[next_buffer];
+		next_buffer = 1 - next_buffer;
+		schar si = sign[i];
+		for(j=0;j<len;j++)
+			buf[j] = (Qfloat) si * (Qfloat) sign[j] * data[index[j]];
+		return buf;
+	}
+
+	double *get_QD() const
+	{
+		return QD;
+	}
+
+	~SVR_Q()
+	{
+		delete cache;
+		delete[] sign;
+		delete[] index;
+		delete[] buffer[0];
+		delete[] buffer[1];
+		delete[] QD;
+	}
+private:
+	int l;
+	Cache *cache;
+	schar *sign;
+	int *index;
+	mutable int next_buffer;
+	Qfloat *buffer[2];
+	double *QD;
+};
+
+//
+// construct and solve various formulations
+//
+static void solve_c_svc(
+	const svm_problem *prob, const svm_parameter* param,
+	double *alpha, Solver::SolutionInfo* si, double Cp, double Cn)
+{
+	int l = prob->l;
+	double *minus_ones = new double[l];
+	schar *y = new schar[l];
+
+	int i;
+
+	for(i=0;i<l;i++)
+	{
+		alpha[i] = 0;
+		minus_ones[i] = -1;
+		if(prob->y[i] > 0) y[i] = +1; else y[i] = -1;
+	}
+
+	Solver s;
+	s.Solve(l, SVC_Q(*prob,*param,y), minus_ones, y,
+		alpha, Cp, Cn, param->eps, si, param->shrinking);
+
+	double sum_alpha=0;
+	for(i=0;i<l;i++)
+		sum_alpha += alpha[i];
+
+	if (Cp==Cn)
+		info("nu = %f\n", sum_alpha/(Cp*prob->l));
+
+	for(i=0;i<l;i++)
+		alpha[i] *= y[i];
+
+	delete[] minus_ones;
+	delete[] y;
+}
+
+static void solve_nu_svc(
+	const svm_problem *prob, const svm_parameter *param,
+	double *alpha, Solver::SolutionInfo* si)
+{
+	int i;
+	int l = prob->l;
+	double nu = param->nu;
+
+	schar *y = new schar[l];
+
+	for(i=0;i<l;i++)
+		if(prob->y[i]>0)
+			y[i] = +1;
+		else
+			y[i] = -1;
+
+	double sum_pos = nu*l/2;
+	double sum_neg = nu*l/2;
+
+	for(i=0;i<l;i++)
+		if(y[i] == +1)
+		{
+			alpha[i] = min(1.0,sum_pos);
+			sum_pos -= alpha[i];
+		}
+		else
+		{
+			alpha[i] = min(1.0,sum_neg);
+			sum_neg -= alpha[i];
+		}
+
+	double *zeros = new double[l];
+
+	for(i=0;i<l;i++)
+		zeros[i] = 0;
+
+	Solver_NU s;
+	s.Solve(l, SVC_Q(*prob,*param,y), zeros, y,
+		alpha, 1.0, 1.0, param->eps, si,  param->shrinking);
+	double r = si->r;
+
+	info("C = %f\n",1/r);
+
+	for(i=0;i<l;i++)
+		alpha[i] *= y[i]/r;
+
+	si->rho /= r;
+	si->obj /= (r*r);
+	si->upper_bound_p = 1/r;
+	si->upper_bound_n = 1/r;
+
+	delete[] y;
+	delete[] zeros;
+}
+
+static void solve_one_class(
+	const svm_problem *prob, const svm_parameter *param,
+	double *alpha, Solver::SolutionInfo* si)
+{
+	int l = prob->l;
+	double *zeros = new double[l];
+	schar *ones = new schar[l];
+	int i;
+
+	int n = (int)(param->nu*prob->l);	// # of alpha's at upper bound
+
+	for(i=0;i<n;i++)
+		alpha[i] = 1;
+	if(n<prob->l)
+		alpha[n] = param->nu * prob->l - n;
+	for(i=n+1;i<l;i++)
+		alpha[i] = 0;
+
+	for(i=0;i<l;i++)
+	{
+		zeros[i] = 0;
+		ones[i] = 1;
+	}
+
+	Solver s;
+	s.Solve(l, ONE_CLASS_Q(*prob,*param), zeros, ones,
+		alpha, 1.0, 1.0, param->eps, si, param->shrinking);
+
+	delete[] zeros;
+	delete[] ones;
+}
+
+static void solve_epsilon_svr(
+	const svm_problem *prob, const svm_parameter *param,
+	double *alpha, Solver::SolutionInfo* si)
+{
+	int l = prob->l;
+	double *alpha2 = new double[2*l];
+	double *linear_term = new double[2*l];
+	schar *y = new schar[2*l];
+	int i;
+
+	for(i=0;i<l;i++)
+	{
+		alpha2[i] = 0;
+		linear_term[i] = param->p - prob->y[i];
+		y[i] = 1;
+
+		alpha2[i+l] = 0;
+		linear_term[i+l] = param->p + prob->y[i];
+		y[i+l] = -1;
+	}
+
+	Solver s;
+	s.Solve(2*l, SVR_Q(*prob,*param), linear_term, y,
+		alpha2, param->C, param->C, param->eps, si, param->shrinking);
+
+	double sum_alpha = 0;
+	for(i=0;i<l;i++)
+	{
+		alpha[i] = alpha2[i] - alpha2[i+l];
+		sum_alpha += fabs(alpha[i]);
+	}
+	info("nu = %f\n",sum_alpha/(param->C*l));
+
+	delete[] alpha2;
+	delete[] linear_term;
+	delete[] y;
+}
+
+static void solve_nu_svr(
+	const svm_problem *prob, const svm_parameter *param,
+	double *alpha, Solver::SolutionInfo* si)
+{
+	int l = prob->l;
+	double C = param->C;
+	double *alpha2 = new double[2*l];
+	double *linear_term = new double[2*l];
+	schar *y = new schar[2*l];
+	int i;
+
+	double sum = C * param->nu * l / 2;
+	for(i=0;i<l;i++)
+	{
+		alpha2[i] = alpha2[i+l] = min(sum,C);
+		sum -= alpha2[i];
+
+		linear_term[i] = - prob->y[i];
+		y[i] = 1;
+
+		linear_term[i+l] = prob->y[i];
+		y[i+l] = -1;
+	}
+
+	Solver_NU s;
+	s.Solve(2*l, SVR_Q(*prob,*param), linear_term, y,
+		alpha2, C, C, param->eps, si, param->shrinking);
+
+	info("epsilon = %f\n",-si->r);
+
+	for(i=0;i<l;i++)
+		alpha[i] = alpha2[i] - alpha2[i+l];
+
+	delete[] alpha2;
+	delete[] linear_term;
+	delete[] y;
+}
+
+//
+// decision_function
+//
+struct decision_function
+{
+	double *alpha;
+	double rho;
+};
+
+static decision_function svm_train_one(
+	const svm_problem *prob, const svm_parameter *param,
+	double Cp, double Cn)
+{
+	double *alpha = Malloc(double,prob->l);
+	Solver::SolutionInfo si;
+	switch(param->svm_type)
+	{
+		case C_SVC:
+			solve_c_svc(prob,param,alpha,&si,Cp,Cn);
+			break;
+		case NU_SVC:
+			solve_nu_svc(prob,param,alpha,&si);
+			break;
+		case ONE_CLASS:
+			solve_one_class(prob,param,alpha,&si);
+			break;
+		case EPSILON_SVR:
+			solve_epsilon_svr(prob,param,alpha,&si);
+			break;
+		case NU_SVR:
+			solve_nu_svr(prob,param,alpha,&si);
+			break;
+	}
+
+	info("obj = %f, rho = %f\n",si.obj,si.rho);
+
+	// output SVs
+
+	int nSV = 0;
+	int nBSV = 0;
+	for(int i=0;i<prob->l;i++)
+	{
+		if(fabs(alpha[i]) > 0)
+		{
+			++nSV;
+			if(prob->y[i] > 0)
+			{
+				if(fabs(alpha[i]) >= si.upper_bound_p)
+					++nBSV;
+			}
+			else
+			{
+				if(fabs(alpha[i]) >= si.upper_bound_n)
+					++nBSV;
+			}
+		}
+	}
+
+	info("nSV = %d, nBSV = %d\n",nSV,nBSV);
+
+	decision_function f;
+	f.alpha = alpha;
+	f.rho = si.rho;
+	return f;
+}
+
+// Platt's binary SVM Probablistic Output: an improvement from Lin et al.
+static void sigmoid_train(
+	int l, const double *dec_values, const double *labels,
+	double& A, double& B)
+{
+	double prior1=0, prior0 = 0;
+	int i;
+
+	for (i=0;i<l;i++)
+		if (labels[i] > 0) prior1+=1;
+		else prior0+=1;
+
+	int max_iter=100;	// Maximal number of iterations
+	double min_step=1e-10;	// Minimal step taken in line search
+	double sigma=1e-12;	// For numerically strict PD of Hessian
+	double eps=1e-5;
+	double hiTarget=(prior1+1.0)/(prior1+2.0);
+	double loTarget=1/(prior0+2.0);
+	double *t=Malloc(double,l);
+	double fApB,p,q,h11,h22,h21,g1,g2,det,dA,dB,gd,stepsize;
+	double newA,newB,newf,d1,d2;
+	int iter;
+
+	// Initial Point and Initial Fun Value
+	A=0.0; B=log((prior0+1.0)/(prior1+1.0));
+	double fval = 0.0;
+
+	for (i=0;i<l;i++)
+	{
+		if (labels[i]>0) t[i]=hiTarget;
+		else t[i]=loTarget;
+		fApB = dec_values[i]*A+B;
+		if (fApB>=0)
+			fval += t[i]*fApB + log(1+exp(-fApB));
+		else
+			fval += (t[i] - 1)*fApB +log(1+exp(fApB));
+	}
+	for (iter=0;iter<max_iter;iter++)
+	{
+		// Update Gradient and Hessian (use H' = H + sigma I)
+		h11=sigma; // numerically ensures strict PD
+		h22=sigma;
+		h21=0.0;g1=0.0;g2=0.0;
+		for (i=0;i<l;i++)
+		{
+			fApB = dec_values[i]*A+B;
+			if (fApB >= 0)
+			{
+				p=exp(-fApB)/(1.0+exp(-fApB));
+				q=1.0/(1.0+exp(-fApB));
+			}
+			else
+			{
+				p=1.0/(1.0+exp(fApB));
+				q=exp(fApB)/(1.0+exp(fApB));
+			}
+			d2=p*q;
+			h11+=dec_values[i]*dec_values[i]*d2;
+			h22+=d2;
+			h21+=dec_values[i]*d2;
+			d1=t[i]-p;
+			g1+=dec_values[i]*d1;
+			g2+=d1;
+		}
+
+		// Stopping Criteria
+		if (fabs(g1)<eps && fabs(g2)<eps)
+			break;
+
+		// Finding Newton direction: -inv(H') * g
+		det=h11*h22-h21*h21;
+		dA=-(h22*g1 - h21 * g2) / det;
+		dB=-(-h21*g1+ h11 * g2) / det;
+		gd=g1*dA+g2*dB;
+
+
+		stepsize = 1;		// Line Search
+		while (stepsize >= min_step)
+		{
+			newA = A + stepsize * dA;
+			newB = B + stepsize * dB;
+
+			// New function value
+			newf = 0.0;
+			for (i=0;i<l;i++)
+			{
+				fApB = dec_values[i]*newA+newB;
+				if (fApB >= 0)
+					newf += t[i]*fApB + log(1+exp(-fApB));
+				else
+					newf += (t[i] - 1)*fApB +log(1+exp(fApB));
+			}
+			// Check sufficient decrease
+			if (newf<fval+0.0001*stepsize*gd)
+			{
+				A=newA;B=newB;fval=newf;
+				break;
+			}
+			else
+				stepsize = stepsize / 2.0;
+		}
+
+		if (stepsize < min_step)
+		{
+			info("Line search fails in two-class probability estimates\n");
+			break;
+		}
+	}
+
+	if (iter>=max_iter)
+		info("Reaching maximal iterations in two-class probability estimates\n");
+	free(t);
+}
+
+static double sigmoid_predict(double decision_value, double A, double B)
+{
+	double fApB = decision_value*A+B;
+	// 1-p used later; avoid catastrophic cancellation
+	if (fApB >= 0)
+		return exp(-fApB)/(1.0+exp(-fApB));
+	else
+		return 1.0/(1+exp(fApB)) ;
+}
+
+// Method 2 from the multiclass_prob paper by Wu, Lin, and Weng
+static void multiclass_probability(int k, double **r, double *p)
+{
+	int t,j;
+	int iter = 0, max_iter=max(100,k);
+	double **Q=Malloc(double *,k);
+	double *Qp=Malloc(double,k);
+	double pQp, eps=0.005/k;
+
+	for (t=0;t<k;t++)
+	{
+		p[t]=1.0/k;  // Valid if k = 1
+		Q[t]=Malloc(double,k);
+		Q[t][t]=0;
+		for (j=0;j<t;j++)
+		{
+			Q[t][t]+=r[j][t]*r[j][t];
+			Q[t][j]=Q[j][t];
+		}
+		for (j=t+1;j<k;j++)
+		{
+			Q[t][t]+=r[j][t]*r[j][t];
+			Q[t][j]=-r[j][t]*r[t][j];
+		}
+	}
+	for (iter=0;iter<max_iter;iter++)
+	{
+		// stopping condition, recalculate QP,pQP for numerical accuracy
+		pQp=0;
+		for (t=0;t<k;t++)
+		{
+			Qp[t]=0;
+			for (j=0;j<k;j++)
+				Qp[t]+=Q[t][j]*p[j];
+			pQp+=p[t]*Qp[t];
+		}
+		double max_error=0;
+		for (t=0;t<k;t++)
+		{
+			double error=fabs(Qp[t]-pQp);
+			if (error>max_error)
+				max_error=error;
+		}
+		if (max_error<eps) break;
+
+		for (t=0;t<k;t++)
+		{
+			double diff=(-Qp[t]+pQp)/Q[t][t];
+			p[t]+=diff;
+			pQp=(pQp+diff*(diff*Q[t][t]+2*Qp[t]))/(1+diff)/(1+diff);
+			for (j=0;j<k;j++)
+			{
+				Qp[j]=(Qp[j]+diff*Q[t][j])/(1+diff);
+				p[j]/=(1+diff);
+			}
+		}
+	}
+	if (iter>=max_iter)
+		info("Exceeds max_iter in multiclass_prob\n");
+	for(t=0;t<k;t++) free(Q[t]);
+	free(Q);
+	free(Qp);
+}
+
+// Cross-validation decision values for probability estimates
+static void svm_binary_svc_probability(
+	const svm_problem *prob, const svm_parameter *param,
+	double Cp, double Cn, double& probA, double& probB)
+{
+	int i;
+	int nr_fold = 5;
+	int *perm = Malloc(int,prob->l);
+	double *dec_values = Malloc(double,prob->l);
+
+	// random shuffle
+	for(i=0;i<prob->l;i++) perm[i]=i;
+	for(i=0;i<prob->l;i++)
+	{
+		int j = i+rand()%(prob->l-i);
+		swap(perm[i],perm[j]);
+	}
+	for(i=0;i<nr_fold;i++)
+	{
+		int begin = i*prob->l/nr_fold;
+		int end = (i+1)*prob->l/nr_fold;
+		int j,k;
+		struct svm_problem subprob;
+
+		subprob.l = prob->l-(end-begin);
+#ifdef _STRING
+		subprob.x = Malloc(struct svm_data,subprob.l);
+#else
+		subprob.x = Malloc(struct svm_node*,subprob.l);
+#endif
+		subprob.y = Malloc(double,subprob.l);
+
+		k=0;
+		for(j=0;j<begin;j++)
+		{
+			subprob.x[k] = prob->x[perm[j]];
+			subprob.y[k] = prob->y[perm[j]];
+			++k;
+		}
+		for(j=end;j<prob->l;j++)
+		{
+			subprob.x[k] = prob->x[perm[j]];
+			subprob.y[k] = prob->y[perm[j]];
+			++k;
+		}
+		int p_count=0,n_count=0;
+		for(j=0;j<k;j++)
+			if(subprob.y[j]>0)
+				p_count++;
+			else
+				n_count++;
+
+		if(p_count==0 && n_count==0)
+			for(j=begin;j<end;j++)
+				dec_values[perm[j]] = 0;
+		else if(p_count > 0 && n_count == 0)
+			for(j=begin;j<end;j++)
+				dec_values[perm[j]] = 1;
+		else if(p_count == 0 && n_count > 0)
+			for(j=begin;j<end;j++)
+				dec_values[perm[j]] = -1;
+		else
+		{
+			svm_parameter subparam = *param;
+			subparam.probability=0;
+			subparam.C=1.0;
+			subparam.nr_weight=2;
+			subparam.weight_label = Malloc(int,2);
+			subparam.weight = Malloc(double,2);
+			subparam.weight_label[0]=+1;
+			subparam.weight_label[1]=-1;
+			subparam.weight[0]=Cp;
+			subparam.weight[1]=Cn;
+			struct svm_model *submodel = svm_train(&subprob,&subparam);
+			for(j=begin;j<end;j++)
+			{
+				svm_predict_values(submodel,prob->x[perm[j]],&(dec_values[perm[j]]));
+				// ensure +1 -1 order; reason not using CV subroutine
+				dec_values[perm[j]] *= submodel->label[0];
+			}
+			svm_free_and_destroy_model(&submodel);
+			svm_destroy_param(&subparam);
+		}
+		free(subprob.x);
+		free(subprob.y);
+	}
+	sigmoid_train(prob->l,dec_values,prob->y,probA,probB);
+	free(dec_values);
+	free(perm);
+}
+
+// Return parameter of a Laplace distribution
+static double svm_svr_probability(
+	const svm_problem *prob, const svm_parameter *param)
+{
+	int i;
+	int nr_fold = 5;
+	double *ymv = Malloc(double,prob->l);
+	double mae = 0;
+
+	svm_parameter newparam = *param;
+	newparam.probability = 0;
+	svm_cross_validation(prob,&newparam,nr_fold,ymv);
+	for(i=0;i<prob->l;i++)
+	{
+		ymv[i]=prob->y[i]-ymv[i];
+		mae += fabs(ymv[i]);
+	}
+	mae /= prob->l;
+	double std=sqrt(2*mae*mae);
+	int count=0;
+	mae=0;
+	for(i=0;i<prob->l;i++)
+		if (fabs(ymv[i]) > 5*std)
+			count=count+1;
+		else
+			mae+=fabs(ymv[i]);
+	mae /= (prob->l-count);
+	info("Prob. model for test data: target value = predicted value + z,\nz: Laplace distribution e^(-|z|/sigma)/(2sigma),sigma= %g\n",mae);
+	free(ymv);
+	return mae;
+}
+
+
+// label: label name, start: begin of each class, count: #data of classes, perm: indices to the original data
+// perm, length l, must be allocated before calling this subroutine
+static void svm_group_classes(const svm_problem *prob, int *nr_class_ret, int **label_ret, int **start_ret, int **count_ret, int *perm)
+{
+	int l = prob->l;
+	int max_nr_class = 16;
+	int nr_class = 0;
+	int *label = Malloc(int,max_nr_class);
+	int *count = Malloc(int,max_nr_class);
+	int *data_label = Malloc(int,l);
+	int i;
+
+	for(i=0;i<l;i++)
+	{
+		int this_label = (int)prob->y[i];
+		int j;
+		for(j=0;j<nr_class;j++)
+		{
+			if(this_label == label[j])
+			{
+				++count[j];
+				break;
+			}
+		}
+		data_label[i] = j;
+		if(j == nr_class)
+		{
+			if(nr_class == max_nr_class)
+			{
+				max_nr_class *= 2;
+				label = (int *)realloc(label,max_nr_class*sizeof(int));
+				count = (int *)realloc(count,max_nr_class*sizeof(int));
+			}
+			label[nr_class] = this_label;
+			count[nr_class] = 1;
+			++nr_class;
+		}
+	}
+
+	//
+	// Labels are ordered by their first occurrence in the training set.
+	// However, for two-class sets with -1/+1 labels and -1 appears first,
+	// we swap labels to ensure that internally the binary SVM has positive data corresponding to the +1 instances.
+	//
+	if (nr_class == 2 && label[0] == -1 && label[1] == 1)
+	{
+		swap(label[0],label[1]);
+		swap(count[0],count[1]);
+		for(i=0;i<l;i++)
+		{
+			if(data_label[i] == 0)
+				data_label[i] = 1;
+			else
+				data_label[i] = 0;
+		}
+	}
+
+	int *start = Malloc(int,nr_class);
+	start[0] = 0;
+	for(i=1;i<nr_class;i++)
+		start[i] = start[i-1]+count[i-1];
+	for(i=0;i<l;i++)
+	{
+		perm[start[data_label[i]]] = i;
+		++start[data_label[i]];
+	}
+	start[0] = 0;
+	for(i=1;i<nr_class;i++)
+		start[i] = start[i-1]+count[i-1];
+
+	*nr_class_ret = nr_class;
+	*label_ret = label;
+	*start_ret = start;
+	*count_ret = count;
+	free(data_label);
+}
+
+//
+// Interface functions
+//
+svm_model *svm_train(const svm_problem *prob, const svm_parameter *param)
+{
+	svm_model *model = Malloc(svm_model,1);
+	model->param = *param;
+	model->free_sv = 0;	// XXX
+
+	if(param->svm_type == ONE_CLASS ||
+	   param->svm_type == EPSILON_SVR ||
+	   param->svm_type == NU_SVR)
+	{
+		// regression or one-class-svm
+		model->nr_class = 2;
+		model->label = NULL;
+		model->nSV = NULL;
+		model->probA = NULL; model->probB = NULL;
+		model->sv_coef = Malloc(double *,1);
+
+		if(param->probability &&
+		   (param->svm_type == EPSILON_SVR ||
+		    param->svm_type == NU_SVR))
+		{
+			model->probA = Malloc(double,1);
+			model->probA[0] = svm_svr_probability(prob,param);
+		}
+
+		decision_function f = svm_train_one(prob,param,0,0);
+		model->rho = Malloc(double,1);
+		model->rho[0] = f.rho;
+
+		int nSV = 0;
+		int i;
+		for(i=0;i<prob->l;i++)
+			if(fabs(f.alpha[i]) > 0) ++nSV;
+		model->l = nSV;
+#ifdef _STRING
+		model->SV = Malloc(svm_data,nSV);
+#else
+		model->SV = Malloc(svm_node *,nSV);
+#endif
+		model->sv_coef[0] = Malloc(double,nSV);
+		model->sv_indices = Malloc(int,nSV);
+		int j = 0;
+		for(i=0;i<prob->l;i++)
+			if(fabs(f.alpha[i]) > 0)
+			{
+				model->SV[j] = prob->x[i];
+				model->sv_coef[0][j] = f.alpha[i];
+				model->sv_indices[j] = i+1;
+				++j;
+			}
+
+		free(f.alpha);
+	}
+	else
+	{
+		// classification
+		int l = prob->l;
+		int nr_class;
+		int *label = NULL;
+		int *start = NULL;
+		int *count = NULL;
+		int *perm = Malloc(int,l);
+
+		// group training data of the same class
+		svm_group_classes(prob,&nr_class,&label,&start,&count,perm);
+		if(nr_class == 1)
+			info("WARNING: training data in only one class. See README for details.\n");
+
+#ifdef _STRING
+		svm_data *x = Malloc(svm_data,l);
+#else
+		svm_node **x = Malloc(svm_node *,l);
+#endif
+		int i;
+		for(i=0;i<l;i++)
+			x[i] = prob->x[perm[i]];
+
+		// calculate weighted C
+
+		double *weighted_C = Malloc(double, nr_class);
+		for(i=0;i<nr_class;i++)
+			weighted_C[i] = param->C;
+		for(i=0;i<param->nr_weight;i++)
+		{
+			int j;
+			for(j=0;j<nr_class;j++)
+				if(param->weight_label[i] == label[j])
+					break;
+			if(j == nr_class)
+				fprintf(stderr,"WARNING: class label %d specified in weight is not found\n", param->weight_label[i]);
+			else
+				weighted_C[j] *= param->weight[i];
+		}
+
+		// train k*(k-1)/2 models
+
+		bool *nonzero = Malloc(bool,l);
+		for(i=0;i<l;i++)
+			nonzero[i] = false;
+		decision_function *f = Malloc(decision_function,nr_class*(nr_class-1)/2);
+
+		double *probA=NULL,*probB=NULL;
+		if (param->probability)
+		{
+			probA=Malloc(double,nr_class*(nr_class-1)/2);
+			probB=Malloc(double,nr_class*(nr_class-1)/2);
+		}
+
+		int p = 0;
+		for(i=0;i<nr_class;i++)
+			for(int j=i+1;j<nr_class;j++)
+			{
+				svm_problem sub_prob;
+				int si = start[i], sj = start[j];
+				int ci = count[i], cj = count[j];
+				sub_prob.l = ci+cj;
+#ifdef _STRING
+				sub_prob.x = Malloc(svm_data,sub_prob.l);
+#else
+				sub_prob.x = Malloc(svm_node *,sub_prob.l);
+#endif
+				sub_prob.y = Malloc(double,sub_prob.l);
+				int k;
+				for(k=0;k<ci;k++)
+				{
+					sub_prob.x[k] = x[si+k];
+					sub_prob.y[k] = +1;
+				}
+				for(k=0;k<cj;k++)
+				{
+					sub_prob.x[ci+k] = x[sj+k];
+					sub_prob.y[ci+k] = -1;
+				}
+
+				if(param->probability)
+					svm_binary_svc_probability(&sub_prob,param,weighted_C[i],weighted_C[j],probA[p],probB[p]);
+
+				f[p] = svm_train_one(&sub_prob,param,weighted_C[i],weighted_C[j]);
+				for(k=0;k<ci;k++)
+					if(!nonzero[si+k] && fabs(f[p].alpha[k]) > 0)
+						nonzero[si+k] = true;
+				for(k=0;k<cj;k++)
+					if(!nonzero[sj+k] && fabs(f[p].alpha[ci+k]) > 0)
+						nonzero[sj+k] = true;
+				free(sub_prob.x);
+				free(sub_prob.y);
+				++p;
+			}
+
+		// build output
+
+		model->nr_class = nr_class;
+
+		model->label = Malloc(int,nr_class);
+		for(i=0;i<nr_class;i++)
+			model->label[i] = label[i];
+
+		model->rho = Malloc(double,nr_class*(nr_class-1)/2);
+		for(i=0;i<nr_class*(nr_class-1)/2;i++)
+			model->rho[i] = f[i].rho;
+
+		if(param->probability)
+		{
+			model->probA = Malloc(double,nr_class*(nr_class-1)/2);
+			model->probB = Malloc(double,nr_class*(nr_class-1)/2);
+			for(i=0;i<nr_class*(nr_class-1)/2;i++)
+			{
+				model->probA[i] = probA[i];
+				model->probB[i] = probB[i];
+			}
+		}
+		else
+		{
+			model->probA=NULL;
+			model->probB=NULL;
+		}
+
+		int total_sv = 0;
+		int *nz_count = Malloc(int,nr_class);
+		model->nSV = Malloc(int,nr_class);
+		for(i=0;i<nr_class;i++)
+		{
+			int nSV = 0;
+			for(int j=0;j<count[i];j++)
+				if(nonzero[start[i]+j])
+				{
+					++nSV;
+					++total_sv;
+				}
+			model->nSV[i] = nSV;
+			nz_count[i] = nSV;
+		}
+
+		info("Total nSV = %d\n",total_sv);
+
+		model->l = total_sv;
+#ifdef _STRING
+		model->SV = Malloc(svm_data,total_sv);
+#else
+		model->SV = Malloc(svm_node *,total_sv);
+#endif
+		model->sv_indices = Malloc(int,total_sv);
+		p = 0;
+		for(i=0;i<l;i++)
+			if(nonzero[i])
+			{
+				model->SV[p] = x[i];
+				model->sv_indices[p++] = perm[i] + 1;
+			}
+
+		int *nz_start = Malloc(int,nr_class);
+		nz_start[0] = 0;
+		for(i=1;i<nr_class;i++)
+			nz_start[i] = nz_start[i-1]+nz_count[i-1];
+
+		model->sv_coef = Malloc(double *,nr_class-1);
+		for(i=0;i<nr_class-1;i++)
+			model->sv_coef[i] = Malloc(double,total_sv);
+
+		p = 0;
+		for(i=0;i<nr_class;i++)
+			for(int j=i+1;j<nr_class;j++)
+			{
+				// classifier (i,j): coefficients with
+				// i are in sv_coef[j-1][nz_start[i]...],
+				// j are in sv_coef[i][nz_start[j]...]
+
+				int si = start[i];
+				int sj = start[j];
+				int ci = count[i];
+				int cj = count[j];
+
+				int q = nz_start[i];
+				int k;
+				for(k=0;k<ci;k++)
+					if(nonzero[si+k])
+						model->sv_coef[j-1][q++] = f[p].alpha[k];
+				q = nz_start[j];
+				for(k=0;k<cj;k++)
+					if(nonzero[sj+k])
+						model->sv_coef[i][q++] = f[p].alpha[ci+k];
+				++p;
+			}
+
+		free(label);
+		free(probA);
+		free(probB);
+		free(count);
+		free(perm);
+		free(start);
+		free(x);
+		free(weighted_C);
+		free(nonzero);
+		for(i=0;i<nr_class*(nr_class-1)/2;i++)
+			free(f[i].alpha);
+		free(f);
+		free(nz_count);
+		free(nz_start);
+	}
+	return model;
+}
+
+// Stratified cross validation
+void svm_cross_validation(const svm_problem *prob, const svm_parameter *param, int nr_fold, double *target)
+{
+	int i;
+	int *fold_start;
+	int l = prob->l;
+	int *perm = Malloc(int,l);
+	int nr_class;
+	if (nr_fold > l)
+	{
+		nr_fold = l;
+		fprintf(stderr,"WARNING: # folds > # data. Will use # folds = # data instead (i.e., leave-one-out cross validation)\n");
+	}
+	fold_start = Malloc(int,nr_fold+1);
+	// stratified cv may not give leave-one-out rate
+	// Each class to l folds -> some folds may have zero elements
+	if((param->svm_type == C_SVC ||
+	    param->svm_type == NU_SVC) && nr_fold < l)
+	{
+		int *start = NULL;
+		int *label = NULL;
+		int *count = NULL;
+		svm_group_classes(prob,&nr_class,&label,&start,&count,perm);
+
+		// random shuffle and then data grouped by fold using the array perm
+		int *fold_count = Malloc(int,nr_fold);
+		int c;
+		int *index = Malloc(int,l);
+		for(i=0;i<l;i++)
+			index[i]=perm[i];
+		for (c=0; c<nr_class; c++)
+			for(i=0;i<count[c];i++)
+			{
+				int j = i+rand()%(count[c]-i);
+				swap(index[start[c]+j],index[start[c]+i]);
+			}
+		for(i=0;i<nr_fold;i++)
+		{
+			fold_count[i] = 0;
+			for (c=0; c<nr_class;c++)
+				fold_count[i]+=(i+1)*count[c]/nr_fold-i*count[c]/nr_fold;
+		}
+		fold_start[0]=0;
+		for (i=1;i<=nr_fold;i++)
+			fold_start[i] = fold_start[i-1]+fold_count[i-1];
+		for (c=0; c<nr_class;c++)
+			for(i=0;i<nr_fold;i++)
+			{
+				int begin = start[c]+i*count[c]/nr_fold;
+				int end = start[c]+(i+1)*count[c]/nr_fold;
+				for(int j=begin;j<end;j++)
+				{
+					perm[fold_start[i]] = index[j];
+					fold_start[i]++;
+				}
+			}
+		fold_start[0]=0;
+		for (i=1;i<=nr_fold;i++)
+			fold_start[i] = fold_start[i-1]+fold_count[i-1];
+		free(start);
+		free(label);
+		free(count);
+		free(index);
+		free(fold_count);
+	}
+	else
+	{
+		for(i=0;i<l;i++) perm[i]=i;
+		for(i=0;i<l;i++)
+		{
+			int j = i+rand()%(l-i);
+			swap(perm[i],perm[j]);
+		}
+		for(i=0;i<=nr_fold;i++)
+			fold_start[i]=i*l/nr_fold;
+	}
+
+	for(i=0;i<nr_fold;i++)
+	{
+		int begin = fold_start[i];
+		int end = fold_start[i+1];
+		int j,k;
+		struct svm_problem subprob;
+
+		subprob.l = l-(end-begin);
+#ifdef _STRING
+		subprob.x = Malloc(struct svm_data,subprob.l);
+#else
+		subprob.x = Malloc(struct svm_node*,subprob.l);
+#endif
+		subprob.y = Malloc(double,subprob.l);
+
+		k=0;
+		for(j=0;j<begin;j++)
+		{
+			subprob.x[k] = prob->x[perm[j]];
+			subprob.y[k] = prob->y[perm[j]];
+			++k;
+		}
+		for(j=end;j<l;j++)
+		{
+			subprob.x[k] = prob->x[perm[j]];
+			subprob.y[k] = prob->y[perm[j]];
+			++k;
+		}
+		struct svm_model *submodel = svm_train(&subprob,param);
+		if(param->probability &&
+		   (param->svm_type == C_SVC || param->svm_type == NU_SVC))
+		{
+			double *prob_estimates=Malloc(double,svm_get_nr_class(submodel));
+			for(j=begin;j<end;j++)
+				target[perm[j]] = svm_predict_probability(submodel,prob->x[perm[j]],prob_estimates);
+			free(prob_estimates);
+		}
+		else
+			for(j=begin;j<end;j++)
+				target[perm[j]] = svm_predict(submodel,prob->x[perm[j]]);
+		svm_free_and_destroy_model(&submodel);
+		free(subprob.x);
+		free(subprob.y);
+	}
+	free(fold_start);
+	free(perm);
+}
+
+
+int svm_get_svm_type(const svm_model *model)
+{
+	return model->param.svm_type;
+}
+#ifdef _STRING
+int svm_get_data_type(const svm_model *model)
+{
+	return model->param.data_type;
+}
+#endif
+int svm_get_nr_class(const svm_model *model)
+{
+	return model->nr_class;
+}
+
+void svm_get_labels(const svm_model *model, int* label)
+{
+	if (model->label != NULL)
+		for(int i=0;i<model->nr_class;i++)
+			label[i] = model->label[i];
+}
+
+void svm_get_sv_indices(const svm_model *model, int* indices)
+{
+	if (model->sv_indices != NULL)
+		for(int i=0;i<model->l;i++)
+			indices[i] = model->sv_indices[i];
+}
+
+int svm_get_nr_sv(const svm_model *model)
+{
+	return model->l;
+}
+
+double svm_get_svr_probability(const svm_model *model)
+{
+	if ((model->param.svm_type == EPSILON_SVR || model->param.svm_type == NU_SVR) &&
+	    model->probA!=NULL)
+		return model->probA[0];
+	else
+	{
+		fprintf(stderr,"Model doesn't contain information for SVR probability inference\n");
+		return 0;
+	}
+}
+
+#ifdef _STRING
+double svm_predict_values(const svm_model *model, const svm_data x, double* dec_values)
+#else
+double svm_predict_values(const svm_model *model, const svm_node *x, double* dec_values)
+#endif
+{
+	int i;
+	if(model->param.svm_type == ONE_CLASS ||
+	   model->param.svm_type == EPSILON_SVR ||
+	   model->param.svm_type == NU_SVR)
+	{
+		double *sv_coef = model->sv_coef[0];
+		double sum = 0;
+		for(i=0;i<model->l;i++)
+			sum += sv_coef[i] * Kernel::k_function(x,model->SV[i],model->param);
+		sum -= model->rho[0];
+		*dec_values = sum;
+
+		if(model->param.svm_type == ONE_CLASS)
+			return (sum>0)?1:-1;
+		else
+			return sum;
+	}
+	else
+	{
+		int nr_class = model->nr_class;
+		int l = model->l;
+
+		double *kvalue = Malloc(double,l);
+		for(i=0;i<l;i++)
+			kvalue[i] = Kernel::k_function(x,model->SV[i],model->param);
+
+		int *start = Malloc(int,nr_class);
+		start[0] = 0;
+		for(i=1;i<nr_class;i++)
+			start[i] = start[i-1]+model->nSV[i-1];
+
+		int *vote = Malloc(int,nr_class);
+		for(i=0;i<nr_class;i++)
+			vote[i] = 0;
+
+		int p=0;
+		for(i=0;i<nr_class;i++)
+			for(int j=i+1;j<nr_class;j++)
+			{
+				double sum = 0;
+				int si = start[i];
+				int sj = start[j];
+				int ci = model->nSV[i];
+				int cj = model->nSV[j];
+
+				int k;
+				double *coef1 = model->sv_coef[j-1];
+				double *coef2 = model->sv_coef[i];
+				for(k=0;k<ci;k++)
+					sum += coef1[si+k] * kvalue[si+k];
+				for(k=0;k<cj;k++)
+					sum += coef2[sj+k] * kvalue[sj+k];
+				sum -= model->rho[p];
+				dec_values[p] = sum;
+
+				if(dec_values[p] > 0)
+					++vote[i];
+				else
+					++vote[j];
+				p++;
+			}
+
+		int vote_max_idx = 0;
+		for(i=1;i<nr_class;i++)
+			if(vote[i] > vote[vote_max_idx])
+				vote_max_idx = i;
+
+		free(kvalue);
+		free(start);
+		free(vote);
+		return model->label[vote_max_idx];
+	}
+}
+
+#ifdef _STRING
+double svm_predict(const svm_model *model, const svm_data x)
+#else
+double svm_predict(const svm_model *model, const svm_node *x)
+#endif
+{
+	int nr_class = model->nr_class;
+	double *dec_values;
+	if(model->param.svm_type == ONE_CLASS ||
+	   model->param.svm_type == EPSILON_SVR ||
+	   model->param.svm_type == NU_SVR)
+		dec_values = Malloc(double, 1);
+	else
+		dec_values = Malloc(double, nr_class*(nr_class-1)/2);
+	double pred_result = svm_predict_values(model, x, dec_values);
+	free(dec_values);
+	return pred_result;
+}
+
+#ifdef _STRING
+double svm_predict_probability(
+	const svm_model *model, const svm_data x, double *prob_estimates)
+#else
+double svm_predict_probability(
+	const svm_model *model, const svm_node *x, double *prob_estimates)
+#endif
+{
+	if ((model->param.svm_type == C_SVC || model->param.svm_type == NU_SVC) &&
+	    model->probA!=NULL && model->probB!=NULL)
+	{
+		int i;
+		int nr_class = model->nr_class;
+		double *dec_values = Malloc(double, nr_class*(nr_class-1)/2);
+		svm_predict_values(model, x, dec_values);
+
+		double min_prob=1e-7;
+		double **pairwise_prob=Malloc(double *,nr_class);
+		for(i=0;i<nr_class;i++)
+			pairwise_prob[i]=Malloc(double,nr_class);
+		int k=0;
+		for(i=0;i<nr_class;i++)
+			for(int j=i+1;j<nr_class;j++)
+			{
+				pairwise_prob[i][j]=min(max(sigmoid_predict(dec_values[k],model->probA[k],model->probB[k]),min_prob),1-min_prob);
+				pairwise_prob[j][i]=1-pairwise_prob[i][j];
+				k++;
+			}
+		if (nr_class == 2)
+		{
+			prob_estimates[0] = pairwise_prob[0][1];
+			prob_estimates[1] = pairwise_prob[1][0];
+		}
+		else
+			multiclass_probability(nr_class,pairwise_prob,prob_estimates);
+
+		int prob_max_idx = 0;
+		for(i=1;i<nr_class;i++)
+			if(prob_estimates[i] > prob_estimates[prob_max_idx])
+				prob_max_idx = i;
+		for(i=0;i<nr_class;i++)
+			free(pairwise_prob[i]);
+		free(dec_values);
+		free(pairwise_prob);
+		return model->label[prob_max_idx];
+	}
+	else
+		return svm_predict(model, x);
+}
+
+static const char *svm_type_table[] =
+{
+	"c_svc","nu_svc","one_class","epsilon_svr","nu_svr",NULL
+};
+
+#ifdef _STRING
+static const char *data_type_table[] =
+{
+	"vector","string",NULL
+};
+#endif
+
+#ifdef _STRING
+static const char *kernel_type_table[]=
+{
+	"linear","polynomial","rbf","sigmoid","precomputed","edit",NULL
+};
+#else
+static const char *kernel_type_table[]=
+{
+	"linear","polynomial","rbf","sigmoid","precomputed",NULL
+};
+#endif
+
+int svm_save_model(const char *model_file_name, const svm_model *model)
+{
+	FILE *fp = fopen(model_file_name,"w");
+	if(fp==NULL) return -1;
+
+	char *old_locale = setlocale(LC_ALL, NULL);
+	if (old_locale) {
+		old_locale = strdup(old_locale);
+	}
+	setlocale(LC_ALL, "C");
+
+	const svm_parameter& param = model->param;
+
+	fprintf(fp,"svm_type %s\n", svm_type_table[param.svm_type]);
+#ifdef _STRING
+	fprintf(fp,"data_type %s\n", data_type_table[param.data_type]);
+#endif
+	fprintf(fp,"kernel_type %s\n", kernel_type_table[param.kernel_type]);
+
+	if(param.kernel_type == POLY)
+		fprintf(fp,"degree %d\n", param.degree);
+
+#ifdef _STRING
+	if(param.kernel_type == POLY || param.kernel_type == RBF || param.kernel_type == SIGMOID || param.kernel_type == EDIT)
+		fprintf(fp,"gamma %g\n", param.gamma);
+#else
+	if(param.kernel_type == POLY || param.kernel_type == RBF || param.kernel_type == SIGMOID)
+		fprintf(fp,"gamma %.17g\n", param.gamma);
+#endif
+
+	if(param.kernel_type == POLY || param.kernel_type == SIGMOID)
+		fprintf(fp,"coef0 %.17g\n", param.coef0);
+
+	int nr_class = model->nr_class;
+	int l = model->l;
+	fprintf(fp, "nr_class %d\n", nr_class);
+	fprintf(fp, "total_sv %d\n",l);
+
+	{
+		fprintf(fp, "rho");
+		for(int i=0;i<nr_class*(nr_class-1)/2;i++)
+			fprintf(fp," %.17g",model->rho[i]);
+		fprintf(fp, "\n");
+	}
+
+	if(model->label)
+	{
+		fprintf(fp, "label");
+		for(int i=0;i<nr_class;i++)
+			fprintf(fp," %d",model->label[i]);
+		fprintf(fp, "\n");
+	}
+
+	if(model->probA) // regression has probA only
+	{
+		fprintf(fp, "probA");
+		for(int i=0;i<nr_class*(nr_class-1)/2;i++)
+			fprintf(fp," %.17g",model->probA[i]);
+		fprintf(fp, "\n");
+	}
+	if(model->probB)
+	{
+		fprintf(fp, "probB");
+		for(int i=0;i<nr_class*(nr_class-1)/2;i++)
+			fprintf(fp," %.17g",model->probB[i]);
+		fprintf(fp, "\n");
+	}
+
+	if(model->nSV)
+	{
+		fprintf(fp, "nr_sv");
+		for(int i=0;i<nr_class;i++)
+			fprintf(fp," %d",model->nSV[i]);
+		fprintf(fp, "\n");
+	}
+
+	fprintf(fp, "SV\n");
+	const double * const *sv_coef = model->sv_coef;
+#ifdef _STRING
+	const svm_data *SV = model->SV;
+#else
+	const svm_node * const *SV = model->SV;
+#endif
+
+	for(int i=0;i<l;i++)
+	{
+		for(int j=0;j<nr_class-1;j++)
+			fprintf(fp, "%.17g ",sv_coef[j][i]);
+
+#ifdef _STRING
+		if (param.data_type==STRING)
+			fprintf(fp, "%s\n", SV[i].s);
+		else
+		{
+			const svm_node *p = SV[i].v;
+#else
+		const svm_node *p = SV[i];
+#endif
+		if(param.kernel_type == PRECOMPUTED)
+			fprintf(fp,"0:%d ",(int)(p->value));
+		else
+			while(p->index != -1)
+			{
+				fprintf(fp,"%d:%.8g ",p->index,p->value);
+				p++;
+			}
+		fprintf(fp, "\n");
+#ifdef _STRING
+		}
+#endif
+	}
+
+	setlocale(LC_ALL, old_locale);
+	free(old_locale);
+
+	if (ferror(fp) != 0 || fclose(fp) != 0) return -1;
+	else return 0;
+}
+
+static char *line = NULL;
+static int max_line_len;
+
+static char* readline(FILE *input)
+{
+	int len;
+
+	if(fgets(line,max_line_len,input) == NULL)
+		return NULL;
+
+	while(strrchr(line,'\n') == NULL)
+	{
+		max_line_len *= 2;
+		line = (char *) realloc(line,max_line_len);
+		len = (int) strlen(line);
+		if(fgets(line+len,max_line_len-len,input) == NULL)
+			break;
+	}
+	return line;
+}
+
+//
+// FSCANF helps to handle fscanf failures.
+// Its do-while block avoids the ambiguity when
+// if (...)
+//    FSCANF();
+// is used
+//
+#define FSCANF(_stream, _format, _var) do{ if (fscanf(_stream, _format, _var) != 1) return false; }while(0)
+bool read_model_header(FILE *fp, svm_model* model)
+{
+	svm_parameter& param = model->param;
+	// parameters for training only won't be assigned, but arrays are assigned as NULL for safety
+	param.nr_weight = 0;
+	param.weight_label = NULL;
+	param.weight = NULL;
+
+	char cmd[81];
+	while(1)
+	{
+		FSCANF(fp,"%80s",cmd);
+
+		if(strcmp(cmd,"svm_type")==0)
+		{
+			FSCANF(fp,"%80s",cmd);
+			int i;
+			for(i=0;svm_type_table[i];i++)
+			{
+				if(strcmp(svm_type_table[i],cmd)==0)
+				{
+					param.svm_type=i;
+					break;
+				}
+			}
+			if(svm_type_table[i] == NULL)
+			{
+				fprintf(stderr,"unknown svm type.\n");
+				return false;
+			}
+		}
+#ifdef _STRING
+		else if(strcmp(cmd,"data_type")==0)
+		{		
+			FSCANF(fp,"%80s",cmd);
+			int i;
+			for(i=0;data_type_table[i];i++)
+			{
+				if(strcmp(data_type_table[i],cmd)==0)
+				{
+					param.data_type=i;
+					break;
+				}
+			}
+			if(data_type_table[i] == NULL)
+			{
+				fprintf(stderr,"unknown data type.\n");
+				free(model->rho);
+				free(model->label);
+				free(model->nSV);
+				free(model);
+				return NULL;
+			}
+		}
+#endif
+		else if(strcmp(cmd,"kernel_type")==0)
+		{
+			FSCANF(fp,"%80s",cmd);
+			int i;
+			for(i=0;kernel_type_table[i];i++)
+			{
+				if(strcmp(kernel_type_table[i],cmd)==0)
+				{
+					param.kernel_type=i;
+					break;
+				}
+			}
+			if(kernel_type_table[i] == NULL)
+			{
+				fprintf(stderr,"unknown kernel function.\n");
+				return false;
+			}
+		}
+		else if(strcmp(cmd,"degree")==0)
+			FSCANF(fp,"%d",&param.degree);
+		else if(strcmp(cmd,"gamma")==0)
+			FSCANF(fp,"%lf",&param.gamma);
+		else if(strcmp(cmd,"coef0")==0)
+			FSCANF(fp,"%lf",&param.coef0);
+		else if(strcmp(cmd,"nr_class")==0)
+			FSCANF(fp,"%d",&model->nr_class);
+		else if(strcmp(cmd,"total_sv")==0)
+			FSCANF(fp,"%d",&model->l);
+		else if(strcmp(cmd,"rho")==0)
+		{
+			int n = model->nr_class * (model->nr_class-1)/2;
+			model->rho = Malloc(double,n);
+			for(int i=0;i<n;i++)
+				FSCANF(fp,"%lf",&model->rho[i]);
+		}
+		else if(strcmp(cmd,"label")==0)
+		{
+			int n = model->nr_class;
+			model->label = Malloc(int,n);
+			for(int i=0;i<n;i++)
+				FSCANF(fp,"%d",&model->label[i]);
+		}
+		else if(strcmp(cmd,"probA")==0)
+		{
+			int n = model->nr_class * (model->nr_class-1)/2;
+			model->probA = Malloc(double,n);
+			for(int i=0;i<n;i++)
+				FSCANF(fp,"%lf",&model->probA[i]);
+		}
+		else if(strcmp(cmd,"probB")==0)
+		{
+			int n = model->nr_class * (model->nr_class-1)/2;
+			model->probB = Malloc(double,n);
+			for(int i=0;i<n;i++)
+				FSCANF(fp,"%lf",&model->probB[i]);
+		}
+		else if(strcmp(cmd,"nr_sv")==0)
+		{
+			int n = model->nr_class;
+			model->nSV = Malloc(int,n);
+			for(int i=0;i<n;i++)
+				FSCANF(fp,"%d",&model->nSV[i]);
+		}
+		else if(strcmp(cmd,"SV")==0)
+		{
+			while(1)
+			{
+				int c = getc(fp);
+				if(c==EOF || c=='\n') break;
+			}
+			break;
+		}
+		else
+		{
+			fprintf(stderr,"unknown text in model file: [%s]\n",cmd);
+			return false;
+		}
+	}
+
+	return true;
+
+}
+
+svm_model *svm_load_model(const char *model_file_name)
+{
+	FILE *fp = fopen(model_file_name,"rb");
+	if(fp==NULL) return NULL;
+
+	char *old_locale = setlocale(LC_ALL, NULL);
+	if (old_locale) {
+		old_locale = strdup(old_locale);
+	}
+	setlocale(LC_ALL, "C");
+
+	// read parameters
+
+	svm_model *model = Malloc(svm_model,1);
+	model->rho = NULL;
+	model->probA = NULL;
+	model->probB = NULL;
+	model->sv_indices = NULL;
+	model->label = NULL;
+	model->nSV = NULL;
+
+	// read header
+	if (!read_model_header(fp, model))
+	{
+		fprintf(stderr, "ERROR: fscanf failed to read model\n");
+		setlocale(LC_ALL, old_locale);
+		free(old_locale);
+		free(model->rho);
+		free(model->label);
+		free(model->nSV);
+		free(model);
+		return NULL;
+	}
+
+	// read sv_coef and SV
+
+	int elements = 0;
+	long pos = ftell(fp);
+
+	max_line_len = 1024;
+	line = Malloc(char,max_line_len);
+	char *p,*endptr,*idx,*val;
+
+	while(readline(fp)!=NULL)
+	{
+		p = strtok(line,":");
+		while(1)
+		{
+			p = strtok(NULL,":");
+			if(p == NULL)
+				break;
+			++elements;
+		}
+	}
+	elements += model->l;
+
+	fseek(fp,pos,SEEK_SET);
+
+	int m = model->nr_class - 1;
+	int l = model->l;
+	model->sv_coef = Malloc(double *,m);
+	int i;
+	for(i=0;i<m;i++)
+		model->sv_coef[i] = Malloc(double,l);
+#ifdef _STRING
+	model->SV = Malloc(svm_data,l);
+#else
+	model->SV = Malloc(svm_node*,l);
+#endif
+
+#ifdef _STRING
+	if(model->param.data_type==STRING)
+	{
+		for(i=0;i<l;i++)
+		{
+			readline(fp);
+			p = strtok(line, " \t");
+			model->sv_coef[0][i] = strtod(p,&endptr);
+			for(int k=1;k<m;k++)
+			{
+				p = strtok(NULL, " \t");
+				model->sv_coef[k][i] = strtod(p,&endptr);
+			}
+
+			val = strtok(NULL,"\n");
+			model->SV[i].s = Malloc(char, strlen(val)+1);
+			strcpy(model->SV[i].s, val);
+			model->SV[i].v = NULL;
+		}
+	}
+	else
+	{
+#endif
+
+	svm_node *x_space = NULL;
+	if(l>0) x_space = Malloc(svm_node,elements);
+
+	int j=0;
+	for(i=0;i<l;i++)
+	{
+#ifdef _STRING
+		model->SV[i].v = &x_space[j];
+		model->SV[i].s = NULL;
+#else
+		model->SV[i] = &x_space[j];
+#endif
+
+		readline(fp);
+		p = strtok(line, " \t");
+		model->sv_coef[0][i] = strtod(p,&endptr);
+		for(int k=1;k<m;k++)
+		{
+			p = strtok(NULL, " \t");
+			model->sv_coef[k][i] = strtod(p,&endptr);
+		}
+
+		while(1)
+		{
+			idx = strtok(NULL, ":");
+			val = strtok(NULL, " \t");
+
+			if(val == NULL)
+				break;
+			x_space[j].index = (int) strtol(idx,&endptr,10);
+			x_space[j].value = strtod(val,&endptr);
+
+			++j;
+		}
+
+		x_space[j++].index = -1;
+	}
+
+#ifdef _STRING
+	} // end of else
+#endif
+
+	free(line);
+	setlocale(LC_ALL, old_locale);
+	free(old_locale);
+
+	if (ferror(fp) != 0 || fclose(fp) != 0)
+		return NULL;
+
+	model->free_sv = 1;	// XXX
+	return model;
+}
+
+void svm_free_model_content(svm_model* model_ptr)
+{
+#ifdef _STRING
+	if(model_ptr->free_sv && model_ptr->l > 0 && model_ptr->SV != NULL && model_ptr->param.data_type==STRING)
+		for(int i=0;i<model_ptr->l;i++)
+			free((void *)(model_ptr->SV[i].s));
+	if(model_ptr->free_sv && model_ptr->l > 0 && model_ptr->SV != NULL && model_ptr->param.data_type==VECTOR)
+		free((void *)(model_ptr->SV[0].v));
+#else
+	if(model_ptr->free_sv && model_ptr->l > 0 && model_ptr->SV != NULL)
+		free((void *)(model_ptr->SV[0]));
+#endif
+	if(model_ptr->sv_coef)
+	{
+		for(int i=0;i<model_ptr->nr_class-1;i++)
+			free(model_ptr->sv_coef[i]);
+	}
+
+	free(model_ptr->SV);
+	model_ptr->SV = NULL;
+
+	free(model_ptr->sv_coef);
+	model_ptr->sv_coef = NULL;
+
+	free(model_ptr->rho);
+	model_ptr->rho = NULL;
+
+	free(model_ptr->label);
+	model_ptr->label= NULL;
+
+	free(model_ptr->probA);
+	model_ptr->probA = NULL;
+
+	free(model_ptr->probB);
+	model_ptr->probB= NULL;
+
+	free(model_ptr->sv_indices);
+	model_ptr->sv_indices = NULL;
+
+	free(model_ptr->nSV);
+	model_ptr->nSV = NULL;
+}
+
+void svm_free_and_destroy_model(svm_model** model_ptr_ptr)
+{
+	if(model_ptr_ptr != NULL && *model_ptr_ptr != NULL)
+	{
+		svm_free_model_content(*model_ptr_ptr);
+		free(*model_ptr_ptr);
+		*model_ptr_ptr = NULL;
+	}
+}
+
+void svm_destroy_param(svm_parameter* param)
+{
+	free(param->weight_label);
+	free(param->weight);
+}
+
+const char *svm_check_parameter(const svm_problem *prob, const svm_parameter *param)
+{
+	// svm_type
+
+	int svm_type = param->svm_type;
+	if(svm_type != C_SVC &&
+	   svm_type != NU_SVC &&
+	   svm_type != ONE_CLASS &&
+	   svm_type != EPSILON_SVR &&
+	   svm_type != NU_SVR)
+		return "unknown svm type";
+
+	// kernel_type, degree
+
+	int kernel_type = param->kernel_type;
+#ifdef _STRING
+	if(kernel_type != LINEAR &&
+	   kernel_type != POLY &&
+	   kernel_type != RBF &&
+	   kernel_type != SIGMOID &&
+	   kernel_type != PRECOMPUTED &&
+	   kernel_type != EDIT)
+		return "unknown kernel type";
+#else
+	if(kernel_type != LINEAR &&
+	   kernel_type != POLY &&
+	   kernel_type != RBF &&
+	   kernel_type != SIGMOID &&
+	   kernel_type != PRECOMPUTED)
+		return "unknown kernel type";
+#endif
+
+	if((kernel_type == POLY || kernel_type == RBF || kernel_type == SIGMOID) &&
+	   param->gamma < 0)
+		return "gamma < 0";
+
+	if(kernel_type == POLY && param->degree < 0)
+		return "degree of polynomial kernel < 0";
+
+	// cache_size,eps,C,nu,p,shrinking
+
+	if(param->cache_size <= 0)
+		return "cache_size <= 0";
+
+	if(param->eps <= 0)
+		return "eps <= 0";
+
+	if(svm_type == C_SVC ||
+	   svm_type == EPSILON_SVR ||
+	   svm_type == NU_SVR)
+		if(param->C <= 0)
+			return "C <= 0";
+
+	if(svm_type == NU_SVC ||
+	   svm_type == ONE_CLASS ||
+	   svm_type == NU_SVR)
+		if(param->nu <= 0 || param->nu > 1)
+			return "nu <= 0 or nu > 1";
+
+	if(svm_type == EPSILON_SVR)
+		if(param->p < 0)
+			return "p < 0";
+
+	if(param->shrinking != 0 &&
+	   param->shrinking != 1)
+		return "shrinking != 0 and shrinking != 1";
+
+	if(param->probability != 0 &&
+	   param->probability != 1)
+		return "probability != 0 and probability != 1";
+
+	if(param->probability == 1 &&
+	   svm_type == ONE_CLASS)
+		return "one-class SVM probability output not supported yet";
+
+
+	// check whether nu-svc is feasible
+
+	if(svm_type == NU_SVC)
+	{
+		int l = prob->l;
+		int max_nr_class = 16;
+		int nr_class = 0;
+		int *label = Malloc(int,max_nr_class);
+		int *count = Malloc(int,max_nr_class);
+
+		int i;
+		for(i=0;i<l;i++)
+		{
+			int this_label = (int)prob->y[i];
+			int j;
+			for(j=0;j<nr_class;j++)
+				if(this_label == label[j])
+				{
+					++count[j];
+					break;
+				}
+			if(j == nr_class)
+			{
+				if(nr_class == max_nr_class)
+				{
+					max_nr_class *= 2;
+					label = (int *)realloc(label,max_nr_class*sizeof(int));
+					count = (int *)realloc(count,max_nr_class*sizeof(int));
+				}
+				label[nr_class] = this_label;
+				count[nr_class] = 1;
+				++nr_class;
+			}
+		}
+
+		for(i=0;i<nr_class;i++)
+		{
+			int n1 = count[i];
+			for(int j=i+1;j<nr_class;j++)
+			{
+				int n2 = count[j];
+				if(param->nu*(n1+n2)/2 > min(n1,n2))
+				{
+					free(label);
+					free(count);
+					return "specified nu is infeasible";
+				}
+			}
+		}
+		free(label);
+		free(count);
+	}
+
+	return NULL;
+}
+
+int svm_check_probability_model(const svm_model *model)
+{
+	return ((model->param.svm_type == C_SVC || model->param.svm_type == NU_SVC) &&
+		model->probA!=NULL && model->probB!=NULL) ||
+		((model->param.svm_type == EPSILON_SVR || model->param.svm_type == NU_SVR) &&
+		 model->probA!=NULL);
+}
+
+void svm_set_print_string_function(void (*print_func)(const char *))
+{
+	if(print_func == NULL)
+		svm_print_string = &print_string_stdout;
+	else
+		svm_print_string = print_func;
+}
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm.def NeuroMiner-1-main.clara/libsvm-string-3.25/svm.def
--- NeuroMiner-1-main/libsvm-string-3.25/svm.def	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm.def	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,21 @@
+LIBRARY libsvm
+EXPORTS
+	svm_train	@1
+	svm_cross_validation	@2
+	svm_save_model	@3
+	svm_load_model	@4
+	svm_get_svm_type	@5
+	svm_get_nr_class	@6
+	svm_get_labels	@7
+	svm_get_svr_probability	@8
+	svm_predict_values	@9
+	svm_predict	@10
+	svm_predict_probability	@11
+	svm_free_model_content	@12
+	svm_free_and_destroy_model	@13
+	svm_destroy_param	@14
+	svm_check_parameter	@15
+	svm_check_probability_model	@16
+	svm_set_print_string_function	@17
+	svm_get_sv_indices	@18
+	svm_get_nr_sv	@19
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/svm.h NeuroMiner-1-main.clara/libsvm-string-3.25/svm.h
--- NeuroMiner-1-main/libsvm-string-3.25/svm.h	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/svm.h	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,141 @@
+#ifndef _LIBSVM_H
+#define _LIBSVM_H
+
+#define LIBSVM_VERSION 325
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+extern int libsvm_version;
+
+struct svm_node
+{
+	int index;
+	double value;
+};
+
+#ifdef _STRING
+struct svm_data
+{
+	struct svm_node *v;
+	char *s;
+};
+struct svm_problem
+{
+	int l;
+	double *y;
+	struct svm_data *x;
+};
+
+#else
+struct svm_problem
+{
+	int l;
+	double *y;
+	struct svm_node **x;
+};
+#endif
+
+enum { C_SVC, NU_SVC, ONE_CLASS, EPSILON_SVR, NU_SVR };	/* svm_type */
+#ifdef _STRING
+enum { VECTOR, STRING }; /* data_type */
+enum { LINEAR, POLY, RBF, SIGMOID, PRECOMPUTED, EDIT }; /* kernel_type */
+#else
+enum { LINEAR, POLY, RBF, SIGMOID, PRECOMPUTED }; /* kernel_type */
+#endif
+
+struct svm_parameter
+{
+	int svm_type;
+#ifdef _STRING
+	int data_type;
+#endif
+	int kernel_type;
+	int degree;	/* for poly */
+	double gamma;	/* for poly/rbf/sigmoid */
+	double coef0;	/* for poly/sigmoid */
+
+	/* these are for training only */
+	double cache_size; /* in MB */
+	double eps;	/* stopping criteria */
+	double C;	/* for C_SVC, EPSILON_SVR and NU_SVR */
+	int nr_weight;		/* for C_SVC */
+	int *weight_label;	/* for C_SVC */
+	double* weight;		/* for C_SVC */
+	double nu;	/* for NU_SVC, ONE_CLASS, and NU_SVR */
+	double p;	/* for EPSILON_SVR */
+	int shrinking;	/* use the shrinking heuristics */
+	int probability; /* do probability estimates */
+};
+
+//
+// svm_model
+//
+struct svm_model
+{
+	struct svm_parameter param;	/* parameter */
+	int nr_class;		/* number of classes, = 2 in regression/one class svm */
+	int l;			/* total #SV */
+#ifdef _STRING
+	struct svm_data *SV;		/* SVs (SV[l]) */
+#else
+	struct svm_node **SV;		/* SVs (SV[l]) */
+#endif
+	double **sv_coef;	/* coefficients for SVs in decision functions (sv_coef[k-1][l]) */
+	double *rho;		/* constants in decision functions (rho[k*(k-1)/2]) */
+	double *probA;		/* pariwise probability information */
+	double *probB;
+	int *sv_indices;        /* sv_indices[0,...,nSV-1] are values in [1,...,num_traning_data] to indicate SVs in the training set */
+
+	/* for classification only */
+
+	int *label;		/* label of each class (label[k]) */
+	int *nSV;		/* number of SVs for each class (nSV[k]) */
+				/* nSV[0] + nSV[1] + ... + nSV[k-1] = l */
+	/* XXX */
+	int free_sv;		/* 1 if svm_model is created by svm_load_model*/
+				/* 0 if svm_model is created by svm_train */
+};
+
+struct svm_model *svm_train(const struct svm_problem *prob, const struct svm_parameter *param);
+void svm_cross_validation(const struct svm_problem *prob, const struct svm_parameter *param, int nr_fold, double *target);
+
+int svm_save_model(const char *model_file_name, const struct svm_model *model);
+struct svm_model *svm_load_model(const char *model_file_name);
+
+int svm_get_svm_type(const struct svm_model *model);
+#ifdef _STRING
+int svm_get_data_type(const struct svm_model *model);
+#endif
+int svm_get_nr_class(const struct svm_model *model);
+void svm_get_labels(const struct svm_model *model, int *label);
+void svm_get_sv_indices(const struct svm_model *model, int *sv_indices);
+int svm_get_nr_sv(const struct svm_model *model);
+double svm_get_svr_probability(const struct svm_model *model);
+
+#ifdef _STRING
+double svm_predict_values(const struct svm_model *model, const struct svm_data x, double* dec_values);
+double svm_predict(const struct svm_model *model, const struct svm_data x);
+double svm_predict_probability(const struct svm_model *model, const struct svm_data x, double* prob_estimates);
+
+#else
+double svm_predict_values(const struct svm_model *model, const struct svm_node *x, double* dec_values);
+double svm_predict(const struct svm_model *model, const struct svm_node *x);
+double svm_predict_probability(const struct svm_model *model, const struct svm_node *x, double* prob_estimates);
+#endif
+
+void svm_free_model_content(struct svm_model *model_ptr);
+void svm_free_and_destroy_model(struct svm_model **model_ptr_ptr);
+void svm_destroy_param(struct svm_parameter *param);
+
+const char *svm_check_parameter(const struct svm_problem *prob, const struct svm_parameter *param);
+int svm_check_probability_model(const struct svm_model *model);
+
+void svm_set_print_string_function(void (*print_func)(const char *));
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _LIBSVM_H */
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/libsvm-string-3.25/word.data NeuroMiner-1-main.clara/libsvm-string-3.25/word.data
--- NeuroMiner-1-main/libsvm-string-3.25/word.data	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/libsvm-string-3.25/word.data	2021-04-26 09:41:36.000000000 +0200
@@ -0,0 +1,3 @@
+1 abcca
+1 abcac
+2 bcadc
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/nk_ComputeSVMPmap.m NeuroMiner-1-main.clara/nk_ComputeSVMPmap.m
--- NeuroMiner-1-main/nk_ComputeSVMPmap.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/nk_ComputeSVMPmap.m	2021-05-08 09:55:03.850000000 +0200
@@ -1,4 +1,4 @@
-function p_map = nk_ComputeSVMPmap( Y, labels, model)
+function p_map = nk_ComputeSVMPmap(Y, labels, model)
 
 p       = sum(labels==1)/max(size(labels));
 if ~isfield(model,'w')
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/nk_GetParamDescription2.m NeuroMiner-1-main.clara/nk_GetParamDescription2.m
--- NeuroMiner-1-main/nk_GetParamDescription2.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/nk_GetParamDescription2.m	2021-06-26 14:13:41.510000000 +0200
@@ -502,8 +502,19 @@
                                 covsizeextr = sprintf('1 covariate');
                             end
                             preprocact{i} = sprintf('Deviation-based weighting [ %s: %s%s ]', params.ACTPARAM{i}.DEVMAP.algostr, covsizeextr, grpstr);
+                        case 'graphSparsity' % CHANGE SPARSITY
+                
+                            preprocact{i} = 'Apply sparsity threshold to conenctivity matrices [';
+                       
+                            if isfield(params.ACTPARAM{i}.GRAPHSPARSITY,'perc') && ~isempty(params.ACTPARAM{i}.GRAPHSPARSITY.perc)
+                                preprocact{i} = sprintf('%s, Single-Value [ %s ]', preprocact{i}, nk_ConcatParamstr(params.ACTPARAM{i}.GRAPHSPARSITY.perc));
+                            end
+                            preprocact{i} = sprintf('%s ]', preprocact{i});
+                        case 'graphMetrics'
+                            preprocact{i} = 'Compute graph metrics from connectivity matrices';
+                            end
                                 
-                    end
+                   
                 else
                     preprocact{i} = 'undefined';
                 end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/nk_MLOptimizerPrep.m NeuroMiner-1-main.clara/nk_MLOptimizerPrep.m
--- NeuroMiner-1-main/nk_MLOptimizerPrep.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/nk_MLOptimizerPrep.m	2021-05-04 09:33:18.750000000 +0200
@@ -235,7 +235,7 @@
                 case 3 % Running with precomputed CVdatamats (aggregation run --- allowing to tweak some post-training model selection options)
                    
                     [gdmat, emptfl]         = nk_GenCVdataMaster2(NM.id, CV(1), [], fullfile(NM.analysis{inp.analind}.rootdir, algostr), [], [], inp.varind, inp.varstr, inp.concatfl);
-                    if ~emptfl && ~isempty(gdmat), 
+                    if ~emptfl && ~isempty(gdmat)
                         inp.preprocmat      = cell(inp.nF,1);
                         inp.gdmat           = gdmat; 
                         inp.GridAct         = ~cellfun(@isempty,gdmat{1}); 
@@ -243,7 +243,7 @@
                 case 4 % Running with existing CVresults (simple aggregation run --- basically for multiple modalities)
                     
                     [gdanalmat, emptfl]     = nk_GenCVresultsMaster(NM.id,[], fullfile(NM.analysis{inp.analind}.rootdir, algostr)); 
-                    if ~emptfl && ~isempty(gdanalmat), 
+                    if ~emptfl && ~isempty(gdanalmat)
                         inp.preprocmat      = cell(inp.nF,1);
                         inp.gdmat           = [];
                         inp.gdanalmat       = gdanalmat;
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/nk_PrepMLParams.m NeuroMiner-1-main.clara/nk_PrepMLParams.m
--- NeuroMiner-1-main/nk_PrepMLParams.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/nk_PrepMLParams.m	2021-05-19 22:31:47.120000000 +0200
@@ -43,5 +43,24 @@
                  if isfield(SVM.(SVM.prog),'MakeInsensitive') && SVM.(SVM.prog).MakeInsensitive
                      CMDSTR.CCLambda = nk_ReturnParam('CC-Lambda', Params_desc, Params(i,:));
                  end
+                 if SVM.kernel.kerndef == 5
+                     CMDSTR.WLiter = nk_ReturnParam('Kernel', Params_desc, Params(i,:)); 
+                 end
+                 if SVM.kernel.kerndef == 6
+                     CMDSTR.WLiter = nk_ReturnParam('Kernel', Params_desc, Params(i,:)); 
+                 end
+                 if SVM.kernel.kerndef == 7
+                     CMDSTR.WLiter = nk_ReturnParam('Kernel', Params_desc, Params(i,:)); 
+                 end
+                 if SVM.kernel.kerndef == 8
+                     if SVM.kernel.customfunc_nargin >0
+                        for n = 1:SVM.kernel.customfunc_nargin
+                            argName = sprintf('customkernel_arg%d', n); 
+                            eval(sprintf("CMDSTR.%s = nk_ReturnParam('Kernel function argument %d', Params_desc, Params(i,:))", argName, n));%, argName, argName));
+                            %nk_ReturnParam('Kernel function', Params_desc, Params(i,:));
+                        end
+                     end
+                     %CMDSTR.KernelFunc =  
+                 end
         end
 end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/nm.m NeuroMiner-1-main.clara/nm.m
--- NeuroMiner-1-main/nm.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/nm.m	2021-05-31 20:30:40.550000000 +0200
@@ -354,7 +354,7 @@
             NM = nk_UpdateRootPaths(NM, analind, newdir);
     end
 
-catch ERR
+nmcatch ERR
     return
 end
 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/preproc/nk_GenPreprocSequence.m NeuroMiner-1-main.clara/preproc/nk_GenPreprocSequence.m
--- NeuroMiner-1-main/preproc/nk_GenPreprocSequence.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/preproc/nk_GenPreprocSequence.m	2021-06-26 13:02:54.030000000 +0200
@@ -454,6 +454,19 @@
                     InputParam.P{ac}.DEVMAP.Params = PX.Params;
                     InputParam.P{ac}.DEVMAP.Params_desc = PX.Params_desc;
                 end
+            case 'graphSparsity'
+                if VERBOSE, fprintf('\n* APPLY SPARSITY THRESHOLD TO CONNECTIVITY MATRICES'); end
+                InputParam.P{ac} =  TemplParam.ACTPARAM{ac};
+                if isfield(TemplParam.ACTPARAM{ac},'PX') && ~isempty(TemplParam.ACTPARAM{ac}.PX.opt)
+                    InputParam.P{ac}.opt = TemplParam.ACTPARAM{ac}.PX.opt;
+                end
+             case 'graphMetrics'
+                if VERBOSE, fprintf('\n* COMPUTE GRAPH METRICS FROM CONNECTIVITY MATRICES'); end
+                InputParam.P{ac} =  TemplParam.ACTPARAM{ac};
+                if isfield(TemplParam.ACTPARAM{ac},'PX') && ~isempty(TemplParam.ACTPARAM{ac}.PX.opt)
+                    InputParam.P{ac}.opt = TemplParam.ACTPARAM{ac}.PX.opt;
+                end
+                
         end
     end
     if VERBOSE, fprintf('\nPreprocessing sequence setup completed. Executing ...'); end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/preproc/nk_PerfElimZeroObj.m NeuroMiner-1-main.clara/preproc/nk_PerfElimZeroObj.m
--- NeuroMiner-1-main/preproc/nk_PerfElimZeroObj.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/preproc/nk_PerfElimZeroObj.m	2021-06-03 12:02:27.490000000 +0200
@@ -20,7 +20,7 @@
 % =========================================================================
 function [Y, IN] = PerfElimZeroObj(Y, IN)
 
-global VERBOSE
+global VERBOSE SVM
 % Defaults
 if isempty(IN),eIN=true; else eIN=false; end
 
@@ -30,16 +30,20 @@
     if ~isfield(IN,'nan'),  IN.nan = 1;  end
     if ~isfield(IN,'perc'), IN.perc = []; end
     % Identify zero-variance columns
-    if IN.zero == 1,  indNullVar = var(Y)==0; else, indNullVar = false(1,size(Y,2));end
+    if IN.zero == 1,        indNullVar = var(Y)==0;         else, indNullVar = false(1,size(Y,2));end
     % Identify columns containing NaNs
-    if IN.nan == 1,   indNan = any(isnan(Y)); else, indNan = false(1,size(Y,2)); end
+    if IN.nan == 1,         indNan = any(isnan(Y));         else, indNan = false(1,size(Y,2)); end
     % Identify Inf columns
-    if IN.inf == 1,   indInf = any(isinf(Y)); else, indInf = false(1,size(Y,2));end
+    if IN.inf == 1,         indInf = any(isinf(Y));         else, indInf = false(1,size(Y,2));end
+    
     if ~isempty(IN.perc)
         R = nk_CountUniques(Y, IN.perc); indUT = R.UT';
     else
         indUT = false(1,size(Y,2)); 
     end
+    
+
+    
     % Put together
     IN.NonPruneVec = ~(indNullVar | indNan | indInf | indUT);
     if ~isempty(IN.perc)
@@ -51,4 +55,10 @@
     end
     
 end
-Y = Y(:,IN.NonPruneVec);
\ No newline at end of file
+    % prune zero-variance graph edges 
+if SVM.kernel.kerndef >=5
+    %if IN.zero == 1,   indGraphNullVar = var(Y)==0;    else, indGraphNullVar = false(1,size(Y,2));end
+    Y(:,~(IN.NonPruneVec)) = 0;
+else
+    Y = Y(:,IN.NonPruneVec);
+end
\ No newline at end of file
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/preproc/nk_PerfPreprocessObj_core.m NeuroMiner-1-main.clara/preproc/nk_PerfPreprocessObj_core.m
--- NeuroMiner-1-main/preproc/nk_PerfPreprocessObj_core.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/preproc/nk_PerfPreprocessObj_core.m	2021-06-26 16:02:00.740000000 +0200
@@ -796,6 +796,62 @@
 end
 
 % =========================================================================
+function [SrcParam, InputParam, TrParami, actparam ] = act_graphSparsity(SrcParam, InputParam, ~, TrParami, actparam)
+global VERBOSE
+trfl    = actparam.trfl;
+tsfl    = actparam.tsfl;
+paramfl = actparam.paramfl;
+i       = actparam.i;
+tsproc  = false;  
+
+if isfield(actparam,'opt')
+    InputParam.P{i}.GRAPHSPARSITY.p = actparam.opt;
+end
+
+if paramfl && tsfl 
+     tsproc = true;
+elseif trfl
+    if VERBOSE;fprintf('\tGraph sparsity thresholding ...'); end
+    [InputParam.Tr, TrParami] = graph_PerfSparsityThres(InputParam.Tr, InputParam.P{i}.GRAPHSPARSITY);
+    % All subsequent processing steps that use fixed column indices have to
+    % be adjusted to the pruned matrix 
+%     for z=i+1:numel(InputParam.P)
+%         if isfield(InputParam.P{z},'IMPUTE') && ~isempty(InputParam.P{z}.IMPUTE.blockind)
+%             InputParam.P{z}.IMPUTE.blockind = InputParam.P{z}.IMPUTE.blockind(TrParami.NonPruneVec);
+%         end
+%     end
+    if tsfl, tsproc = true; end
+end
+
+if tsproc, InputParam.Ts = graph_PerfSparsityThres(InputParam.Ts, TrParami); end
+end
+
+function [SrcParam, InputParam, TrParami, actparam ] = act_graphMetrics(SrcParam, InputParam, ~, TrParami, actparam)
+global VERBOSE
+trfl    = actparam.trfl;
+tsfl    = actparam.tsfl;
+paramfl = actparam.paramfl;
+i       = actparam.i;
+tsproc  = false;  
+
+if isfield(actparam,'opt')
+    InputParam.P{i}.GRAPHMETRICS.p = actparam.opt;
+end
+
+if paramfl && tsfl 
+     tsproc = true;
+elseif trfl
+    if VERBOSE;fprintf('\tGraph metrics computation ...'); end
+    [InputParam.Tr, TrParami] = graph_PerfGraphMetrics(InputParam.Tr, InputParam.P{i}.GRAPHMETRICS);
+    % All 
+    if tsfl, tsproc = true; end
+end
+
+if tsproc, InputParam.Ts = graph_PerfGraphMetrics(InputParam.Ts, TrParami); end
+end
+
+
+% =========================================================================
 function [InputParam, SrcParam] = perform_adasyn(InputParam, SrcParam)
 global SVM MODEFL
 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/preproc/nk_PreprocessPrep.m NeuroMiner-1-main.clara/preproc/nk_PreprocessPrep.m
--- NeuroMiner-1-main/preproc/nk_PreprocessPrep.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/preproc/nk_PreprocessPrep.m	2021-06-18 09:48:05.310000000 +0200
@@ -193,6 +193,10 @@
                         [TEMPL.Tr{curclass}, TEMPL.Param{curclass}] = nk_GenPreprocSequence(InputParam, PREPROC, SrcParam);
                     end
                 end
+                
+                
+                % check if sparsity threshold has to be applied
+                
 
                 % ======================== PREPROCESSING PIPELINE ========================
                 % These stepps require cross-validation as they require group-level
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/ACCURACY.m NeuroMiner-1-main.clara/trainpredict/ACCURACY.m
--- NeuroMiner-1-main/trainpredict/ACCURACY.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/ACCURACY.m	2021-05-01 14:31:06.330000000 +0200
@@ -6,6 +6,7 @@
 % (c) Nikolaos Koutsouleris, 06/2011
 
 function param = ACCURACY(expected, predicted)
+
 if isempty(expected), param = []; return; end
 ind0 = expected ~=0;
 expected = expected(ind0); 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/FoldPerm.m NeuroMiner-1-main.clara/trainpredict/FoldPerm.m
--- NeuroMiner-1-main/trainpredict/FoldPerm.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/FoldPerm.m	2021-05-04 17:30:24.540000000 +0200
@@ -28,7 +28,7 @@
 
 function [IN, OUT] = FoldPerm(IN, OUT, strout, fRFE, fFull, RetrainImmediate, fKX, LoopParam)
 
-global VERBOSE CV MODEFL RAND MULTILABEL MULTI RFE CVPOS
+global VERBOSE CV MODEFL RAND MULTILABEL MULTI RFE CVPOS Ytrain
 
 RF = []; fMULTI = false; VI = []; 
 if MULTI.flag ==1 && ... 
@@ -154,7 +154,7 @@
             tTrL{curclass}(TrInd) = IN.Y.TrL{i,j}{curclass}(:,MULTILABEL.curdim);
             tCVL{curclass} = zeros(size(tCV{1},1),1); 
             tCVL{curclass}(CVInd) = IN.Y.CVL{i,j}{curclass}(:,MULTILABEL.curdim);
-            if fFull, 
+            if fFull 
                 modelTrL{curclass} = [modelTrL{curclass}; IN.Y.CVL{i,j}{curclass}(:,MULTILABEL.curdim)]; 
             end
             
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/computeWLKernel.m NeuroMiner-1-main.clara/trainpredict/computeWLKernel.m
--- NeuroMiner-1-main/trainpredict/computeWLKernel.m	1970-01-01 01:00:00.000000000 +0100
+++ NeuroMiner-1-main.clara/trainpredict/computeWLKernel.m	2021-05-03 13:40:46.960000000 +0200
@@ -0,0 +1,6 @@
+function [Y] = computeWLKernel(X,Y)
+
+Y = repmat(1,145,145)
+Y
+end
+
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_ConcatLIBSVMParamStr.m NeuroMiner-1-main.clara/trainpredict/nk_ConcatLIBSVMParamStr.m
--- NeuroMiner-1-main/trainpredict/nk_ConcatLIBSVMParamStr.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_ConcatLIBSVMParamStr.m	2021-04-29 16:44:51.170000000 +0200
@@ -3,7 +3,7 @@
 global CMDSTR
 
 %% Build parameter string
-cmdstr = CMDSTR.simplemodel;
+cmdstr = CMDSTR.simplemodel; 
 for i = 1:numel(CMDSTR.ParamStr)
     cmdstr = [ ' -' CMDSTR.ParamStr{i} ' ' strtrim(Params(i,:)) cmdstr ];
 end
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_DefineCmdStr.m NeuroMiner-1-main.clara/trainpredict/nk_DefineCmdStr.m
--- NeuroMiner-1-main/trainpredict/nk_DefineCmdStr.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_DefineCmdStr.m	2021-05-10 10:33:45.950000000 +0200
@@ -1,5 +1,4 @@
 function vargout = nk_DefineCmdStr(SVM, MODEFL)
-
 vargout = [];
 
 switch SVM.prog
@@ -55,6 +54,8 @@
                         vargout.ParamStr = [vargout.ParamStr 'g'];
                     case ' -t 3'
                          vargout.ParamStr = [vargout.ParamStr 'g', 'r'];
+                    case ' -t 4'
+                        vargout.ParamStr = vargout.ParamStr; 
                     otherwise
                         switch SVM.LIBSVM.LIBSVMver
                             case {0,1,2} 
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_GenRobSVMCmd.m NeuroMiner-1-main.clara/trainpredict/nk_GenRobSVMCmd.m
--- NeuroMiner-1-main/trainpredict/nk_GenRobSVMCmd.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_GenRobSVMCmd.m	2021-04-29 16:51:02.640000000 +0200
@@ -18,6 +18,8 @@
         cmd = sprintf('%s -r %g -d %g', options.coef0, options.degree);
     case '-t 2'
         cmd = sprintf('%s -g %g', options.gamma);
+    case '-t 4'
+        cmd = sprintf('%s -k %s', options.kernel);
 end
 
 cmd = sprintf('%s -m %g -e %g -h %g -b %g', cmd, ...
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_GetParam2_LIBSVM.m NeuroMiner-1-main.clara/trainpredict/nk_GetParam2_LIBSVM.m
--- NeuroMiner-1-main/trainpredict/nk_GetParam2_LIBSVM.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_GetParam2_LIBSVM.m	2021-07-07 22:03:31.600000000 +0200
@@ -1,3 +1,4 @@
+
 % =========================================================================
 % FORMAT function [param, model] = nk_GetParam_LIBSVM(Y, label, ModelOnly, 
 %                                                                 ...cmdstr)
@@ -10,10 +11,27 @@
 
 function [param, model] = nk_GetParam2_LIBSVM(Y, label, ModelOnly, cmd)
                                             
-global SVM EVALFUNC LIBSVMTRAIN MODEFL CMDSTR                         
 
-param = [];flw = 0;
+global SVM EVALFUNC LIBSVMTRAIN MODEFL CMDSTR GK %CV CVPOS inparams
+
+param = [];flw = 0; GK = struct('gkernelBool',0);
 if strcmp(LIBSVMTRAIN,'svmtrain312'), flw = 1; end
+if SVM.kernel.kerndef == 5, GK.gkernelBool = 1; GK.gkernelFunction = 'WL'; GK.iter = CMDSTR.WLiter; end
+if SVM.kernel.kerndef == 6, GK.gkernelBool = 1; GK.gkernelFunction = 'WLspdelta'; GK.iter = CMDSTR.WLiter; end
+if SVM.kernel.kerndef == 7, GK.gkernelBool = 1; GK.gkernelFunction = 'WLedge'; GK.iter = CMDSTR.WLiter; end
+if SVM.kernel.kerndef == 8 
+    GK.evalStr = strings; 
+    if SVM.kernel.customfunc_nargin > 0
+        for n = 1:SVM.kernel.customfunc_nargin
+            argName = sprintf('customkernel_arg%d', n); 
+            arg_i = CMDSTR.(argName);
+            if CMDSTR.(argName)
+                GK.evalStr = sprintf('%s, %d' , GK.evalStr, arg_i);
+            end
+        end
+    end
+    %GK.iter = CMDSTR.WLiter; 
+end % function has to be on Matlab path!
 
 % Check if weighting is necessary
 cmd = nk_SetWeightStr(SVM, MODEFL, CMDSTR, label, cmd);
@@ -32,7 +50,16 @@
     % MKL-based learning not implemented yet
    
 else % Univariate case
-    
+    if GK.gkernelBool == 1
+        Y = GraphKernel_matrixInput(Y,Y, GK.gkernelFunction, GK.iter);
+        numTrain = size(Y,1);
+        Y = [(1:numTrain)' , Y];
+    end
+    if SVM.kernel.kerndef == 8
+ 
+        Y = eval(sprintf('feval(SVM.kernel.customfunc, Y, Y %s)', GK.evalStr));
+        Y = [(1:numTrain)' , Y];
+    end
     if size(label,1) ~= size(Y,1), label = label'; end
     if SVM.RVMflag, label(label==-1)=2; end
     if ~SVM.LIBSVM.Weighting && (isfield(SVM,'AdaBoost') && SVM.AdaBoost.flag)        
@@ -55,8 +82,9 @@
         if flw
             model = feval( LIBSVMTRAIN, W, label, Y, cmd);
         else
-            model = feval( LIBSVMTRAIN, label, Y, cmd );
+            model = feval( LIBSVMTRAIN, label, Y, cmd);
         end
+    
     end
     if ~ModelOnly
         [param.target, param.dec_values] = nk_GetTestPerf_LIBSVM([], Y, label, model);
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_GetTestPerf_LIBSVM.m NeuroMiner-1-main.clara/trainpredict/nk_GetTestPerf_LIBSVM.m
--- NeuroMiner-1-main/trainpredict/nk_GetTestPerf_LIBSVM.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_GetTestPerf_LIBSVM.m	2021-06-30 18:33:56.600000000 +0200
@@ -1,5 +1,17 @@
 function [rs, ds] = nk_GetTestPerf_LIBSVM(~, tXtest, Ytest, md, ~, ~)
-global SVM LIBSVMPREDICT
+ global SVM LIBSVMPREDICT GK Ytrain
+
+
+if GK.gkernelBool
+    if SVM.kernel.kerndef == 8
+        tXtest = eval(sprintf('feval(SVM.kernel.customfunc, tXtest, Ytrain{1,1}{1,1} %s)', GK.evalStr));
+    else
+        tXtest = GraphKernel_matrixInput(tXtest,Ytrain{1,1}{1,1}, GK.gkernelFunction, GK.iter); 
+        numTest = size(tXtest,1);
+        tXtest = [(1:numTest)', tXtest];
+    %[lbl, acc, dec] = svmpredict(Ytest, tXtest, md, []);
+    end
+end
 
 [err_test, ~, predict_test] = feval( LIBSVMPREDICT, Ytest, tXtest, md, sprintf(' -b %g',SVM.LIBSVM.Optimization.b)); 
 try
diff -Naur -x '\.*' -x '*.mexmaci64' -x '*.asv' -x '*.mat' -x '*.csv' -x '*.pdf' NeuroMiner-1-main/trainpredict/nk_ReturnMLParams.m NeuroMiner-1-main.clara/trainpredict/nk_ReturnMLParams.m
--- NeuroMiner-1-main/trainpredict/nk_ReturnMLParams.m	2020-10-21 16:27:36.000000000 +0200
+++ NeuroMiner-1-main.clara/trainpredict/nk_ReturnMLParams.m	2021-05-09 12:41:55.660000000 +0200
@@ -36,7 +36,7 @@
     
     switch SVM.kernel.kernstr
         
-        case {' -t 0',' -t 4',' -t 5', 'lin','linear','lin_kernel','none','lin_elm'}
+        case {' -t 0',' -t 5', 'lin','linear','lin_kernel','none','lin_elm'}
 
             if VERBOSE, fprintf('\n%s #%g: Kernel parameters will be omitted.',strout,i); end
 
@@ -48,6 +48,8 @@
         case ' -t 3'
             [Params{i}{end+1}, Params_desc{i}{end+1} ] = nk_ReturnParam('ML-Kernel parameter(s)',PX.Params_desc, PX.Params);
             [Params{i}{end+1}, Params_desc{i}{end+1} ] = nk_ReturnParam('ML-Sigmoid coefficients',PX.Params_desc, PX.Params);
+        case ' -t 4'
+            [Params{i}{end+1}, Params_desc{i}{end+1} ] = nk_ReturnParam('ML-Kernel parameter(s)',PX.Params_desc, PX.Params);
 
         otherwise
             % This is the gamma exponent parameter of the RBF kernel
